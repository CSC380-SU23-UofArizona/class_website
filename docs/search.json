[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Principles Of Data Science",
    "section": "",
    "text": "Latest Lectures Week 9.1\nFinals Released released"
  },
  {
    "objectID": "index.html#recent",
    "href": "index.html#recent",
    "title": "Principles Of Data Science",
    "section": "",
    "text": "Latest Lectures Week 9.1\nFinals Released released"
  },
  {
    "objectID": "index.html#checklist",
    "href": "index.html#checklist",
    "title": "Principles Of Data Science",
    "section": "CheckList",
    "text": "CheckList\n\nDue Dates\n\nFinals | Due by August 7th, 2023 | 11:59 pm\nWeekly Checkins 7,8,9 | Due by August 6th,2023 | 11:59 pm"
  },
  {
    "objectID": "index.html#class-playlist",
    "href": "index.html#class-playlist",
    "title": "Principles Of Data Science",
    "section": "Class Playlist",
    "text": "Class Playlist"
  },
  {
    "objectID": "index.html#staff",
    "href": "index.html#staff",
    "title": "Principles Of Data Science",
    "section": "Staff",
    "text": "Staff\n\n\n\n\n   Enfa George  Summer Instructor \n\n\nOffice Hours :\n\n\nWed : 2 pm to 3 pm\n\n\nFri : 3 pm to 4 pm\n\n\nBy Appointment\n\n\nWebsite : beingenfa.com Incoming PhD Student  Comp Sci Fall 2023  Focus on NLP - Fairness and Ethics, Public Policy\n\n\n\n\n\n\n\n\n\n\n   Bennett Brixen  Summer TA\n\n\nOffice Hours :\n\n\nMon : 11 am to 12 pm\n\n\nTue : 1 pm to 2 pm\n\n\nThur : 4 pm to 5 pm\n\n\nGitHub : BBrixen Incoming Masters Student  Comp Sci Fall 2023  Focus on Algorithm Design and Analysis"
  },
  {
    "objectID": "Course_Content/Week_4&5/home.html",
    "href": "Course_Content/Week_4&5/home.html",
    "title": "Week 4 : Data Preprocessing",
    "section": "",
    "text": "Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nDevelop proficiency in using data science tools and programming languages.\nExplore the ethical considerations associated with data-driven decision-making.\n\n\n\n\n\nConvert a raw data source into a version appropriate for downstream analysis using Python.\nDemonstrate awareness of bias and ethics in data science.\n\n\n\n\n\n\n\n4.1 - Pandas cont…\n4.2 - Data Preprocessing - Part 1 | (Tabular Data, Audio and Image)\n4.3 - Data Preprocessing - Part 2 & Data Visualization\n\n\n\n\n\n\n\n\nHomework 2: Due July 7, 5 pm\nBonus Homework: Due July 7, 5 pm\n\n\n\n\n\nWeekly Checkin for Week 3 : Due 5 pm, July 2, Sunday ( Based on Lecture 2.x.1 & 3.1, 3.2, 3.3)\nWeekly Checkin for Week 4 : Due 5 pm, July 7, ( Based on Lecture 4.1,4.2,4.3)\n\n\n\n\n\nGithub experience in class . Take Survey on D2L&gt;Survey\n\n\n\n\n\n\n\nStrikethrough text is changes in plan.\nGreen is new items added after planning.\nChecked boxes are completed items.\n\n\n\n\n\n\n\n4.1 - cont .. Pandas Data Processing\n4.2 - Data Preprocessing - Part 1 | (Tabular Data, Audio and Image)\n4.3 - Data Preprocessing - Part 2\n\n\n\n\n\nHomework 2 released\nBonus Homework (tentative)\n\n\n\n\n\nCheck-in for week 3 is due on 5 pm, July 2, Sunday. (Based on Lecture 2.x.1 & 3.1, 3.2, 3.3)\nCheck-in for week 4 opens ( Based on Lecture 4.1, 4.2, 4.3 )\n\n\n\n\n\nWeek 4’s assigned reading already published.\nWeek 5 assigned reading will be announced.\n\n\n\n\n\nContract Finalisation.\n\n\n\n\n\nGithub experience in class . Take Survey on D2L&gt;Survey"
  },
  {
    "objectID": "Course_Content/Week_4&5/home.html#objectives",
    "href": "Course_Content/Week_4&5/home.html#objectives",
    "title": "Week 4 : Data Preprocessing",
    "section": "",
    "text": "Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nDevelop proficiency in using data science tools and programming languages.\nExplore the ethical considerations associated with data-driven decision-making.\n\n\n\n\n\nConvert a raw data source into a version appropriate for downstream analysis using Python.\nDemonstrate awareness of bias and ethics in data science."
  },
  {
    "objectID": "Course_Content/Week_4&5/home.html#lectures",
    "href": "Course_Content/Week_4&5/home.html#lectures",
    "title": "Week 4 : Data Preprocessing",
    "section": "",
    "text": "4.1 - Pandas cont…\n4.2 - Data Preprocessing - Part 1 | (Tabular Data, Audio and Image)\n4.3 - Data Preprocessing - Part 2 & Data Visualization"
  },
  {
    "objectID": "Course_Content/Week_4&5/home.html#activities",
    "href": "Course_Content/Week_4&5/home.html#activities",
    "title": "Week 4 : Data Preprocessing",
    "section": "",
    "text": "Homework 2: Due July 7, 5 pm\nBonus Homework: Due July 7, 5 pm\n\n\n\n\n\nWeekly Checkin for Week 3 : Due 5 pm, July 2, Sunday ( Based on Lecture 2.x.1 & 3.1, 3.2, 3.3)\nWeekly Checkin for Week 4 : Due 5 pm, July 7, ( Based on Lecture 4.1,4.2,4.3)\n\n\n\n\n\nGithub experience in class . Take Survey on D2L&gt;Survey"
  },
  {
    "objectID": "Course_Content/Week_4&5/home.html#summary-plan-versus-achievements",
    "href": "Course_Content/Week_4&5/home.html#summary-plan-versus-achievements",
    "title": "Week 4 : Data Preprocessing",
    "section": "",
    "text": "Strikethrough text is changes in plan.\nGreen is new items added after planning.\nChecked boxes are completed items.\n\n\n\n\n\n\n\n4.1 - cont .. Pandas Data Processing\n4.2 - Data Preprocessing - Part 1 | (Tabular Data, Audio and Image)\n4.3 - Data Preprocessing - Part 2\n\n\n\n\n\nHomework 2 released\nBonus Homework (tentative)\n\n\n\n\n\nCheck-in for week 3 is due on 5 pm, July 2, Sunday. (Based on Lecture 2.x.1 & 3.1, 3.2, 3.3)\nCheck-in for week 4 opens ( Based on Lecture 4.1, 4.2, 4.3 )\n\n\n\n\n\nWeek 4’s assigned reading already published.\nWeek 5 assigned reading will be announced.\n\n\n\n\n\nContract Finalisation.\n\n\n\n\n\nGithub experience in class . Take Survey on D2L&gt;Survey"
  },
  {
    "objectID": "Course_Content/Week_4&5/1/home.html",
    "href": "Course_Content/Week_4&5/1/home.html",
    "title": "4.1 - Pandas cont…",
    "section": "",
    "text": "Continuing learning pandas functions\n\nGroup By\n\nHow does it work ? Split/Apply/Combine\n\nMulti-Index\nJoins\n\nInner\nOuter\nLeft Join\nRight Join\nCross\n\nWe also saw:\n\nidxmax()\npd.concat()\n\n\n\n\n\n\n\nSpotify Top 50 Playlist Songs | @anxods | Kaggle \nJupyter Notebook\n\n\n\n\n\n\n\n\n\n4.2 - Data Cleaning"
  },
  {
    "objectID": "Course_Content/Week_4&5/1/home.html#objectives",
    "href": "Course_Content/Week_4&5/1/home.html#objectives",
    "title": "4.1 - Pandas cont…",
    "section": "",
    "text": "Continuing learning pandas functions\n\nGroup By\n\nHow does it work ? Split/Apply/Combine\n\nMulti-Index\nJoins\n\nInner\nOuter\nLeft Join\nRight Join\nCross\n\nWe also saw:\n\nidxmax()\npd.concat()"
  },
  {
    "objectID": "Course_Content/Week_4&5/1/home.html#materials",
    "href": "Course_Content/Week_4&5/1/home.html#materials",
    "title": "4.1 - Pandas cont…",
    "section": "",
    "text": "Spotify Top 50 Playlist Songs | @anxods | Kaggle \nJupyter Notebook"
  },
  {
    "objectID": "Course_Content/Week_4&5/1/home.html#next-class",
    "href": "Course_Content/Week_4&5/1/home.html#next-class",
    "title": "4.1 - Pandas cont…",
    "section": "",
    "text": "4.2 - Data Cleaning"
  },
  {
    "objectID": "Course_Content/Week_4&5/1/Notebook.html",
    "href": "Course_Content/Week_4&5/1/Notebook.html",
    "title": "Lecture 4.1",
    "section": "",
    "text": "import pandas as pd\n\n\nWORLD_DATA_PATH = \"data/spotify-top-50-playlist-songs-anxods/data/spotify-streaming-top-50-world.csv\"\nUSA_DATA_PATH = \"data/spotify-top-50-playlist-songs-anxods/data/spotify-streaming-top-50-usa.csv\"\n\n\nworld_df = pd.read_csv(WORLD_DATA_PATH)\nusa_df = pd.read_csv(USA_DATA_PATH)\n\n\nworld_df.sample()\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n263\n2023-05-23\n14\nEl Azul\nJunior H & Peso Pluma\n95\n187225\nsingle\n1\n2023-02-10\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27333ed35...\n\n\n\n\n\n\n\n\nusa_df.sample(1)\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n254\n2023-05-23\n5\nKill Bill\nSZA\n94\n153946\nalbum\n23\n2022-12-08\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2730c471c...\n\n\n\n\n\n\n\n\n\n\nsource = “https://www.kaggle.com/code/alenavorushilova/grouping-sorting-and-filtering-data-tutorial”\n\nworld_df.groupby(\"position\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x117d3ae90&gt;\n\n\nQ:For each artist in the dataset, what was the longest song that made it to top 50?\n\nworld_df.sample(1)\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n105\n2023-05-20\n6\nDaylight\nDavid Kushner\n97\n212953\nsingle\n1\n2023-04-14\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27395ca6a...\n\n\n\n\n\n\n\n\nworld_df.groupby(\"artist\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x117d3b880&gt;\n\n\n\nworld_df.groupby(\"artist\")[\"duration_ms\"].max().to_frame()\n\n\n\n\n\n\n\n\nduration_ms\n\n\nartist\n\n\n\n\n\nAlessandra\n147979\n\n\nArctic Monkeys\n183956\n\n\nBTS\n229953\n\n\nBad Bunny\n231704\n\n\nBeyoncé & Kendrick Lamar\n260962\n\n\n...\n...\n\n\nTyler, The Creator\n180386\n\n\nYahritza Y Su Esencia & Grupo Frontera\n160517\n\n\nYandel\n216148\n\n\nYng Lvcas & Peso Pluma\n234352\n\n\nd4vd\n242484\n\n\n\n\n75 rows × 1 columns\n\n\n\nQ: For each position (1 to 50), which artist was in that rank the maximum number of days ( in total , needn’t be consecutive)\n\nworld_df.groupby('position')\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x117f5c2b0&gt;\n\n\n\nworld_df.groupby('position')['artist']\n\n&lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x117f5c310&gt;\n\n\n\nworld_df.groupby('position')['artist'].value_counts().to_frame()\n\n\n\n\n\n\n\n\n\nartist\n\n\nposition\nartist\n\n\n\n\n\n1\nEslabon Armado\n31\n\n\nBad Bunny\n2\n\n\nBizarrap & Peso Pluma\n2\n\n\nBTS\n1\n\n\n2\nBizarrap & Peso Pluma\n16\n\n\n...\n...\n...\n\n\n50\nLil Durk\n1\n\n\nLil Mabu\n1\n\n\nNatanael Cano & Peso Pluma & Gabito Ballesteros\n1\n\n\nSam Smith\n1\n\n\nTaylor Swift\n1\n\n\n\n\n790 rows × 1 columns\n\n\n\n\n\n\n\nworld_df.index\n\nRangeIndex(start=0, stop=1800, step=1)\n\n\n\nworld_df_with_rank_counts = world_df.groupby('position')['artist'].value_counts().to_frame()\nworld_df_with_rank_counts.index\n\nMultiIndex([( 1,                                  'Eslabon Armado'),\n            ( 1,                                       'Bad Bunny'),\n            ( 1,                           'Bizarrap & Peso Pluma'),\n            ( 1,                                             'BTS'),\n            ( 2,                           'Bizarrap & Peso Pluma'),\n            ( 2,                                       'Bad Bunny'),\n            ( 2,                                  'Eslabon Armado'),\n            ( 2,                      'Grupo Frontera & Bad Bunny'),\n            ( 2,                          'Yng Lvcas & Peso Pluma'),\n            ( 3,                                       'Bad Bunny'),\n            ...\n            (50,                       'DENNIS & MC Kevin o Chris'),\n            (50,                                        'Dua Lipa'),\n            (50,                           'Eden Muñoz & Junior H'),\n            (50,                                  'Eladio Carrion'),\n            (50,                                      'Kali Uchis'),\n            (50,                                        'Lil Durk'),\n            (50,                                        'Lil Mabu'),\n            (50, 'Natanael Cano & Peso Pluma & Gabito Ballesteros'),\n            (50,                                       'Sam Smith'),\n            (50,                                    'Taylor Swift')],\n           names=['position', 'artist'], length=790)\n\n\n\n\n\nworld_df_with_rank_counts.index.get_level_values('position')#world_df_with_rank_counts.index.get_level_values(0)\n\nInt64Index([ 1,  1,  1,  1,  2,  2,  2,  2,  2,  3,\n            ...\n            50, 50, 50, 50, 50, 50, 50, 50, 50, 50],\n           dtype='int64', name='position', length=790)\n\n\n\nworld_df_with_rank_counts.index.get_level_values('artist') #world_df_with_rank_counts.index.get_level_values(1)\n\nIndex(['Eslabon Armado', 'Bad Bunny', 'Bizarrap & Peso Pluma', 'BTS',\n       'Bizarrap & Peso Pluma', 'Bad Bunny', 'Eslabon Armado',\n       'Grupo Frontera & Bad Bunny', 'Yng Lvcas & Peso Pluma', 'Bad Bunny',\n       ...\n       'DENNIS & MC Kevin o Chris', 'Dua Lipa', 'Eden Muñoz & Junior H',\n       'Eladio Carrion', 'Kali Uchis', 'Lil Durk', 'Lil Mabu',\n       'Natanael Cano & Peso Pluma & Gabito Ballesteros', 'Sam Smith',\n       'Taylor Swift'],\n      dtype='object', name='artist', length=790)\n\n\n\nworld_df_with_rank_counts['rank'] = world_df_with_rank_counts.index.get_level_values('position')\nworld_df_with_rank_counts['artist(s)'] = world_df_with_rank_counts.index.get_level_values('artist') \nworld_df_with_rank_counts.sample(2)\n\n\n\n\n\n\n\n\n\nartist\nrank\nartist(s)\n\n\nposition\nartist\n\n\n\n\n\n\n\n39\nTaylor Swift\n1\n39\nTaylor Swift\n\n\n46\nDua Lipa\n1\n46\nDua Lipa\n\n\n\n\n\n\n\n\nworld_df_with_rank_counts.reset_index(drop = True, inplace = True)\n\n\nworld_df_with_rank_counts\n\n\n\n\n\n\n\n\nartist\nrank\nartist(s)\n\n\n\n\n0\n31\n1\nEslabon Armado\n\n\n1\n2\n1\nBad Bunny\n\n\n2\n2\n1\nBizarrap & Peso Pluma\n\n\n3\n1\n1\nBTS\n\n\n4\n16\n2\nBizarrap & Peso Pluma\n\n\n...\n...\n...\n...\n\n\n785\n1\n50\nLil Durk\n\n\n786\n1\n50\nLil Mabu\n\n\n787\n1\n50\nNatanael Cano & Peso Pluma & Gabito Ballesteros\n\n\n788\n1\n50\nSam Smith\n\n\n789\n1\n50\nTaylor Swift\n\n\n\n\n790 rows × 3 columns\n\n\n\n\nworld_df_with_rank_counts.rename({'artist':'count'},axis = 'columns',inplace=True)\n\n\nworld_df_with_rank_counts\n\n\n\n\n\n\n\n\ncount\nrank\nartist(s)\n\n\n\n\n0\n31\n1\nEslabon Armado\n\n\n1\n2\n1\nBad Bunny\n\n\n2\n2\n1\nBizarrap & Peso Pluma\n\n\n3\n1\n1\nBTS\n\n\n4\n16\n2\nBizarrap & Peso Pluma\n\n\n...\n...\n...\n...\n\n\n785\n1\n50\nLil Durk\n\n\n786\n1\n50\nLil Mabu\n\n\n787\n1\n50\nNatanael Cano & Peso Pluma & Gabito Ballesteros\n\n\n788\n1\n50\nSam Smith\n\n\n789\n1\n50\nTaylor Swift\n\n\n\n\n790 rows × 3 columns\n\n\n\n\n\n\n\nworld_df_with_rank_counts = world_df.groupby('position')['artist'].value_counts().to_frame()\nworld_df_with_rank_counts.sample(2)\n\n\n\n\n\n\n\n\n\nartist\n\n\nposition\nartist\n\n\n\n\n\n26\nMiley Cyrus\n1\n\n\n33\nTina Turner\n1\n\n\n\n\n\n\n\n\nworld_df_with_rank_counts.rename({'artist':'count'},axis = 'columns', inplace = True)\n\n\nworld_df_with_rank_counts.reset_index(inplace=True)\n\n\nworld_df_with_rank_counts\n\n\n\n\n\n\n\n\nposition\nartist\ncount\n\n\n\n\n0\n1\nEslabon Armado\n31\n\n\n1\n1\nBad Bunny\n2\n\n\n2\n1\nBizarrap & Peso Pluma\n2\n\n\n3\n1\nBTS\n1\n\n\n4\n2\nBizarrap & Peso Pluma\n16\n\n\n...\n...\n...\n...\n\n\n785\n50\nLil Durk\n1\n\n\n786\n50\nLil Mabu\n1\n\n\n787\n50\nNatanael Cano & Peso Pluma & Gabito Ballesteros\n1\n\n\n788\n50\nSam Smith\n1\n\n\n789\n50\nTaylor Swift\n1\n\n\n\n\n790 rows × 3 columns\n\n\n\n\nworld_df_with_rank_counts[world_df_with_rank_counts['position'] ==1]\n\n\n\n\n\n\n\n\nposition\nartist\ncount\n\n\n\n\n0\n1\nEslabon Armado\n31\n\n\n1\n1\nBad Bunny\n2\n\n\n2\n1\nBizarrap & Peso Pluma\n2\n\n\n3\n1\nBTS\n1\n\n\n\n\n\n\n\n\nmax_count_index = world_df_with_rank_counts[world_df_with_rank_counts['position'] ==1]['count'].idxmax() # new : idxmax()\n\n\nworld_df_with_rank_counts.loc[max_count_index].to_frame().T #Revisiting .loc\n\n\n\n\n\n\n\n\nposition\nartist\ncount\n\n\n\n\n0\n1\nEslabon Armado\n31\n\n\n\n\n\n\n\n\n# for easier view : **Q: For each position (1 to 50), which artist was in that rank the maximum number of days** ( in total , needn't be consecutive)\n\nlist_of_dataframes = []\nfor position in range(1,51):\n    max_count_index = world_df_with_rank_counts[world_df_with_rank_counts['position'] ==position]['count'].idxmax()\n    list_of_dataframes.append(world_df_with_rank_counts.loc[max_count_index].to_frame().T )\n\n\nlist_of_dataframes[10]\n\n\n\n\n\n\n\n\nposition\nartist\ncount\n\n\n\n\n69\n11\nSZA\n9\n\n\n\n\n\n\n\n\n\n\nmerged_df = pd.concat(list_of_dataframes)\nmerged_df\n\n\n\n\n\n\n\n\nposition\nartist\ncount\n\n\n\n\n0\n1\nEslabon Armado\n31\n\n\n4\n2\nBizarrap & Peso Pluma\n16\n\n\n9\n3\nBad Bunny\n18\n\n\n14\n4\nYng Lvcas & Peso Pluma\n13\n\n\n20\n5\nGrupo Frontera & Bad Bunny\n11\n\n\n26\n6\nMiley Cyrus\n19\n\n\n33\n7\nDavid Kushner\n10\n\n\n41\n8\nSZA\n9\n\n\n50\n9\nHarry Styles\n14\n\n\n59\n10\nDavid Kushner\n8\n\n\n69\n11\nSZA\n9\n\n\n79\n12\nFuerza Regida\n7\n\n\n92\n13\nFeid & Young Miko\n11\n\n\n105\n14\nMetro Boomin\n12\n\n\n116\n15\nMetro Boomin\n5\n\n\n130\n16\nMetro Boomin\n8\n\n\n144\n17\nRema\n6\n\n\n163\n18\nRema\n5\n\n\n180\n19\nMetro Boomin\n7\n\n\n196\n20\nMetro Boomin\n8\n\n\n213\n21\nJunior H & Peso Pluma\n4\n\n\n232\n22\nPinkPantheress & Ice Spice\n7\n\n\n250\n23\nMetro Boomin\n5\n\n\n267\n24\nLana Del Rey\n4\n\n\n283\n25\nThe Weeknd\n6\n\n\n303\n26\nMorgan Wallen\n6\n\n\n323\n27\nMorgan Wallen\n5\n\n\n341\n28\nTaylor Swift\n8\n\n\n355\n29\nArctic Monkeys\n5\n\n\n373\n30\nThe Weeknd\n10\n\n\n388\n31\nThe Weeknd\n6\n\n\n406\n32\nTaylor Swift\n5\n\n\n425\n33\nd4vd\n4\n\n\n446\n34\nMetro Boomin\n4\n\n\n465\n35\nThe Weeknd\n6\n\n\n485\n36\nThe Weeknd\n6\n\n\n505\n37\nYandel\n4\n\n\n529\n38\nThe Weeknd\n7\n\n\n548\n39\nThe Weeknd\n7\n\n\n566\n40\nTom Odell\n5\n\n\n585\n41\nJVKE\n5\n\n\n602\n42\nLibianca\n6\n\n\n623\n43\nOneRepublic\n5\n\n\n643\n44\nLibianca\n6\n\n\n663\n45\nYahritza Y Su Esencia & Grupo Frontera\n4\n\n\n687\n46\nManuel Turizo\n4\n\n\n710\n47\nManuel Turizo\n4\n\n\n730\n48\nSam Smith\n4\n\n\n747\n49\nTaylor Swift\n4\n\n\n769\n50\nPeso Pluma\n4\n\n\n\n\n\n\n\n\n\n\n\nmerged_df.set_index(\"position\",drop = True,inplace = True)\n\n\ndel merged_df['count']\n\n\nmerged_df\n\n\n\n\n\n\n\n\nartist\n\n\nposition\n\n\n\n\n\n1\nEslabon Armado\n\n\n2\nBizarrap & Peso Pluma\n\n\n3\nBad Bunny\n\n\n4\nYng Lvcas & Peso Pluma\n\n\n5\nGrupo Frontera & Bad Bunny\n\n\n6\nMiley Cyrus\n\n\n7\nDavid Kushner\n\n\n8\nSZA\n\n\n9\nHarry Styles\n\n\n10\nDavid Kushner\n\n\n11\nSZA\n\n\n12\nFuerza Regida\n\n\n13\nFeid & Young Miko\n\n\n14\nMetro Boomin\n\n\n15\nMetro Boomin\n\n\n16\nMetro Boomin\n\n\n17\nRema\n\n\n18\nRema\n\n\n19\nMetro Boomin\n\n\n20\nMetro Boomin\n\n\n21\nJunior H & Peso Pluma\n\n\n22\nPinkPantheress & Ice Spice\n\n\n23\nMetro Boomin\n\n\n24\nLana Del Rey\n\n\n25\nThe Weeknd\n\n\n26\nMorgan Wallen\n\n\n27\nMorgan Wallen\n\n\n28\nTaylor Swift\n\n\n29\nArctic Monkeys\n\n\n30\nThe Weeknd\n\n\n31\nThe Weeknd\n\n\n32\nTaylor Swift\n\n\n33\nd4vd\n\n\n34\nMetro Boomin\n\n\n35\nThe Weeknd\n\n\n36\nThe Weeknd\n\n\n37\nYandel\n\n\n38\nThe Weeknd\n\n\n39\nThe Weeknd\n\n\n40\nTom Odell\n\n\n41\nJVKE\n\n\n42\nLibianca\n\n\n43\nOneRepublic\n\n\n44\nLibianca\n\n\n45\nYahritza Y Su Esencia & Grupo Frontera\n\n\n46\nManuel Turizo\n\n\n47\nManuel Turizo\n\n\n48\nSam Smith\n\n\n49\nTaylor Swift\n\n\n50\nPeso Pluma\n\n\n\n\n\n\n\nQ. How many times has an artist taken a spot between 1 and 50 ?\nFor each artist, how many times have they taken a spot between 1 and 50 (both inclusive)\n\nworld_df.sample()\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n1458\n2023-06-16\n9\nAs It Was\nHarry Styles\n93\n167303\nalbum\n13\n2022-05-20\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2732e8ed7...\n\n\n\n\n\n\n\n\nrank_count_df = world_df.groupby('artist')['position'].value_counts().to_frame()\nrank_count_df\n\n\n\n\n\n\n\n\n\nposition\n\n\nartist\nposition\n\n\n\n\n\nAlessandra\n42\n1\n\n\n47\n1\n\n\nArctic Monkeys\n29\n5\n\n\n25\n4\n\n\n22\n3\n\n\n...\n...\n...\n\n\nd4vd\n39\n1\n\n\n40\n1\n\n\n41\n1\n\n\n42\n1\n\n\n43\n1\n\n\n\n\n790 rows × 1 columns\n\n\n\n\nrank_count_df.rename({'position':'number of times'}, axis = 'columns',inplace = True)\n\n\nrank_count_df.reset_index(inplace = True)\n\n\nrank_count_df\n\n\n\n\n\n\n\n\nartist\nposition\nnumber of times\n\n\n\n\n0\nAlessandra\n42\n1\n\n\n1\nAlessandra\n47\n1\n\n\n2\nArctic Monkeys\n29\n5\n\n\n3\nArctic Monkeys\n25\n4\n\n\n4\nArctic Monkeys\n22\n3\n\n\n...\n...\n...\n...\n\n\n785\nd4vd\n39\n1\n\n\n786\nd4vd\n40\n1\n\n\n787\nd4vd\n41\n1\n\n\n788\nd4vd\n42\n1\n\n\n789\nd4vd\n43\n1\n\n\n\n\n790 rows × 3 columns\n\n\n\n\ntop_50_count_for_each_artist = rank_count_df.groupby('artist')['number of times'].sum().to_frame()\n\n\ntop_50_count_for_each_artist.sort_values(by = 'number of times', ascending=False)\n\n\n\n\n\n\n\n\nnumber of times\n\n\nartist\n\n\n\n\n\nThe Weeknd\n108\n\n\nTaylor Swift\n92\n\n\nMetro Boomin\n91\n\n\nMiley Cyrus\n72\n\n\nROSALÍA & Rauw Alejandro\n36\n\n\n...\n...\n\n\nJISOO\n1\n\n\nStray Kids\n1\n\n\nFast & Furious: The Fast Saga\n1\n\n\nSaiko & Feid & Quevedo\n1\n\n\nEladio Carrion\n1\n\n\n\n\n75 rows × 1 columns\n\n\n\n\n\n\n\n\n(discussed in Lecture 3.3)\n\n\nworld_info = world_df[['date','position','artist','song']]\nworld_info.sample(3)\n\n\n\n\n\n\n\n\ndate\nposition\nartist\nsong\n\n\n\n\n1037\n2023-06-07\n38\nThe Weeknd\nDie For You\n\n\n557\n2023-05-29\n8\nSZA\nKill Bill\n\n\n975\n2023-06-06\n26\nFast & Furious: The Fast Saga & Jimin & BTS\nAngel Pt. 1 (feat. Jimin of BTS, JVKE & Muni L...\n\n\n\n\n\n\n\n\nworld_info.shape\n\n(1800, 4)\n\n\n\nworld_df['position'] = world_df['position'].astype('int')\n\n\nusa_info = usa_df[['date','position','artist','song']]\nusa_info.sample(3)\n\n\n\n\n\n\n\n\ndate\nposition\nartist\nsong\n\n\n\n\n1763\n2023-06-22\n14\nMetro Boomin\nAnnihilate (Spider-Man: Across the Spider-Vers...\n\n\n1785\n2023-06-22\n36\nMetro Boomin\nCreepin' (with The Weeknd & 21 Savage)\n\n\n120\n2023-05-20\n21\nPeso Pluma\nPor las Noches\n\n\n\n\n\n\n\n\nusa_info.shape\n\n(1800, 4)\n\n\n\nusa_info['position'] = usa_info['position'].astype('int')\n\n/var/folders/r0/bggl6hf15j708chpxqrx5tp00000gn/T/ipykernel_2958/3011614831.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  usa_info['position'] = usa_info['position'].astype('int')\n\n\n\nworld_info.merge(right=usa_info, on= ['date','artist','song'], how='inner') #default is inner\n\n\n\n\n\n\n\n\ndate\nposition_x\nartist\nsong\nposition_y\n\n\n\n\n0\n2023-05-18\n1\nEslabon Armado\nElla Baila Sola\n1\n\n\n1\n2023-05-18\n2\nGrupo Frontera & Bad Bunny\nun x100to\n4\n\n\n2\n2023-05-18\n3\nYng Lvcas & Peso Pluma\nLa Bebe - Remix\n8\n\n\n3\n2023-05-18\n4\nFIFTY FIFTY\nCupid - Twin Ver.\n7\n\n\n4\n2023-05-18\n5\nMiley Cyrus\nFlowers\n14\n\n\n...\n...\n...\n...\n...\n...\n\n\n983\n2023-06-22\n37\nJunior H & Peso Pluma\nEl Azul\n39\n\n\n984\n2023-06-22\n43\nKali Uchis\nMoonlight\n40\n\n\n985\n2023-06-22\n44\nPinkPantheress & Ice Spice\nBoy's a Liar Pt. 2\n18\n\n\n986\n2023-06-22\n48\nPeso Pluma & Natanael Cano\nPRC\n37\n\n\n987\n2023-06-22\n50\nLil Durk\nAll My Life (feat. J. Cole)\n8\n\n\n\n\n988 rows × 5 columns\n\n\n\n\n\n\n\nworld_info.merge(right=usa_info, on= ['date','artist','song'], how='inner', suffixes=['_world','_usa']) #default is inner\n\n\n\n\n\n\n\n\ndate\nposition_world\nartist\nsong\nposition_usa\n\n\n\n\n0\n2023-05-18\n1\nEslabon Armado\nElla Baila Sola\n1\n\n\n1\n2023-05-18\n2\nGrupo Frontera & Bad Bunny\nun x100to\n4\n\n\n2\n2023-05-18\n3\nYng Lvcas & Peso Pluma\nLa Bebe - Remix\n8\n\n\n3\n2023-05-18\n4\nFIFTY FIFTY\nCupid - Twin Ver.\n7\n\n\n4\n2023-05-18\n5\nMiley Cyrus\nFlowers\n14\n\n\n...\n...\n...\n...\n...\n...\n\n\n983\n2023-06-22\n37\nJunior H & Peso Pluma\nEl Azul\n39\n\n\n984\n2023-06-22\n43\nKali Uchis\nMoonlight\n40\n\n\n985\n2023-06-22\n44\nPinkPantheress & Ice Spice\nBoy's a Liar Pt. 2\n18\n\n\n986\n2023-06-22\n48\nPeso Pluma & Natanael Cano\nPRC\n37\n\n\n987\n2023-06-22\n50\nLil Durk\nAll My Life (feat. J. Cole)\n8\n\n\n\n\n988 rows × 5 columns\n\n\n\nOn a given date, a particular artist was on BOTH global and US spotify top 50. position_world gives their global spotify position, and position_usa gives their spotify top 50 in US position\n\n\n\n\n\nworld_info.merge(right=usa_info, on= ['date','artist','song'], how='outer', suffixes=['_world','_usa']) #default is inner\n\n\n\n\n\n\n\n\ndate\nposition_world\nartist\nsong\nposition_usa\n\n\n\n\n0\n2023-05-18\n1.0\nEslabon Armado\nElla Baila Sola\n1.0\n\n\n1\n2023-05-18\n2.0\nGrupo Frontera & Bad Bunny\nun x100to\n4.0\n\n\n2\n2023-05-18\n3.0\nYng Lvcas & Peso Pluma\nLa Bebe - Remix\n8.0\n\n\n3\n2023-05-18\n4.0\nFIFTY FIFTY\nCupid - Twin Ver.\n7.0\n\n\n4\n2023-05-18\n5.0\nMiley Cyrus\nFlowers\n14.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n2607\n2023-06-22\nNaN\nPeso Pluma\nPor las Noches\n45.0\n\n\n2608\n2023-06-22\nNaN\nNLE Choppa\nSLUT ME OUT\n47.0\n\n\n2609\n2023-06-22\nNaN\nDrake\nJimmy Cooks (feat. 21 Savage)\n48.0\n\n\n2610\n2023-06-22\nNaN\nKendrick Lamar\nMoney Trees\n49.0\n\n\n2611\n2023-06-22\nNaN\nSZA\nSnooze\n50.0\n\n\n\n\n2612 rows × 5 columns\n\n\n\nOn a given date, a particular artist was on EITHER global or US spotify top 50. position_world gives their global spotify position, and position_usa gives their spotify top 50 in US position\n\n\n\n\n\nworld_info.merge(right=usa_info, on= ['date','artist','song'], how='left', suffixes=['_world','_usa']) #default is inner\n\n\n\n\n\n\n\n\ndate\nposition_world\nartist\nsong\nposition_usa\n\n\n\n\n0\n2023-05-18\n1\nEslabon Armado\nElla Baila Sola\n1.0\n\n\n1\n2023-05-18\n2\nGrupo Frontera & Bad Bunny\nun x100to\n4.0\n\n\n2\n2023-05-18\n3\nYng Lvcas & Peso Pluma\nLa Bebe - Remix\n8.0\n\n\n3\n2023-05-18\n4\nFIFTY FIFTY\nCupid - Twin Ver.\n7.0\n\n\n4\n2023-05-18\n5\nMiley Cyrus\nFlowers\n14.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n1795\n2023-06-22\n46\nGlass Animals\nHeat Waves\nNaN\n\n\n1796\n2023-06-22\n47\nYahritza Y Su Esencia & Grupo Frontera\nFrágil\nNaN\n\n\n1797\n2023-06-22\n48\nPeso Pluma & Natanael Cano\nPRC\n37.0\n\n\n1798\n2023-06-22\n49\nDua Lipa\nDance The Night (From Barbie The Album)\nNaN\n\n\n1799\n2023-06-22\n50\nLil Durk\nAll My Life (feat. J. Cole)\n8.0\n\n\n\n\n1800 rows × 5 columns\n\n\n\nOn a given date, a particular artist was on global spotify top 50 list. If the same artist for the same song, on the same date, was also on USA top 50, then their rank is given in position_usa. If they were not in USA top 50, position_usa would be NaN( Not a Number)\n\n\n\n\n\nworld_info.merge(right=usa_info, on= ['date','artist','song'], how='right', suffixes=['_world','_usa']) #default is inner\n\n\n\n\n\n\n\n\ndate\nposition_world\nartist\nsong\nposition_usa\n\n\n\n\n0\n2023-05-18\n1.0\nEslabon Armado\nElla Baila Sola\n1\n\n\n1\n2023-05-18\n27.0\nMorgan Wallen\nLast Night\n2\n\n\n2\n2023-05-18\n26.0\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n3\n\n\n3\n2023-05-18\n2.0\nGrupo Frontera & Bad Bunny\nun x100to\n4\n\n\n4\n2023-05-18\n7.0\nSZA\nKill Bill\n5\n\n\n...\n...\n...\n...\n...\n...\n\n\n1795\n2023-06-22\n31.0\nThe Weeknd\nDie For You\n46\n\n\n1796\n2023-06-22\nNaN\nNLE Choppa\nSLUT ME OUT\n47\n\n\n1797\n2023-06-22\nNaN\nDrake\nJimmy Cooks (feat. 21 Savage)\n48\n\n\n1798\n2023-06-22\nNaN\nKendrick Lamar\nMoney Trees\n49\n\n\n1799\n2023-06-22\nNaN\nSZA\nSnooze\n50\n\n\n\n\n1800 rows × 5 columns\n\n\n\nOn a given date, a particular artist was on usa spotify top 50 list. If the same artist for the same song, on the same date, was also on Global top 50, then their rank is given in position_world. If they were not in Global top 50, position_world would be NaN( Not a Number)\n\n\n\n\n\nleft = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\nleft\n\n\n\n\n\n\n\n\ncol1\ncol2\n\n\n\n\n0\n1\n3\n\n\n1\n2\n4\n\n\n\n\n\n\n\n\nright = pd.DataFrame({'col3': [5, 6]}) \nright\n\n\n\n\n\n\n\n\ncol3\n\n\n\n\n0\n5\n\n\n1\n6\n\n\n\n\n\n\n\n\nleft.merge(right,how=\"cross\")\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n0\n1\n3\n5\n\n\n1\n1\n3\n6\n\n\n2\n2\n4\n5\n\n\n3\n2\n4\n6"
  },
  {
    "objectID": "Course_Content/Week_4&5/1/Notebook.html#revisiting-groupby",
    "href": "Course_Content/Week_4&5/1/Notebook.html#revisiting-groupby",
    "title": "Lecture 4.1",
    "section": "",
    "text": "source = “https://www.kaggle.com/code/alenavorushilova/grouping-sorting-and-filtering-data-tutorial”\n\nworld_df.groupby(\"position\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x117d3ae90&gt;\n\n\nQ:For each artist in the dataset, what was the longest song that made it to top 50?\n\nworld_df.sample(1)\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n105\n2023-05-20\n6\nDaylight\nDavid Kushner\n97\n212953\nsingle\n1\n2023-04-14\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27395ca6a...\n\n\n\n\n\n\n\n\nworld_df.groupby(\"artist\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x117d3b880&gt;\n\n\n\nworld_df.groupby(\"artist\")[\"duration_ms\"].max().to_frame()\n\n\n\n\n\n\n\n\nduration_ms\n\n\nartist\n\n\n\n\n\nAlessandra\n147979\n\n\nArctic Monkeys\n183956\n\n\nBTS\n229953\n\n\nBad Bunny\n231704\n\n\nBeyoncé & Kendrick Lamar\n260962\n\n\n...\n...\n\n\nTyler, The Creator\n180386\n\n\nYahritza Y Su Esencia & Grupo Frontera\n160517\n\n\nYandel\n216148\n\n\nYng Lvcas & Peso Pluma\n234352\n\n\nd4vd\n242484\n\n\n\n\n75 rows × 1 columns\n\n\n\nQ: For each position (1 to 50), which artist was in that rank the maximum number of days ( in total , needn’t be consecutive)\n\nworld_df.groupby('position')\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x117f5c2b0&gt;\n\n\n\nworld_df.groupby('position')['artist']\n\n&lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x117f5c310&gt;\n\n\n\nworld_df.groupby('position')['artist'].value_counts().to_frame()\n\n\n\n\n\n\n\n\n\nartist\n\n\nposition\nartist\n\n\n\n\n\n1\nEslabon Armado\n31\n\n\nBad Bunny\n2\n\n\nBizarrap & Peso Pluma\n2\n\n\nBTS\n1\n\n\n2\nBizarrap & Peso Pluma\n16\n\n\n...\n...\n...\n\n\n50\nLil Durk\n1\n\n\nLil Mabu\n1\n\n\nNatanael Cano & Peso Pluma & Gabito Ballesteros\n1\n\n\nSam Smith\n1\n\n\nTaylor Swift\n1\n\n\n\n\n790 rows × 1 columns"
  },
  {
    "objectID": "Course_Content/Week_4&5/1/Notebook.html#introducing-multiindex-in-pandas-dataframes",
    "href": "Course_Content/Week_4&5/1/Notebook.html#introducing-multiindex-in-pandas-dataframes",
    "title": "Lecture 4.1",
    "section": "",
    "text": "world_df.index\n\nRangeIndex(start=0, stop=1800, step=1)\n\n\n\nworld_df_with_rank_counts = world_df.groupby('position')['artist'].value_counts().to_frame()\nworld_df_with_rank_counts.index\n\nMultiIndex([( 1,                                  'Eslabon Armado'),\n            ( 1,                                       'Bad Bunny'),\n            ( 1,                           'Bizarrap & Peso Pluma'),\n            ( 1,                                             'BTS'),\n            ( 2,                           'Bizarrap & Peso Pluma'),\n            ( 2,                                       'Bad Bunny'),\n            ( 2,                                  'Eslabon Armado'),\n            ( 2,                      'Grupo Frontera & Bad Bunny'),\n            ( 2,                          'Yng Lvcas & Peso Pluma'),\n            ( 3,                                       'Bad Bunny'),\n            ...\n            (50,                       'DENNIS & MC Kevin o Chris'),\n            (50,                                        'Dua Lipa'),\n            (50,                           'Eden Muñoz & Junior H'),\n            (50,                                  'Eladio Carrion'),\n            (50,                                      'Kali Uchis'),\n            (50,                                        'Lil Durk'),\n            (50,                                        'Lil Mabu'),\n            (50, 'Natanael Cano & Peso Pluma & Gabito Ballesteros'),\n            (50,                                       'Sam Smith'),\n            (50,                                    'Taylor Swift')],\n           names=['position', 'artist'], length=790)\n\n\n\n\n\nworld_df_with_rank_counts.index.get_level_values('position')#world_df_with_rank_counts.index.get_level_values(0)\n\nInt64Index([ 1,  1,  1,  1,  2,  2,  2,  2,  2,  3,\n            ...\n            50, 50, 50, 50, 50, 50, 50, 50, 50, 50],\n           dtype='int64', name='position', length=790)\n\n\n\nworld_df_with_rank_counts.index.get_level_values('artist') #world_df_with_rank_counts.index.get_level_values(1)\n\nIndex(['Eslabon Armado', 'Bad Bunny', 'Bizarrap & Peso Pluma', 'BTS',\n       'Bizarrap & Peso Pluma', 'Bad Bunny', 'Eslabon Armado',\n       'Grupo Frontera & Bad Bunny', 'Yng Lvcas & Peso Pluma', 'Bad Bunny',\n       ...\n       'DENNIS & MC Kevin o Chris', 'Dua Lipa', 'Eden Muñoz & Junior H',\n       'Eladio Carrion', 'Kali Uchis', 'Lil Durk', 'Lil Mabu',\n       'Natanael Cano & Peso Pluma & Gabito Ballesteros', 'Sam Smith',\n       'Taylor Swift'],\n      dtype='object', name='artist', length=790)\n\n\n\nworld_df_with_rank_counts['rank'] = world_df_with_rank_counts.index.get_level_values('position')\nworld_df_with_rank_counts['artist(s)'] = world_df_with_rank_counts.index.get_level_values('artist') \nworld_df_with_rank_counts.sample(2)\n\n\n\n\n\n\n\n\n\nartist\nrank\nartist(s)\n\n\nposition\nartist\n\n\n\n\n\n\n\n39\nTaylor Swift\n1\n39\nTaylor Swift\n\n\n46\nDua Lipa\n1\n46\nDua Lipa\n\n\n\n\n\n\n\n\nworld_df_with_rank_counts.reset_index(drop = True, inplace = True)\n\n\nworld_df_with_rank_counts\n\n\n\n\n\n\n\n\nartist\nrank\nartist(s)\n\n\n\n\n0\n31\n1\nEslabon Armado\n\n\n1\n2\n1\nBad Bunny\n\n\n2\n2\n1\nBizarrap & Peso Pluma\n\n\n3\n1\n1\nBTS\n\n\n4\n16\n2\nBizarrap & Peso Pluma\n\n\n...\n...\n...\n...\n\n\n785\n1\n50\nLil Durk\n\n\n786\n1\n50\nLil Mabu\n\n\n787\n1\n50\nNatanael Cano & Peso Pluma & Gabito Ballesteros\n\n\n788\n1\n50\nSam Smith\n\n\n789\n1\n50\nTaylor Swift\n\n\n\n\n790 rows × 3 columns\n\n\n\n\nworld_df_with_rank_counts.rename({'artist':'count'},axis = 'columns',inplace=True)\n\n\nworld_df_with_rank_counts\n\n\n\n\n\n\n\n\ncount\nrank\nartist(s)\n\n\n\n\n0\n31\n1\nEslabon Armado\n\n\n1\n2\n1\nBad Bunny\n\n\n2\n2\n1\nBizarrap & Peso Pluma\n\n\n3\n1\n1\nBTS\n\n\n4\n16\n2\nBizarrap & Peso Pluma\n\n\n...\n...\n...\n...\n\n\n785\n1\n50\nLil Durk\n\n\n786\n1\n50\nLil Mabu\n\n\n787\n1\n50\nNatanael Cano & Peso Pluma & Gabito Ballesteros\n\n\n788\n1\n50\nSam Smith\n\n\n789\n1\n50\nTaylor Swift\n\n\n\n\n790 rows × 3 columns\n\n\n\n\n\n\n\nworld_df_with_rank_counts = world_df.groupby('position')['artist'].value_counts().to_frame()\nworld_df_with_rank_counts.sample(2)\n\n\n\n\n\n\n\n\n\nartist\n\n\nposition\nartist\n\n\n\n\n\n26\nMiley Cyrus\n1\n\n\n33\nTina Turner\n1\n\n\n\n\n\n\n\n\nworld_df_with_rank_counts.rename({'artist':'count'},axis = 'columns', inplace = True)\n\n\nworld_df_with_rank_counts.reset_index(inplace=True)\n\n\nworld_df_with_rank_counts\n\n\n\n\n\n\n\n\nposition\nartist\ncount\n\n\n\n\n0\n1\nEslabon Armado\n31\n\n\n1\n1\nBad Bunny\n2\n\n\n2\n1\nBizarrap & Peso Pluma\n2\n\n\n3\n1\nBTS\n1\n\n\n4\n2\nBizarrap & Peso Pluma\n16\n\n\n...\n...\n...\n...\n\n\n785\n50\nLil Durk\n1\n\n\n786\n50\nLil Mabu\n1\n\n\n787\n50\nNatanael Cano & Peso Pluma & Gabito Ballesteros\n1\n\n\n788\n50\nSam Smith\n1\n\n\n789\n50\nTaylor Swift\n1\n\n\n\n\n790 rows × 3 columns\n\n\n\n\nworld_df_with_rank_counts[world_df_with_rank_counts['position'] ==1]\n\n\n\n\n\n\n\n\nposition\nartist\ncount\n\n\n\n\n0\n1\nEslabon Armado\n31\n\n\n1\n1\nBad Bunny\n2\n\n\n2\n1\nBizarrap & Peso Pluma\n2\n\n\n3\n1\nBTS\n1\n\n\n\n\n\n\n\n\nmax_count_index = world_df_with_rank_counts[world_df_with_rank_counts['position'] ==1]['count'].idxmax() # new : idxmax()\n\n\nworld_df_with_rank_counts.loc[max_count_index].to_frame().T #Revisiting .loc\n\n\n\n\n\n\n\n\nposition\nartist\ncount\n\n\n\n\n0\n1\nEslabon Armado\n31\n\n\n\n\n\n\n\n\n# for easier view : **Q: For each position (1 to 50), which artist was in that rank the maximum number of days** ( in total , needn't be consecutive)\n\nlist_of_dataframes = []\nfor position in range(1,51):\n    max_count_index = world_df_with_rank_counts[world_df_with_rank_counts['position'] ==position]['count'].idxmax()\n    list_of_dataframes.append(world_df_with_rank_counts.loc[max_count_index].to_frame().T )\n\n\nlist_of_dataframes[10]\n\n\n\n\n\n\n\n\nposition\nartist\ncount\n\n\n\n\n69\n11\nSZA\n9\n\n\n\n\n\n\n\n\n\n\nmerged_df = pd.concat(list_of_dataframes)\nmerged_df\n\n\n\n\n\n\n\n\nposition\nartist\ncount\n\n\n\n\n0\n1\nEslabon Armado\n31\n\n\n4\n2\nBizarrap & Peso Pluma\n16\n\n\n9\n3\nBad Bunny\n18\n\n\n14\n4\nYng Lvcas & Peso Pluma\n13\n\n\n20\n5\nGrupo Frontera & Bad Bunny\n11\n\n\n26\n6\nMiley Cyrus\n19\n\n\n33\n7\nDavid Kushner\n10\n\n\n41\n8\nSZA\n9\n\n\n50\n9\nHarry Styles\n14\n\n\n59\n10\nDavid Kushner\n8\n\n\n69\n11\nSZA\n9\n\n\n79\n12\nFuerza Regida\n7\n\n\n92\n13\nFeid & Young Miko\n11\n\n\n105\n14\nMetro Boomin\n12\n\n\n116\n15\nMetro Boomin\n5\n\n\n130\n16\nMetro Boomin\n8\n\n\n144\n17\nRema\n6\n\n\n163\n18\nRema\n5\n\n\n180\n19\nMetro Boomin\n7\n\n\n196\n20\nMetro Boomin\n8\n\n\n213\n21\nJunior H & Peso Pluma\n4\n\n\n232\n22\nPinkPantheress & Ice Spice\n7\n\n\n250\n23\nMetro Boomin\n5\n\n\n267\n24\nLana Del Rey\n4\n\n\n283\n25\nThe Weeknd\n6\n\n\n303\n26\nMorgan Wallen\n6\n\n\n323\n27\nMorgan Wallen\n5\n\n\n341\n28\nTaylor Swift\n8\n\n\n355\n29\nArctic Monkeys\n5\n\n\n373\n30\nThe Weeknd\n10\n\n\n388\n31\nThe Weeknd\n6\n\n\n406\n32\nTaylor Swift\n5\n\n\n425\n33\nd4vd\n4\n\n\n446\n34\nMetro Boomin\n4\n\n\n465\n35\nThe Weeknd\n6\n\n\n485\n36\nThe Weeknd\n6\n\n\n505\n37\nYandel\n4\n\n\n529\n38\nThe Weeknd\n7\n\n\n548\n39\nThe Weeknd\n7\n\n\n566\n40\nTom Odell\n5\n\n\n585\n41\nJVKE\n5\n\n\n602\n42\nLibianca\n6\n\n\n623\n43\nOneRepublic\n5\n\n\n643\n44\nLibianca\n6\n\n\n663\n45\nYahritza Y Su Esencia & Grupo Frontera\n4\n\n\n687\n46\nManuel Turizo\n4\n\n\n710\n47\nManuel Turizo\n4\n\n\n730\n48\nSam Smith\n4\n\n\n747\n49\nTaylor Swift\n4\n\n\n769\n50\nPeso Pluma\n4\n\n\n\n\n\n\n\n\n\n\n\nmerged_df.set_index(\"position\",drop = True,inplace = True)\n\n\ndel merged_df['count']\n\n\nmerged_df\n\n\n\n\n\n\n\n\nartist\n\n\nposition\n\n\n\n\n\n1\nEslabon Armado\n\n\n2\nBizarrap & Peso Pluma\n\n\n3\nBad Bunny\n\n\n4\nYng Lvcas & Peso Pluma\n\n\n5\nGrupo Frontera & Bad Bunny\n\n\n6\nMiley Cyrus\n\n\n7\nDavid Kushner\n\n\n8\nSZA\n\n\n9\nHarry Styles\n\n\n10\nDavid Kushner\n\n\n11\nSZA\n\n\n12\nFuerza Regida\n\n\n13\nFeid & Young Miko\n\n\n14\nMetro Boomin\n\n\n15\nMetro Boomin\n\n\n16\nMetro Boomin\n\n\n17\nRema\n\n\n18\nRema\n\n\n19\nMetro Boomin\n\n\n20\nMetro Boomin\n\n\n21\nJunior H & Peso Pluma\n\n\n22\nPinkPantheress & Ice Spice\n\n\n23\nMetro Boomin\n\n\n24\nLana Del Rey\n\n\n25\nThe Weeknd\n\n\n26\nMorgan Wallen\n\n\n27\nMorgan Wallen\n\n\n28\nTaylor Swift\n\n\n29\nArctic Monkeys\n\n\n30\nThe Weeknd\n\n\n31\nThe Weeknd\n\n\n32\nTaylor Swift\n\n\n33\nd4vd\n\n\n34\nMetro Boomin\n\n\n35\nThe Weeknd\n\n\n36\nThe Weeknd\n\n\n37\nYandel\n\n\n38\nThe Weeknd\n\n\n39\nThe Weeknd\n\n\n40\nTom Odell\n\n\n41\nJVKE\n\n\n42\nLibianca\n\n\n43\nOneRepublic\n\n\n44\nLibianca\n\n\n45\nYahritza Y Su Esencia & Grupo Frontera\n\n\n46\nManuel Turizo\n\n\n47\nManuel Turizo\n\n\n48\nSam Smith\n\n\n49\nTaylor Swift\n\n\n50\nPeso Pluma\n\n\n\n\n\n\n\nQ. How many times has an artist taken a spot between 1 and 50 ?\nFor each artist, how many times have they taken a spot between 1 and 50 (both inclusive)\n\nworld_df.sample()\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n1458\n2023-06-16\n9\nAs It Was\nHarry Styles\n93\n167303\nalbum\n13\n2022-05-20\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2732e8ed7...\n\n\n\n\n\n\n\n\nrank_count_df = world_df.groupby('artist')['position'].value_counts().to_frame()\nrank_count_df\n\n\n\n\n\n\n\n\n\nposition\n\n\nartist\nposition\n\n\n\n\n\nAlessandra\n42\n1\n\n\n47\n1\n\n\nArctic Monkeys\n29\n5\n\n\n25\n4\n\n\n22\n3\n\n\n...\n...\n...\n\n\nd4vd\n39\n1\n\n\n40\n1\n\n\n41\n1\n\n\n42\n1\n\n\n43\n1\n\n\n\n\n790 rows × 1 columns\n\n\n\n\nrank_count_df.rename({'position':'number of times'}, axis = 'columns',inplace = True)\n\n\nrank_count_df.reset_index(inplace = True)\n\n\nrank_count_df\n\n\n\n\n\n\n\n\nartist\nposition\nnumber of times\n\n\n\n\n0\nAlessandra\n42\n1\n\n\n1\nAlessandra\n47\n1\n\n\n2\nArctic Monkeys\n29\n5\n\n\n3\nArctic Monkeys\n25\n4\n\n\n4\nArctic Monkeys\n22\n3\n\n\n...\n...\n...\n...\n\n\n785\nd4vd\n39\n1\n\n\n786\nd4vd\n40\n1\n\n\n787\nd4vd\n41\n1\n\n\n788\nd4vd\n42\n1\n\n\n789\nd4vd\n43\n1\n\n\n\n\n790 rows × 3 columns\n\n\n\n\ntop_50_count_for_each_artist = rank_count_df.groupby('artist')['number of times'].sum().to_frame()\n\n\ntop_50_count_for_each_artist.sort_values(by = 'number of times', ascending=False)\n\n\n\n\n\n\n\n\nnumber of times\n\n\nartist\n\n\n\n\n\nThe Weeknd\n108\n\n\nTaylor Swift\n92\n\n\nMetro Boomin\n91\n\n\nMiley Cyrus\n72\n\n\nROSALÍA & Rauw Alejandro\n36\n\n\n...\n...\n\n\nJISOO\n1\n\n\nStray Kids\n1\n\n\nFast & Furious: The Fast Saga\n1\n\n\nSaiko & Feid & Quevedo\n1\n\n\nEladio Carrion\n1\n\n\n\n\n75 rows × 1 columns"
  },
  {
    "objectID": "Course_Content/Week_4&5/1/Notebook.html#joins",
    "href": "Course_Content/Week_4&5/1/Notebook.html#joins",
    "title": "Lecture 4.1",
    "section": "",
    "text": "(discussed in Lecture 3.3)\n\n\nworld_info = world_df[['date','position','artist','song']]\nworld_info.sample(3)\n\n\n\n\n\n\n\n\ndate\nposition\nartist\nsong\n\n\n\n\n1037\n2023-06-07\n38\nThe Weeknd\nDie For You\n\n\n557\n2023-05-29\n8\nSZA\nKill Bill\n\n\n975\n2023-06-06\n26\nFast & Furious: The Fast Saga & Jimin & BTS\nAngel Pt. 1 (feat. Jimin of BTS, JVKE & Muni L...\n\n\n\n\n\n\n\n\nworld_info.shape\n\n(1800, 4)\n\n\n\nworld_df['position'] = world_df['position'].astype('int')\n\n\nusa_info = usa_df[['date','position','artist','song']]\nusa_info.sample(3)\n\n\n\n\n\n\n\n\ndate\nposition\nartist\nsong\n\n\n\n\n1763\n2023-06-22\n14\nMetro Boomin\nAnnihilate (Spider-Man: Across the Spider-Vers...\n\n\n1785\n2023-06-22\n36\nMetro Boomin\nCreepin' (with The Weeknd & 21 Savage)\n\n\n120\n2023-05-20\n21\nPeso Pluma\nPor las Noches\n\n\n\n\n\n\n\n\nusa_info.shape\n\n(1800, 4)\n\n\n\nusa_info['position'] = usa_info['position'].astype('int')\n\n/var/folders/r0/bggl6hf15j708chpxqrx5tp00000gn/T/ipykernel_2958/3011614831.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  usa_info['position'] = usa_info['position'].astype('int')\n\n\n\nworld_info.merge(right=usa_info, on= ['date','artist','song'], how='inner') #default is inner\n\n\n\n\n\n\n\n\ndate\nposition_x\nartist\nsong\nposition_y\n\n\n\n\n0\n2023-05-18\n1\nEslabon Armado\nElla Baila Sola\n1\n\n\n1\n2023-05-18\n2\nGrupo Frontera & Bad Bunny\nun x100to\n4\n\n\n2\n2023-05-18\n3\nYng Lvcas & Peso Pluma\nLa Bebe - Remix\n8\n\n\n3\n2023-05-18\n4\nFIFTY FIFTY\nCupid - Twin Ver.\n7\n\n\n4\n2023-05-18\n5\nMiley Cyrus\nFlowers\n14\n\n\n...\n...\n...\n...\n...\n...\n\n\n983\n2023-06-22\n37\nJunior H & Peso Pluma\nEl Azul\n39\n\n\n984\n2023-06-22\n43\nKali Uchis\nMoonlight\n40\n\n\n985\n2023-06-22\n44\nPinkPantheress & Ice Spice\nBoy's a Liar Pt. 2\n18\n\n\n986\n2023-06-22\n48\nPeso Pluma & Natanael Cano\nPRC\n37\n\n\n987\n2023-06-22\n50\nLil Durk\nAll My Life (feat. J. Cole)\n8\n\n\n\n\n988 rows × 5 columns\n\n\n\n\n\n\n\nworld_info.merge(right=usa_info, on= ['date','artist','song'], how='inner', suffixes=['_world','_usa']) #default is inner\n\n\n\n\n\n\n\n\ndate\nposition_world\nartist\nsong\nposition_usa\n\n\n\n\n0\n2023-05-18\n1\nEslabon Armado\nElla Baila Sola\n1\n\n\n1\n2023-05-18\n2\nGrupo Frontera & Bad Bunny\nun x100to\n4\n\n\n2\n2023-05-18\n3\nYng Lvcas & Peso Pluma\nLa Bebe - Remix\n8\n\n\n3\n2023-05-18\n4\nFIFTY FIFTY\nCupid - Twin Ver.\n7\n\n\n4\n2023-05-18\n5\nMiley Cyrus\nFlowers\n14\n\n\n...\n...\n...\n...\n...\n...\n\n\n983\n2023-06-22\n37\nJunior H & Peso Pluma\nEl Azul\n39\n\n\n984\n2023-06-22\n43\nKali Uchis\nMoonlight\n40\n\n\n985\n2023-06-22\n44\nPinkPantheress & Ice Spice\nBoy's a Liar Pt. 2\n18\n\n\n986\n2023-06-22\n48\nPeso Pluma & Natanael Cano\nPRC\n37\n\n\n987\n2023-06-22\n50\nLil Durk\nAll My Life (feat. J. Cole)\n8\n\n\n\n\n988 rows × 5 columns\n\n\n\nOn a given date, a particular artist was on BOTH global and US spotify top 50. position_world gives their global spotify position, and position_usa gives their spotify top 50 in US position\n\n\n\n\n\nworld_info.merge(right=usa_info, on= ['date','artist','song'], how='outer', suffixes=['_world','_usa']) #default is inner\n\n\n\n\n\n\n\n\ndate\nposition_world\nartist\nsong\nposition_usa\n\n\n\n\n0\n2023-05-18\n1.0\nEslabon Armado\nElla Baila Sola\n1.0\n\n\n1\n2023-05-18\n2.0\nGrupo Frontera & Bad Bunny\nun x100to\n4.0\n\n\n2\n2023-05-18\n3.0\nYng Lvcas & Peso Pluma\nLa Bebe - Remix\n8.0\n\n\n3\n2023-05-18\n4.0\nFIFTY FIFTY\nCupid - Twin Ver.\n7.0\n\n\n4\n2023-05-18\n5.0\nMiley Cyrus\nFlowers\n14.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n2607\n2023-06-22\nNaN\nPeso Pluma\nPor las Noches\n45.0\n\n\n2608\n2023-06-22\nNaN\nNLE Choppa\nSLUT ME OUT\n47.0\n\n\n2609\n2023-06-22\nNaN\nDrake\nJimmy Cooks (feat. 21 Savage)\n48.0\n\n\n2610\n2023-06-22\nNaN\nKendrick Lamar\nMoney Trees\n49.0\n\n\n2611\n2023-06-22\nNaN\nSZA\nSnooze\n50.0\n\n\n\n\n2612 rows × 5 columns\n\n\n\nOn a given date, a particular artist was on EITHER global or US spotify top 50. position_world gives their global spotify position, and position_usa gives their spotify top 50 in US position\n\n\n\n\n\nworld_info.merge(right=usa_info, on= ['date','artist','song'], how='left', suffixes=['_world','_usa']) #default is inner\n\n\n\n\n\n\n\n\ndate\nposition_world\nartist\nsong\nposition_usa\n\n\n\n\n0\n2023-05-18\n1\nEslabon Armado\nElla Baila Sola\n1.0\n\n\n1\n2023-05-18\n2\nGrupo Frontera & Bad Bunny\nun x100to\n4.0\n\n\n2\n2023-05-18\n3\nYng Lvcas & Peso Pluma\nLa Bebe - Remix\n8.0\n\n\n3\n2023-05-18\n4\nFIFTY FIFTY\nCupid - Twin Ver.\n7.0\n\n\n4\n2023-05-18\n5\nMiley Cyrus\nFlowers\n14.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n1795\n2023-06-22\n46\nGlass Animals\nHeat Waves\nNaN\n\n\n1796\n2023-06-22\n47\nYahritza Y Su Esencia & Grupo Frontera\nFrágil\nNaN\n\n\n1797\n2023-06-22\n48\nPeso Pluma & Natanael Cano\nPRC\n37.0\n\n\n1798\n2023-06-22\n49\nDua Lipa\nDance The Night (From Barbie The Album)\nNaN\n\n\n1799\n2023-06-22\n50\nLil Durk\nAll My Life (feat. J. Cole)\n8.0\n\n\n\n\n1800 rows × 5 columns\n\n\n\nOn a given date, a particular artist was on global spotify top 50 list. If the same artist for the same song, on the same date, was also on USA top 50, then their rank is given in position_usa. If they were not in USA top 50, position_usa would be NaN( Not a Number)\n\n\n\n\n\nworld_info.merge(right=usa_info, on= ['date','artist','song'], how='right', suffixes=['_world','_usa']) #default is inner\n\n\n\n\n\n\n\n\ndate\nposition_world\nartist\nsong\nposition_usa\n\n\n\n\n0\n2023-05-18\n1.0\nEslabon Armado\nElla Baila Sola\n1\n\n\n1\n2023-05-18\n27.0\nMorgan Wallen\nLast Night\n2\n\n\n2\n2023-05-18\n26.0\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n3\n\n\n3\n2023-05-18\n2.0\nGrupo Frontera & Bad Bunny\nun x100to\n4\n\n\n4\n2023-05-18\n7.0\nSZA\nKill Bill\n5\n\n\n...\n...\n...\n...\n...\n...\n\n\n1795\n2023-06-22\n31.0\nThe Weeknd\nDie For You\n46\n\n\n1796\n2023-06-22\nNaN\nNLE Choppa\nSLUT ME OUT\n47\n\n\n1797\n2023-06-22\nNaN\nDrake\nJimmy Cooks (feat. 21 Savage)\n48\n\n\n1798\n2023-06-22\nNaN\nKendrick Lamar\nMoney Trees\n49\n\n\n1799\n2023-06-22\nNaN\nSZA\nSnooze\n50\n\n\n\n\n1800 rows × 5 columns\n\n\n\nOn a given date, a particular artist was on usa spotify top 50 list. If the same artist for the same song, on the same date, was also on Global top 50, then their rank is given in position_world. If they were not in Global top 50, position_world would be NaN( Not a Number)\n\n\n\n\n\nleft = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\nleft\n\n\n\n\n\n\n\n\ncol1\ncol2\n\n\n\n\n0\n1\n3\n\n\n1\n2\n4\n\n\n\n\n\n\n\n\nright = pd.DataFrame({'col3': [5, 6]}) \nright\n\n\n\n\n\n\n\n\ncol3\n\n\n\n\n0\n5\n\n\n1\n6\n\n\n\n\n\n\n\n\nleft.merge(right,how=\"cross\")\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n0\n1\n3\n5\n\n\n1\n1\n3\n6\n\n\n2\n2\n4\n5\n\n\n3\n2\n4\n6"
  },
  {
    "objectID": "Course_Content/Week_4&5/3/home.html",
    "href": "Course_Content/Week_4&5/3/home.html",
    "title": "4.3 - Data Preprocessing - Part 2",
    "section": "",
    "text": "Data Preprocessing - Text :\n\nCleaning Punct, Conversion to Lowercase\nTokenisation\nRemoving Stopwords\nLemmatisation\nStemming\nNltk Library\n\n\n\n\n\n\n\nKaggle - BBC Full Text and Category \nJupyter Notebook\n\n\n\n\n\n\n\n\n\n6.1 Data Visualisation"
  },
  {
    "objectID": "Course_Content/Week_4&5/3/home.html#objectives",
    "href": "Course_Content/Week_4&5/3/home.html#objectives",
    "title": "4.3 - Data Preprocessing - Part 2",
    "section": "",
    "text": "Data Preprocessing - Text :\n\nCleaning Punct, Conversion to Lowercase\nTokenisation\nRemoving Stopwords\nLemmatisation\nStemming\nNltk Library"
  },
  {
    "objectID": "Course_Content/Week_4&5/3/home.html#materials",
    "href": "Course_Content/Week_4&5/3/home.html#materials",
    "title": "4.3 - Data Preprocessing - Part 2",
    "section": "",
    "text": "Kaggle - BBC Full Text and Category \nJupyter Notebook"
  },
  {
    "objectID": "Course_Content/Week_4&5/3/home.html#next-class",
    "href": "Course_Content/Week_4&5/3/home.html#next-class",
    "title": "4.3 - Data Preprocessing - Part 2",
    "section": "",
    "text": "6.1 Data Visualisation"
  },
  {
    "objectID": "Course_Content/Week_4&5/3/Notebook_1.html",
    "href": "Course_Content/Week_4&5/3/Notebook_1.html",
    "title": "Lecture 4.3 (Part 1) : Data Preprocessing - Part 2",
    "section": "",
    "text": "dataset = https://www.kaggle.com/datasets/yufengdev/bbc-fulltext-and-category\n\nimport pandas as pd\n\n\narticles_df = pd.read_csv('data/bbc-text.csv')\n\n\narticles_df.sample()\n\n\n\n\n\n\n\n\ncategory\ntext\n\n\n\n\n301\npolitics\nfox attacks blair s tory lies tony blair lie...\n\n\n\n\n\n\n\nCategories\n\narticles_df['category'].unique()\n\narray(['tech', 'business', 'sport', 'entertainment', 'politics'],\n      dtype=object)\n\n\n\narticles_df['category'].value_counts().to_frame()\n\n\n\n\n\n\n\n\ncategory\n\n\n\n\nsport\n511\n\n\nbusiness\n510\n\n\npolitics\n417\n\n\ntech\n401\n\n\nentertainment\n386\n\n\n\n\n\n\n\n\narticles_df.sample()['text'].values.tolist()\n\n['children vote shrek 2 best film young uk film fans voted animated hollywood hit shrek 2 best film at the children s bafta awards on sunday.  more than 6 000 children voted in the only category chosen by fans. harry potter and the prisoner of azkaban  runner-up in the poll  was the choice of the bafta experts who named it best feature film. bbc one saturday morning show dick and dom in da bungalow won two awards - best entertainment and best presenters for richard mccourt and dominic wood.  former playschool presenter floella benjamin was awarded the special award for outstanding creative contribution to children s film and television. she first appeared on playschool 25 years ago and was made an obe in 2001 for services to broadcasting. south american-themed cartoon joko! jakamoko! toto! won the honour for pre-school animation and its writer tony collingwood for original writer. debbie isitt won the award for best adapted writer for her work with jacqueline wilson s the illustrated mum  which won the award for best schools drama.  schools  factual (primary) - thinking skills: think about it - hiding places  schools  factual (secondary) - in search of the tartan turban  pre-school live action - balamory  animation - brush head  drama - featherboy  factual - serious desert interactive bafta - king arthur international category - 8 simple rules for dating my teenage daughter']\n\n\nSometimes Language Detection is neccasary.\nCapitalization/ Lower case\n\narticles_df['lower_case'] = articles_df['text'].apply(str.lower)\n\n\narticles_df.sample()['lower_case'].values\n\narray(['surprise win for anti-bush film michael moore s anti-bush documentary fahrenheit 9/11 has won best film at the us people s choice awards  voted for by the us public.  mel gibson s the passion of the christ won best drama  despite both films being snubbed so far at us film awards in the run-up to february s oscars. julia roberts won her 10th consecutive crown as favourite female movie star. johnny depp was favourite male movie star and renee zellweger was favourite leading lady at sunday s awards in la.  film sequel shrek 2 took three prizes - voted top animated movie  top film comedy and top sequel. in television categories  desperate housewives was named top new drama and joey  starring former friends actor matt leblanc  was best new comedy. long-running shows will and grace and csi: crime scene investigation were named best tv comedy and tv drama respectively.  nominees for the people s choice awards were picked by a 6 000-strong entertainment weekly magazine panel  and winners were subsequently chosen by 21 million online voters. fahrenheit 9/11 director michael moore dedicated his trophy to soldiers in iraq. his film was highly critical of president george w bush and the us-led invasion of iraq  and moore was an outspoken bush critic in the 2004 presidential campaign inwhich democratic challenger john kerry lost.   this country is still all of ours  not right or left or democrat or republican   moore told the audience at the ceremony in pasadena  california. moore said it was  an historic occasion  that the 31-year-old awards ceremony would name a documentary its best film. unlike many other film-makers  passion of the christ director mel gibson has vowed not to campaign for an oscar for his movie.  to me  really  this is the ultimate goal because one doesn t make work for the elite   gibson said backstage at the event.  to me  the people have spoken.'],\n      dtype=object)\n\n\n\nReplace the Unicode character with equivalent ASCII character or remove them\nReplace the entity references with their actual symbols or removing HTML tags\nReplace the Typos, slang, acronyms or informal abbreviations - depend on different situations or main topics of the NLP such as finance or medical topics.\nList out all the hashtags/ usernames then replace with equivalent words\nReplace the emoticon/ emoji with equivalant word meaning such as “:)” with “smile” , or dropping emojis entirely.\nSpelling correction\n\nRemove punctuation\n\nimport string\n\n\nsample = \"hey! where are you!?\"\nsample_processed = sample.translate (str.maketrans ('', '', string.punctuation))\nsample_processed\n\n'hey where are you'\n\n\n\narticles_df['punct_removed'] = articles_df['lower_case'].apply(lambda doc : doc.translate (str.maketrans ('', '', string.punctuation)))                                                                  \n\n\narticles_df.sample()['punct_removed'].values \n\narray(['zambia confident and cautious zambia s technical director  kalusha bwalya is confident and cautious ahead of the cosafa cup final against angola on saturday in lusaka  bwalya said  nothing short of victory will do  however bwalya warned his side not to be too complacent  i don t want my team to be too comfortable or too sure of victory as it is going to be a difficult game  for me the main aim of the game is to enjoy and to win  zambia have shown their determination to win this final by recalling nine of their foreignbased players however the 41 yearold bwalya  who became the oldest player to appear in the competition when he played and scored against mauritius  is uncertain whether he will take to the field or not the chipolopolo fans however are not being so cautious with a  victory  concert already scheduled for after the match featuring some of the country s top musicians both sides are hoping to win the competition for a record third time  and so keep the trophy for good the chipolopolo won the first two editions of the regional tournament for southern african nations in 1997 and 1998 they were prevented from a third straight win by angola who knocked out the zambians at the semifinal stage in 1999 that victory for angola also marked a first defeat in 14 years for zambia at lusaka s independence stadium  where saturday s game is being played angola named just four overseasbased players in their preliminary squad the palancas negras have been unable to secure the release of many of their portugalbased players'],\n      dtype=object)\n\n\nTokenise\nA token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. Source\nie A token is a meaningful chunk of text that we use to process and understand the information in a document. It can be a word, a phrase, or even a symbol or punctuation mark. Tokens help us break down the text into smaller pieces so that we can analyze and work with it more easily.\n\nimport nltk \n\nnltk.download('punkt')\n\n\nfrom nltk.tokenize import (word_tokenize,\n                          sent_tokenize,\n                          TreebankWordTokenizer,\n                          wordpunct_tokenize,\n                          TweetTokenizer,\n                          MWETokenizer)\n\n\ntreebank_tokenizer = TreebankWordTokenizer()\nmwet_tokenizer= MWETokenizer()\ntweet_tokenizer= TweetTokenizer()\n\ntext=\"Mr. O'Neill thinks that the boys' stories about Chile's capital aren't amusing 🙃\"\ntext\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/enfageorge/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\"Mr. O'Neill thinks that the boys' stories about Chile's capital aren't amusing 🙃\"\n\n\nSource : Introduction to Information Retrieval By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze\n\nprint(\"Word_tokenise : \\n\", word_tokenize(text), \"\\nLength :\" , len(word_tokenize(text)))\nprint(\"\\nWord Punct Tokeniser :  \\n\", wordpunct_tokenize(text), \"\\nLength :\" , len(wordpunct_tokenize(text)))\nprint(\"\\nTree Bank : \\n\", treebank_tokenizer.tokenize(text), \"\\nLength :\" , len(treebank_tokenizer.tokenize(text)))\nprint(\"\\nTweet Tokeniser: \\n\", tweet_tokenizer.tokenize(text), \"\\nLength :\" , len(tweet_tokenizer.tokenize(text)))\nprint(\"\\nMWE Tokeniser: \\n\", mwet_tokenizer.tokenize(word_tokenize(text)), \"\\nLength :\" , len(mwet_tokenizer.tokenize(word_tokenize(text))))\n\nWord_tokenise : \n ['Mr.', \"O'Neill\", 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'s\", 'capital', 'are', \"n't\", 'amusing', '🙃'] \nLength : 16\n\nWord Punct Tokeniser :  \n ['Mr', '.', 'O', \"'\", 'Neill', 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'\", 's', 'capital', 'aren', \"'\", 't', 'amusing', '🙃'] \nLength : 21\n\nTree Bank : \n ['Mr.', \"O'Neill\", 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'s\", 'capital', 'are', \"n't\", 'amusing', '🙃'] \nLength : 16\n\nTweet Tokeniser: \n ['Mr', '.', \"O'Neill\", 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', \"Chile's\", 'capital', \"aren't\", 'amusing', '🙃'] \nLength : 15\n\nMWE Tokeniser: \n ['Mr.', \"O'Neill\", 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'s\", 'capital', 'are', \"n't\", 'amusing', '🙃'] \nLength : 16\n\n\n\nfrom nltk.tokenize import word_tokenize\n\n\narticles_df['tokenized'] = articles_df['punct_removed'].apply(word_tokenize)\n\n\narticles_df.sample()['tokenized'].values\n\narray([list(['jungle', 'tv', 'show', 'ratings', 'drop', 'by', '4m', 'the', 'finale', 'of', 'itv1', 's', 'i', 'm', 'a', 'celebrity', 'get', 'me', 'out', 'of', 'here', 'drew', 'an', 'average', 'of', '109m', 'viewers', 'about', 'four', 'million', 'fewer', 'than', 'the', 'previous', 'series', 'the', 'fourth', 'series', 'of', 'the', 'show', 'peaked', 'on', 'monday', 'at', '119m', 'and', '492', 'of', 'the', 'audience', 'just', 'before', 'joe', 'pasquale', 'won', 'this', 'compared', 'with', 'a', 'peak', 'of', '153m', 'at', 'and', 'a', 'record', '622', 'of', 'the', 'tv', 'audience', 'when', 'kerry', 'mcfadden', 'won', 'in', 'february', 'comic', 'pasquale', 'beat', 'former', 'royal', 'butler', 'paul', 'burrell', 'who', 'came', 'second', 'nightclub', 'owner', 'fran', 'cosgrave', 'who', 'was', 'third', 'pasquale', 'follows', 'kerry', 'mcfadden', 'phil', 'tufnell', 'and', 'tony', 'blackburn', 'as', 'winners', 'of', 'the', 'show', 'singer', 'and', 'tv', 'presenter', 'mcfadden', 'was', 'the', 'show', 's', 'first', 'female', 'winner', 'when', 'cricketer', 'phil', 'tufnell', 'won', 'in', 'may', '2003', '123', 'million', 'people', '50', 'of', 'the', 'viewing', 'public', 'tuned', 'in', 'to', 'watch', 'and', 'when', 'tony', 'blackburn', 'won', 'the', 'first', 'show', 'in', '2002', '109', 'million', 'people', 'saw', 'the', 'show', 'pasquale', 'had', 'been', 'the', 'show', 's', 'hottest', 'ever', 'favourite', 'to', 'win', 'and', 'its', 'hosts', 'anthony', 'mcpartlin', 'and', 'declan', 'donnelly', 'known', 'as', 'ant', 'and', 'dec', 'said', 'monday', 's', 'deciding', 'vote', 'was', 'the', 'closest', 'in', 'the', 'programme', 's', 'history', 'pascuale', 'has', 'been', 'flooded', 'with', 'offers', 'of', 'tv', 'work', 'according', 'to', 'his', 'management', 'company', 'but', 'one', 'of', 'his', 'first', 'jobs', 'on', 'his', 'return', 'is', 'pantomime', 'before', 'joining', 'i', 'm', 'a', 'celebrity', 'he', 'had', 'signed', 'up', 'to', 'play', 'jack', 'in', 'jack', 'and', 'the', 'beanstalk', 'in', 'birmingham', 'and', 'tickets', 'for', 'the', 'show', 'have', 'become', 'increasingly', 'popular', 'since', 'he', 'joined', 'the', 'tv', 'show', 'his', 'manager', 'robert', 'voice', 'said', 'we', 've', 'had', 'interest', 'from', 'different', 'tv', 'producers', 'some', 'are', 'for', 'comedy', 'shows', 'some', 'are', 'newtype', 'projects', 'there', 'are', 'a', 'number', 'of', 'things', 'joe', 'wants', 'to', 'do', 'he', 'is', 'very', 'ambitious', 'he', 'wants', 'to', 'play', 'the', 'west', 'end', 'and', 'do', 'different', 'things', 'other', 'than', 'straightforward', 'comedy', 'we', 'are', 'talking', 'to', 'a', 'couple', 'of', 'west', 'end', 'producers', 'about', 'a', 'musical'])],\n      dtype=object)\n\n\nRemove Stop Words (or/and Frequent words/ Rare words)\nStop words are the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”.\nIt can go wrong too.\n\nMark reported to the CEO : Mark reported CEO\nSuzanne reported as the CEO to the board : Suzanne reported CEO board\n\nIn your NLP pipeline, you might create 4-grams such as reported to the CEO and reported as the CEO. If you remove the stop words from the 4-grams, both examples would be reduced to “reported CEO”, and you would lack the information about the professional hierarchy. In the first example, Mark could have been an assistant to the CEO, whereas in the second example Suzanne was the CEO reporting to the board. Unfortunately, retaining the stop words within your pipeline creates another problem: it increases the length of the n-grams required to make use of these connections formed by the otherwise meaningless stop words. This issue forces us to retain at least 4-grams if you want to avoid the ambiguity of the human resources example. Designing a filter for stop words depends on your particular application.\nSource : https://www.manning.com/books/natural-language-processing-in-action\n\nnltk.download(\"stopwords\")\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/enfageorge/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nfrom nltk.corpus import stopwords\n\n\nstopwords_eng = set(stopwords.words('english'))\nprint(stopwords_eng)\n\n{'again', \"couldn't\", 'but', 'nor', 'he', 'herself', \"needn't\", 'yours', 'then', \"aren't\", 'me', 'isn', 'both', 'between', 'and', 'against', \"that'll\", \"haven't\", 'such', 'further', 'y', 'what', 'after', \"you've\", 'hers', \"hadn't\", 'on', 'about', 'were', 'most', 'haven', 'does', \"mightn't\", 'who', 'them', 'when', 'can', \"weren't\", 'ourselves', 'these', 'than', 'why', 'she', 'the', \"you'd\", 've', 'her', 'during', 'those', 'once', 'aren', 'himself', 'weren', 'same', 'of', 'too', 'while', 'only', 'will', \"hasn't\", 'do', 'any', \"isn't\", 'an', 'which', 'needn', 'below', 'now', 'themselves', 'very', \"shouldn't\", 'shouldn', 'you', \"doesn't\", 'did', 'doing', 'their', 'i', 'over', 'because', 'into', 'it', 'where', 'we', 'doesn', 'ma', 'each', 'at', 'mightn', 'mustn', \"wasn't\", 'how', 'ain', 'that', 'not', \"didn't\", 'other', 'they', \"she's\", 'have', \"won't\", 'out', 'being', 'own', 't', 'theirs', 're', 'll', 'wouldn', 'for', \"should've\", 'am', 'had', 'some', 'd', 'is', 'his', 'under', 'has', 'through', 'yourselves', 'are', 'up', 'more', 'off', 'just', 'a', 'above', 'been', 'so', 'this', 'itself', 'be', \"you'll\", 'all', 'o', 'should', 'was', 'before', 'from', 'don', 'my', 'whom', 'm', \"wouldn't\", 'yourself', 'won', 'couldn', 'your', 'having', 'there', 'hasn', 's', 'didn', 'its', 'until', 'in', 'our', \"it's\", 'to', \"mustn't\", 'by', \"don't\", 'ours', 'or', 'down', 'hadn', 'him', 'few', 'shan', \"shan't\", 'with', 'as', 'wasn', 'myself', 'if', 'no', 'here', \"you're\"}\n\n\n\narticles_df['stopwords_removed'] = articles_df['tokenized'].apply(lambda doc: [word for word in doc if word not in stopwords_eng])\n\n\narticles_df.sample()['stopwords_removed'].values \n\narray([list(['west', 'end', 'honour', 'finest', 'shows', 'west', 'end', 'honouring', 'finest', 'stars', 'shows', 'evening', 'standard', 'theatre', 'awards', 'london', 'monday', 'producers', 'starring', 'nathan', 'lane', 'lee', 'evans', 'best', 'musical', 'ceremony', 'national', 'theatre', 'competing', 'sweeney', 'todd', 'funny', 'thing', 'happened', 'way', 'forum', 'award', 'goat', 'sylvia', 'edward', 'albee', 'pillowman', 'martin', 'mcdonagh', 'alan', 'bennett', 'history', 'boys', 'shortlisted', 'best', 'play', 'category', 'pam', 'ferris', 'victoria', 'hamilton', 'kelly', 'reilly', 'nominated', 'best', 'actress', 'ferris', 'best', 'known', 'television', 'roles', 'programmes', 'darling', 'buds', 'may', 'made', 'shortlist', 'role', 'notes', 'falling', 'leaves', 'royal', 'court', 'theatre', 'meanwhile', 'richard', 'griffiths', 'plays', 'hector', 'history', 'boys', 'national', 'theatre', 'battle', 'best', 'actor', 'award', 'douglas', 'hodge', 'dumb', 'show', 'stanley', 'townsend', 'shining', 'city', 'best', 'director', 'shortlist', 'includes', 'luc', 'bondy', 'cruel', 'tender', 'simon', 'mcburney', 'measure', 'measure', 'rufus', 'norris', 'festen', 'festen', 'also', 'shortlisted', 'best', 'designer', 'category', 'ian', 'macneil', 'jean', 'kalman', 'paul', 'arditti', 'hildegard', 'bechtler', 'iphigenia', 'aulis', 'paul', 'brown', 'false', 'servant', 'milton', 'shulman', 'award', 'outstanding', 'newcomer', 'presented', 'dominic', 'cooper', 'dark', 'materials', 'history', 'boys', 'romola', 'garai', 'calico', 'eddie', 'redmayne', 'goat', 'sylvia', 'ben', 'wishaw', 'hamlet', 'playwrights', 'david', 'eldridge', 'rebecca', 'lenkiewicz', 'owen', 'mccafferty', 'fight', 'charles', 'wintour', 'award', '£30', '000', 'bursary', 'three', '50th', 'anniversary', 'special', 'awards', 'also', 'presented', 'institution', 'playwright', 'individual'])],\n      dtype=object)\n\n\n\nsample_df = articles_df.sample()\nprint(\"Length after tokenisation : \",len(sample_df['tokenized'].values[0]))\nprint(\"Length after stopwords are removed : \",len(sample_df['stopwords_removed'].values[0]))\n\nLength after tokenisation :  295\nLength after stopwords are removed :  140\n\n\nStemming\nThe process of removing affixes or changing affixes or getting to the root words in called stemming.\nEx :\n\nrun, running, runs =&gt; run\nprogramming, programmer ,programs =&gt; program\n\nThere are multiple stemming algorithms, but we will use Snowball (Porter2) Stemming Algorithm here.\n\nfrom nltk.stem import SnowballStemmer\n\nstemmer = nltk.PorterStemmer()\n\n\narticles_df['snowball_stemmer'] = articles_df['stopwords_removed'].apply(lambda doc :  [stemmer.stem(word) for word in doc])\n\n\nexample = articles_df.sample()[['stopwords_removed', 'snowball_stemmer']].values.tolist()\nprint(example[0][0])\n\n['blues', 'slam', 'blackburn', 'savage', 'birmingham', 'confirmed', 'blackburn', 'made', 'bid', 'robbie', 'savage', 'managing', 'director', 'karen', 'brady', 'called', 'derisory', 'rovers', 'reportedly', 'offered', '£500', '000', 'front', 'wales', 'star', '30', 'fee', 'rising', '£22m', 'brady', 'told', 'sun', 'bid', 'waste', 'fax', 'paper', 'time', 'added', 'way', 'things', 'going', 'could', 'affect', 'relationship', 'clubs', 'got', 'robbie', 'head', 'sale', 'savage', 'future', 'birmingham', 'source', 'speculation', 'several', 'weeks', 'fans', 'criticising', 'performances', 'club', 'earlier', 'season', 'however', 'good', 'displays', 'west', 'brom', 'aston', 'villa', 'impressed', 'blues', 'fans', 'crowd', 'gave', 'massive', 'standing', 'ovation', 'came', 'saturday', 'nice', 'said', 'fantastic', 'even', 'though', 'criticised', 'number', 'recent', 'weeks', 'saturday', 'showed', 'much', 'mean', 'say', 'transfer', 'rumours', 'two', 'clubs', 'created', 'speculation', 'phoned', 'every', 'national', 'newspaper', 'saying', 'blackburn', 'trying', 'buy', 'birmingham', 'manager', 'steve', 'bruce', 'insists', 'want', 'sell', 'savage', 'lot', 'said', 'written', 'sav', 'terrific', 'birmingham', 'city', 'last', 'two', 'half', 'years', 'said', 'fans', 'love', 'epitomises', 'works', 'hard', 'like', 'people', 'like', 'many', 'like', 'hell', 'sell', 'someone', 'else', 'interested']\n\n\n\nprint(example[0][1])\n\n['blue', 'slam', 'blackburn', 'savag', 'birmingham', 'confirm', 'blackburn', 'made', 'bid', 'robbi', 'savag', 'manag', 'director', 'karen', 'bradi', 'call', 'derisori', 'rover', 'reportedli', 'offer', '£500', '000', 'front', 'wale', 'star', '30', 'fee', 'rise', '£22m', 'bradi', 'told', 'sun', 'bid', 'wast', 'fax', 'paper', 'time', 'ad', 'way', 'thing', 'go', 'could', 'affect', 'relationship', 'club', 'got', 'robbi', 'head', 'sale', 'savag', 'futur', 'birmingham', 'sourc', 'specul', 'sever', 'week', 'fan', 'criticis', 'perform', 'club', 'earlier', 'season', 'howev', 'good', 'display', 'west', 'brom', 'aston', 'villa', 'impress', 'blue', 'fan', 'crowd', 'gave', 'massiv', 'stand', 'ovat', 'came', 'saturday', 'nice', 'said', 'fantast', 'even', 'though', 'criticis', 'number', 'recent', 'week', 'saturday', 'show', 'much', 'mean', 'say', 'transfer', 'rumour', 'two', 'club', 'creat', 'specul', 'phone', 'everi', 'nation', 'newspap', 'say', 'blackburn', 'tri', 'buy', 'birmingham', 'manag', 'steve', 'bruce', 'insist', 'want', 'sell', 'savag', 'lot', 'said', 'written', 'sav', 'terrif', 'birmingham', 'citi', 'last', 'two', 'half', 'year', 'said', 'fan', 'love', 'epitomis', 'work', 'hard', 'like', 'peopl', 'like', 'mani', 'like', 'hell', 'sell', 'someon', 'els', 'interest']\n\n\nLemmatisation\nLemma is the canonical form dictionary form, or citation form of a set of word forms.In English, for example, break, breaks, broke, broken and breaking are forms of the same lexeme, with break as the lemma by which they are indexed.\nSource : Wikipedia\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/enfageorge/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\nimport nltk\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nnltk.download(\"punkt\")\n\n# Initialize Python porter stemmer\nps = PorterStemmer()\nwnl = WordNetLemmatizer()\n\n# Example inflections to reduce\nexample_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n\n# Perform stemming\nprint(\"{0:20}{1:20}{2:20}\".format(\"--Word--\",\"--Stem--\", \"--Lemma--\"))\nfor word in example_words:\n   print (\"{0:20}{1:20}{2:20}\".format(word, ps.stem(word),wnl.lemmatize(word, pos='v')))\n\n--Word--            --Stem--            --Lemma--           \nprogram             program             program             \nprogramming         program             program             \nprogramer           program             programer           \nprograms            program             program             \nprogrammed          program             program             \n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/enfageorge/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nSource - DataCamp\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n\narticles_df['text lemma'] = articles_df['stopwords_removed'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 150)\n\n\narticles_df.sample()\n\n\n\n\n\n\n\n\ncategory\ntext\nlower_case\npunct_removed\ntokenized\nstopwords_removed\nsnowball_stemmer\ntext lemma\n\n\n\n\n902\ntech\nmobiles not media players yet mobiles are not yet ready to be all-singing all-dancing multimedia devices which will replace portable media players say two reports. despite moves to bring music download services to mobiles people do not want to trade multimedia services with size and battery life said jupiter. a separate study by gartner has also said real-time tv broadcasts to mobiles is unlikely in europe until 2007. technical issues and standards must be resolved first said the r...\nmobiles not media players yet mobiles are not yet ready to be all-singing all-dancing multimedia devices which will replace portable media players say two reports. despite moves to bring music download services to mobiles people do not want to trade multimedia services with size and battery life said jupiter. a separate study by gartner has also said real-time tv broadcasts to mobiles is unlikely in europe until 2007. technical issues and standards must be resolved first said the r...\nmobiles not media players yet mobiles are not yet ready to be allsinging alldancing multimedia devices which will replace portable media players say two reports despite moves to bring music download services to mobiles people do not want to trade multimedia services with size and battery life said jupiter a separate study by gartner has also said realtime tv broadcasts to mobiles is unlikely in europe until 2007 technical issues and standards must be resolved first said the report ...\n[mobiles, not, media, players, yet, mobiles, are, not, yet, ready, to, be, allsinging, alldancing, multimedia, devices, which, will, replace, portable, media, players, say, two, reports, despite, moves, to, bring, music, download, services, to, mobiles, people, do, not, want, to, trade, multimedia, services, with, size, and, battery, life, said, jupiter, a, separate, study, by, gartner, has, also, said, realtime, tv, broadcasts, to, mobiles, is, unlikely, in, europe, until, 2007, technical, ...\n[mobiles, media, players, yet, mobiles, yet, ready, allsinging, alldancing, multimedia, devices, replace, portable, media, players, say, two, reports, despite, moves, bring, music, download, services, mobiles, people, want, trade, multimedia, services, size, battery, life, said, jupiter, separate, study, gartner, also, said, realtime, tv, broadcasts, mobiles, unlikely, europe, 2007, technical, issues, standards, must, resolved, first, said, report, batteries, already, cope, services, operato...\n[mobil, media, player, yet, mobil, yet, readi, allsing, alldanc, multimedia, devic, replac, portabl, media, player, say, two, report, despit, move, bring, music, download, servic, mobil, peopl, want, trade, multimedia, servic, size, batteri, life, said, jupit, separ, studi, gartner, also, said, realtim, tv, broadcast, mobil, unlik, europ, 2007, technic, issu, standard, must, resolv, first, said, report, batteri, alreadi, cope, servic, oper, offer, like, video, playback, video, messag, megapi...\n[mobile, medium, player, yet, mobile, yet, ready, allsinging, alldancing, multimedia, device, replace, portable, medium, player, say, two, report, despite, move, bring, music, download, service, mobile, people, want, trade, multimedia, service, size, battery, life, said, jupiter, separate, study, gartner, also, said, realtime, tv, broadcast, mobile, unlikely, europe, 2007, technical, issue, standard, must, resolved, first, said, report, battery, already, cope, service, operator, offer, like,..."
  },
  {
    "objectID": "Course_Content/Week_4&5/3/Notebook_1.html#data-preprocessing---text",
    "href": "Course_Content/Week_4&5/3/Notebook_1.html#data-preprocessing---text",
    "title": "Lecture 4.3 (Part 1) : Data Preprocessing - Part 2",
    "section": "",
    "text": "dataset = https://www.kaggle.com/datasets/yufengdev/bbc-fulltext-and-category\n\nimport pandas as pd\n\n\narticles_df = pd.read_csv('data/bbc-text.csv')\n\n\narticles_df.sample()\n\n\n\n\n\n\n\n\ncategory\ntext\n\n\n\n\n301\npolitics\nfox attacks blair s tory lies tony blair lie...\n\n\n\n\n\n\n\nCategories\n\narticles_df['category'].unique()\n\narray(['tech', 'business', 'sport', 'entertainment', 'politics'],\n      dtype=object)\n\n\n\narticles_df['category'].value_counts().to_frame()\n\n\n\n\n\n\n\n\ncategory\n\n\n\n\nsport\n511\n\n\nbusiness\n510\n\n\npolitics\n417\n\n\ntech\n401\n\n\nentertainment\n386\n\n\n\n\n\n\n\n\narticles_df.sample()['text'].values.tolist()\n\n['children vote shrek 2 best film young uk film fans voted animated hollywood hit shrek 2 best film at the children s bafta awards on sunday.  more than 6 000 children voted in the only category chosen by fans. harry potter and the prisoner of azkaban  runner-up in the poll  was the choice of the bafta experts who named it best feature film. bbc one saturday morning show dick and dom in da bungalow won two awards - best entertainment and best presenters for richard mccourt and dominic wood.  former playschool presenter floella benjamin was awarded the special award for outstanding creative contribution to children s film and television. she first appeared on playschool 25 years ago and was made an obe in 2001 for services to broadcasting. south american-themed cartoon joko! jakamoko! toto! won the honour for pre-school animation and its writer tony collingwood for original writer. debbie isitt won the award for best adapted writer for her work with jacqueline wilson s the illustrated mum  which won the award for best schools drama.  schools  factual (primary) - thinking skills: think about it - hiding places  schools  factual (secondary) - in search of the tartan turban  pre-school live action - balamory  animation - brush head  drama - featherboy  factual - serious desert interactive bafta - king arthur international category - 8 simple rules for dating my teenage daughter']\n\n\nSometimes Language Detection is neccasary.\nCapitalization/ Lower case\n\narticles_df['lower_case'] = articles_df['text'].apply(str.lower)\n\n\narticles_df.sample()['lower_case'].values\n\narray(['surprise win for anti-bush film michael moore s anti-bush documentary fahrenheit 9/11 has won best film at the us people s choice awards  voted for by the us public.  mel gibson s the passion of the christ won best drama  despite both films being snubbed so far at us film awards in the run-up to february s oscars. julia roberts won her 10th consecutive crown as favourite female movie star. johnny depp was favourite male movie star and renee zellweger was favourite leading lady at sunday s awards in la.  film sequel shrek 2 took three prizes - voted top animated movie  top film comedy and top sequel. in television categories  desperate housewives was named top new drama and joey  starring former friends actor matt leblanc  was best new comedy. long-running shows will and grace and csi: crime scene investigation were named best tv comedy and tv drama respectively.  nominees for the people s choice awards were picked by a 6 000-strong entertainment weekly magazine panel  and winners were subsequently chosen by 21 million online voters. fahrenheit 9/11 director michael moore dedicated his trophy to soldiers in iraq. his film was highly critical of president george w bush and the us-led invasion of iraq  and moore was an outspoken bush critic in the 2004 presidential campaign inwhich democratic challenger john kerry lost.   this country is still all of ours  not right or left or democrat or republican   moore told the audience at the ceremony in pasadena  california. moore said it was  an historic occasion  that the 31-year-old awards ceremony would name a documentary its best film. unlike many other film-makers  passion of the christ director mel gibson has vowed not to campaign for an oscar for his movie.  to me  really  this is the ultimate goal because one doesn t make work for the elite   gibson said backstage at the event.  to me  the people have spoken.'],\n      dtype=object)\n\n\n\nReplace the Unicode character with equivalent ASCII character or remove them\nReplace the entity references with their actual symbols or removing HTML tags\nReplace the Typos, slang, acronyms or informal abbreviations - depend on different situations or main topics of the NLP such as finance or medical topics.\nList out all the hashtags/ usernames then replace with equivalent words\nReplace the emoticon/ emoji with equivalant word meaning such as “:)” with “smile” , or dropping emojis entirely.\nSpelling correction\n\nRemove punctuation\n\nimport string\n\n\nsample = \"hey! where are you!?\"\nsample_processed = sample.translate (str.maketrans ('', '', string.punctuation))\nsample_processed\n\n'hey where are you'\n\n\n\narticles_df['punct_removed'] = articles_df['lower_case'].apply(lambda doc : doc.translate (str.maketrans ('', '', string.punctuation)))                                                                  \n\n\narticles_df.sample()['punct_removed'].values \n\narray(['zambia confident and cautious zambia s technical director  kalusha bwalya is confident and cautious ahead of the cosafa cup final against angola on saturday in lusaka  bwalya said  nothing short of victory will do  however bwalya warned his side not to be too complacent  i don t want my team to be too comfortable or too sure of victory as it is going to be a difficult game  for me the main aim of the game is to enjoy and to win  zambia have shown their determination to win this final by recalling nine of their foreignbased players however the 41 yearold bwalya  who became the oldest player to appear in the competition when he played and scored against mauritius  is uncertain whether he will take to the field or not the chipolopolo fans however are not being so cautious with a  victory  concert already scheduled for after the match featuring some of the country s top musicians both sides are hoping to win the competition for a record third time  and so keep the trophy for good the chipolopolo won the first two editions of the regional tournament for southern african nations in 1997 and 1998 they were prevented from a third straight win by angola who knocked out the zambians at the semifinal stage in 1999 that victory for angola also marked a first defeat in 14 years for zambia at lusaka s independence stadium  where saturday s game is being played angola named just four overseasbased players in their preliminary squad the palancas negras have been unable to secure the release of many of their portugalbased players'],\n      dtype=object)\n\n\nTokenise\nA token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. Source\nie A token is a meaningful chunk of text that we use to process and understand the information in a document. It can be a word, a phrase, or even a symbol or punctuation mark. Tokens help us break down the text into smaller pieces so that we can analyze and work with it more easily.\n\nimport nltk \n\nnltk.download('punkt')\n\n\nfrom nltk.tokenize import (word_tokenize,\n                          sent_tokenize,\n                          TreebankWordTokenizer,\n                          wordpunct_tokenize,\n                          TweetTokenizer,\n                          MWETokenizer)\n\n\ntreebank_tokenizer = TreebankWordTokenizer()\nmwet_tokenizer= MWETokenizer()\ntweet_tokenizer= TweetTokenizer()\n\ntext=\"Mr. O'Neill thinks that the boys' stories about Chile's capital aren't amusing 🙃\"\ntext\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/enfageorge/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\"Mr. O'Neill thinks that the boys' stories about Chile's capital aren't amusing 🙃\"\n\n\nSource : Introduction to Information Retrieval By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze\n\nprint(\"Word_tokenise : \\n\", word_tokenize(text), \"\\nLength :\" , len(word_tokenize(text)))\nprint(\"\\nWord Punct Tokeniser :  \\n\", wordpunct_tokenize(text), \"\\nLength :\" , len(wordpunct_tokenize(text)))\nprint(\"\\nTree Bank : \\n\", treebank_tokenizer.tokenize(text), \"\\nLength :\" , len(treebank_tokenizer.tokenize(text)))\nprint(\"\\nTweet Tokeniser: \\n\", tweet_tokenizer.tokenize(text), \"\\nLength :\" , len(tweet_tokenizer.tokenize(text)))\nprint(\"\\nMWE Tokeniser: \\n\", mwet_tokenizer.tokenize(word_tokenize(text)), \"\\nLength :\" , len(mwet_tokenizer.tokenize(word_tokenize(text))))\n\nWord_tokenise : \n ['Mr.', \"O'Neill\", 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'s\", 'capital', 'are', \"n't\", 'amusing', '🙃'] \nLength : 16\n\nWord Punct Tokeniser :  \n ['Mr', '.', 'O', \"'\", 'Neill', 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'\", 's', 'capital', 'aren', \"'\", 't', 'amusing', '🙃'] \nLength : 21\n\nTree Bank : \n ['Mr.', \"O'Neill\", 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'s\", 'capital', 'are', \"n't\", 'amusing', '🙃'] \nLength : 16\n\nTweet Tokeniser: \n ['Mr', '.', \"O'Neill\", 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', \"Chile's\", 'capital', \"aren't\", 'amusing', '🙃'] \nLength : 15\n\nMWE Tokeniser: \n ['Mr.', \"O'Neill\", 'thinks', 'that', 'the', 'boys', \"'\", 'stories', 'about', 'Chile', \"'s\", 'capital', 'are', \"n't\", 'amusing', '🙃'] \nLength : 16\n\n\n\nfrom nltk.tokenize import word_tokenize\n\n\narticles_df['tokenized'] = articles_df['punct_removed'].apply(word_tokenize)\n\n\narticles_df.sample()['tokenized'].values\n\narray([list(['jungle', 'tv', 'show', 'ratings', 'drop', 'by', '4m', 'the', 'finale', 'of', 'itv1', 's', 'i', 'm', 'a', 'celebrity', 'get', 'me', 'out', 'of', 'here', 'drew', 'an', 'average', 'of', '109m', 'viewers', 'about', 'four', 'million', 'fewer', 'than', 'the', 'previous', 'series', 'the', 'fourth', 'series', 'of', 'the', 'show', 'peaked', 'on', 'monday', 'at', '119m', 'and', '492', 'of', 'the', 'audience', 'just', 'before', 'joe', 'pasquale', 'won', 'this', 'compared', 'with', 'a', 'peak', 'of', '153m', 'at', 'and', 'a', 'record', '622', 'of', 'the', 'tv', 'audience', 'when', 'kerry', 'mcfadden', 'won', 'in', 'february', 'comic', 'pasquale', 'beat', 'former', 'royal', 'butler', 'paul', 'burrell', 'who', 'came', 'second', 'nightclub', 'owner', 'fran', 'cosgrave', 'who', 'was', 'third', 'pasquale', 'follows', 'kerry', 'mcfadden', 'phil', 'tufnell', 'and', 'tony', 'blackburn', 'as', 'winners', 'of', 'the', 'show', 'singer', 'and', 'tv', 'presenter', 'mcfadden', 'was', 'the', 'show', 's', 'first', 'female', 'winner', 'when', 'cricketer', 'phil', 'tufnell', 'won', 'in', 'may', '2003', '123', 'million', 'people', '50', 'of', 'the', 'viewing', 'public', 'tuned', 'in', 'to', 'watch', 'and', 'when', 'tony', 'blackburn', 'won', 'the', 'first', 'show', 'in', '2002', '109', 'million', 'people', 'saw', 'the', 'show', 'pasquale', 'had', 'been', 'the', 'show', 's', 'hottest', 'ever', 'favourite', 'to', 'win', 'and', 'its', 'hosts', 'anthony', 'mcpartlin', 'and', 'declan', 'donnelly', 'known', 'as', 'ant', 'and', 'dec', 'said', 'monday', 's', 'deciding', 'vote', 'was', 'the', 'closest', 'in', 'the', 'programme', 's', 'history', 'pascuale', 'has', 'been', 'flooded', 'with', 'offers', 'of', 'tv', 'work', 'according', 'to', 'his', 'management', 'company', 'but', 'one', 'of', 'his', 'first', 'jobs', 'on', 'his', 'return', 'is', 'pantomime', 'before', 'joining', 'i', 'm', 'a', 'celebrity', 'he', 'had', 'signed', 'up', 'to', 'play', 'jack', 'in', 'jack', 'and', 'the', 'beanstalk', 'in', 'birmingham', 'and', 'tickets', 'for', 'the', 'show', 'have', 'become', 'increasingly', 'popular', 'since', 'he', 'joined', 'the', 'tv', 'show', 'his', 'manager', 'robert', 'voice', 'said', 'we', 've', 'had', 'interest', 'from', 'different', 'tv', 'producers', 'some', 'are', 'for', 'comedy', 'shows', 'some', 'are', 'newtype', 'projects', 'there', 'are', 'a', 'number', 'of', 'things', 'joe', 'wants', 'to', 'do', 'he', 'is', 'very', 'ambitious', 'he', 'wants', 'to', 'play', 'the', 'west', 'end', 'and', 'do', 'different', 'things', 'other', 'than', 'straightforward', 'comedy', 'we', 'are', 'talking', 'to', 'a', 'couple', 'of', 'west', 'end', 'producers', 'about', 'a', 'musical'])],\n      dtype=object)\n\n\nRemove Stop Words (or/and Frequent words/ Rare words)\nStop words are the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”.\nIt can go wrong too.\n\nMark reported to the CEO : Mark reported CEO\nSuzanne reported as the CEO to the board : Suzanne reported CEO board\n\nIn your NLP pipeline, you might create 4-grams such as reported to the CEO and reported as the CEO. If you remove the stop words from the 4-grams, both examples would be reduced to “reported CEO”, and you would lack the information about the professional hierarchy. In the first example, Mark could have been an assistant to the CEO, whereas in the second example Suzanne was the CEO reporting to the board. Unfortunately, retaining the stop words within your pipeline creates another problem: it increases the length of the n-grams required to make use of these connections formed by the otherwise meaningless stop words. This issue forces us to retain at least 4-grams if you want to avoid the ambiguity of the human resources example. Designing a filter for stop words depends on your particular application.\nSource : https://www.manning.com/books/natural-language-processing-in-action\n\nnltk.download(\"stopwords\")\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/enfageorge/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nfrom nltk.corpus import stopwords\n\n\nstopwords_eng = set(stopwords.words('english'))\nprint(stopwords_eng)\n\n{'again', \"couldn't\", 'but', 'nor', 'he', 'herself', \"needn't\", 'yours', 'then', \"aren't\", 'me', 'isn', 'both', 'between', 'and', 'against', \"that'll\", \"haven't\", 'such', 'further', 'y', 'what', 'after', \"you've\", 'hers', \"hadn't\", 'on', 'about', 'were', 'most', 'haven', 'does', \"mightn't\", 'who', 'them', 'when', 'can', \"weren't\", 'ourselves', 'these', 'than', 'why', 'she', 'the', \"you'd\", 've', 'her', 'during', 'those', 'once', 'aren', 'himself', 'weren', 'same', 'of', 'too', 'while', 'only', 'will', \"hasn't\", 'do', 'any', \"isn't\", 'an', 'which', 'needn', 'below', 'now', 'themselves', 'very', \"shouldn't\", 'shouldn', 'you', \"doesn't\", 'did', 'doing', 'their', 'i', 'over', 'because', 'into', 'it', 'where', 'we', 'doesn', 'ma', 'each', 'at', 'mightn', 'mustn', \"wasn't\", 'how', 'ain', 'that', 'not', \"didn't\", 'other', 'they', \"she's\", 'have', \"won't\", 'out', 'being', 'own', 't', 'theirs', 're', 'll', 'wouldn', 'for', \"should've\", 'am', 'had', 'some', 'd', 'is', 'his', 'under', 'has', 'through', 'yourselves', 'are', 'up', 'more', 'off', 'just', 'a', 'above', 'been', 'so', 'this', 'itself', 'be', \"you'll\", 'all', 'o', 'should', 'was', 'before', 'from', 'don', 'my', 'whom', 'm', \"wouldn't\", 'yourself', 'won', 'couldn', 'your', 'having', 'there', 'hasn', 's', 'didn', 'its', 'until', 'in', 'our', \"it's\", 'to', \"mustn't\", 'by', \"don't\", 'ours', 'or', 'down', 'hadn', 'him', 'few', 'shan', \"shan't\", 'with', 'as', 'wasn', 'myself', 'if', 'no', 'here', \"you're\"}\n\n\n\narticles_df['stopwords_removed'] = articles_df['tokenized'].apply(lambda doc: [word for word in doc if word not in stopwords_eng])\n\n\narticles_df.sample()['stopwords_removed'].values \n\narray([list(['west', 'end', 'honour', 'finest', 'shows', 'west', 'end', 'honouring', 'finest', 'stars', 'shows', 'evening', 'standard', 'theatre', 'awards', 'london', 'monday', 'producers', 'starring', 'nathan', 'lane', 'lee', 'evans', 'best', 'musical', 'ceremony', 'national', 'theatre', 'competing', 'sweeney', 'todd', 'funny', 'thing', 'happened', 'way', 'forum', 'award', 'goat', 'sylvia', 'edward', 'albee', 'pillowman', 'martin', 'mcdonagh', 'alan', 'bennett', 'history', 'boys', 'shortlisted', 'best', 'play', 'category', 'pam', 'ferris', 'victoria', 'hamilton', 'kelly', 'reilly', 'nominated', 'best', 'actress', 'ferris', 'best', 'known', 'television', 'roles', 'programmes', 'darling', 'buds', 'may', 'made', 'shortlist', 'role', 'notes', 'falling', 'leaves', 'royal', 'court', 'theatre', 'meanwhile', 'richard', 'griffiths', 'plays', 'hector', 'history', 'boys', 'national', 'theatre', 'battle', 'best', 'actor', 'award', 'douglas', 'hodge', 'dumb', 'show', 'stanley', 'townsend', 'shining', 'city', 'best', 'director', 'shortlist', 'includes', 'luc', 'bondy', 'cruel', 'tender', 'simon', 'mcburney', 'measure', 'measure', 'rufus', 'norris', 'festen', 'festen', 'also', 'shortlisted', 'best', 'designer', 'category', 'ian', 'macneil', 'jean', 'kalman', 'paul', 'arditti', 'hildegard', 'bechtler', 'iphigenia', 'aulis', 'paul', 'brown', 'false', 'servant', 'milton', 'shulman', 'award', 'outstanding', 'newcomer', 'presented', 'dominic', 'cooper', 'dark', 'materials', 'history', 'boys', 'romola', 'garai', 'calico', 'eddie', 'redmayne', 'goat', 'sylvia', 'ben', 'wishaw', 'hamlet', 'playwrights', 'david', 'eldridge', 'rebecca', 'lenkiewicz', 'owen', 'mccafferty', 'fight', 'charles', 'wintour', 'award', '£30', '000', 'bursary', 'three', '50th', 'anniversary', 'special', 'awards', 'also', 'presented', 'institution', 'playwright', 'individual'])],\n      dtype=object)\n\n\n\nsample_df = articles_df.sample()\nprint(\"Length after tokenisation : \",len(sample_df['tokenized'].values[0]))\nprint(\"Length after stopwords are removed : \",len(sample_df['stopwords_removed'].values[0]))\n\nLength after tokenisation :  295\nLength after stopwords are removed :  140\n\n\nStemming\nThe process of removing affixes or changing affixes or getting to the root words in called stemming.\nEx :\n\nrun, running, runs =&gt; run\nprogramming, programmer ,programs =&gt; program\n\nThere are multiple stemming algorithms, but we will use Snowball (Porter2) Stemming Algorithm here.\n\nfrom nltk.stem import SnowballStemmer\n\nstemmer = nltk.PorterStemmer()\n\n\narticles_df['snowball_stemmer'] = articles_df['stopwords_removed'].apply(lambda doc :  [stemmer.stem(word) for word in doc])\n\n\nexample = articles_df.sample()[['stopwords_removed', 'snowball_stemmer']].values.tolist()\nprint(example[0][0])\n\n['blues', 'slam', 'blackburn', 'savage', 'birmingham', 'confirmed', 'blackburn', 'made', 'bid', 'robbie', 'savage', 'managing', 'director', 'karen', 'brady', 'called', 'derisory', 'rovers', 'reportedly', 'offered', '£500', '000', 'front', 'wales', 'star', '30', 'fee', 'rising', '£22m', 'brady', 'told', 'sun', 'bid', 'waste', 'fax', 'paper', 'time', 'added', 'way', 'things', 'going', 'could', 'affect', 'relationship', 'clubs', 'got', 'robbie', 'head', 'sale', 'savage', 'future', 'birmingham', 'source', 'speculation', 'several', 'weeks', 'fans', 'criticising', 'performances', 'club', 'earlier', 'season', 'however', 'good', 'displays', 'west', 'brom', 'aston', 'villa', 'impressed', 'blues', 'fans', 'crowd', 'gave', 'massive', 'standing', 'ovation', 'came', 'saturday', 'nice', 'said', 'fantastic', 'even', 'though', 'criticised', 'number', 'recent', 'weeks', 'saturday', 'showed', 'much', 'mean', 'say', 'transfer', 'rumours', 'two', 'clubs', 'created', 'speculation', 'phoned', 'every', 'national', 'newspaper', 'saying', 'blackburn', 'trying', 'buy', 'birmingham', 'manager', 'steve', 'bruce', 'insists', 'want', 'sell', 'savage', 'lot', 'said', 'written', 'sav', 'terrific', 'birmingham', 'city', 'last', 'two', 'half', 'years', 'said', 'fans', 'love', 'epitomises', 'works', 'hard', 'like', 'people', 'like', 'many', 'like', 'hell', 'sell', 'someone', 'else', 'interested']\n\n\n\nprint(example[0][1])\n\n['blue', 'slam', 'blackburn', 'savag', 'birmingham', 'confirm', 'blackburn', 'made', 'bid', 'robbi', 'savag', 'manag', 'director', 'karen', 'bradi', 'call', 'derisori', 'rover', 'reportedli', 'offer', '£500', '000', 'front', 'wale', 'star', '30', 'fee', 'rise', '£22m', 'bradi', 'told', 'sun', 'bid', 'wast', 'fax', 'paper', 'time', 'ad', 'way', 'thing', 'go', 'could', 'affect', 'relationship', 'club', 'got', 'robbi', 'head', 'sale', 'savag', 'futur', 'birmingham', 'sourc', 'specul', 'sever', 'week', 'fan', 'criticis', 'perform', 'club', 'earlier', 'season', 'howev', 'good', 'display', 'west', 'brom', 'aston', 'villa', 'impress', 'blue', 'fan', 'crowd', 'gave', 'massiv', 'stand', 'ovat', 'came', 'saturday', 'nice', 'said', 'fantast', 'even', 'though', 'criticis', 'number', 'recent', 'week', 'saturday', 'show', 'much', 'mean', 'say', 'transfer', 'rumour', 'two', 'club', 'creat', 'specul', 'phone', 'everi', 'nation', 'newspap', 'say', 'blackburn', 'tri', 'buy', 'birmingham', 'manag', 'steve', 'bruce', 'insist', 'want', 'sell', 'savag', 'lot', 'said', 'written', 'sav', 'terrif', 'birmingham', 'citi', 'last', 'two', 'half', 'year', 'said', 'fan', 'love', 'epitomis', 'work', 'hard', 'like', 'peopl', 'like', 'mani', 'like', 'hell', 'sell', 'someon', 'els', 'interest']\n\n\nLemmatisation\nLemma is the canonical form dictionary form, or citation form of a set of word forms.In English, for example, break, breaks, broke, broken and breaking are forms of the same lexeme, with break as the lemma by which they are indexed.\nSource : Wikipedia\n\nnltk.download('wordnet')\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/enfageorge/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue\n\n\n\nimport nltk\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nnltk.download(\"punkt\")\n\n# Initialize Python porter stemmer\nps = PorterStemmer()\nwnl = WordNetLemmatizer()\n\n# Example inflections to reduce\nexample_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n\n# Perform stemming\nprint(\"{0:20}{1:20}{2:20}\".format(\"--Word--\",\"--Stem--\", \"--Lemma--\"))\nfor word in example_words:\n   print (\"{0:20}{1:20}{2:20}\".format(word, ps.stem(word),wnl.lemmatize(word, pos='v')))\n\n--Word--            --Stem--            --Lemma--           \nprogram             program             program             \nprogramming         program             program             \nprogramer           program             programer           \nprograms            program             program             \nprogrammed          program             program             \n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /Users/enfageorge/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nSource - DataCamp\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n\narticles_df['text lemma'] = articles_df['stopwords_removed'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 150)\n\n\narticles_df.sample()\n\n\n\n\n\n\n\n\ncategory\ntext\nlower_case\npunct_removed\ntokenized\nstopwords_removed\nsnowball_stemmer\ntext lemma\n\n\n\n\n902\ntech\nmobiles not media players yet mobiles are not yet ready to be all-singing all-dancing multimedia devices which will replace portable media players say two reports. despite moves to bring music download services to mobiles people do not want to trade multimedia services with size and battery life said jupiter. a separate study by gartner has also said real-time tv broadcasts to mobiles is unlikely in europe until 2007. technical issues and standards must be resolved first said the r...\nmobiles not media players yet mobiles are not yet ready to be all-singing all-dancing multimedia devices which will replace portable media players say two reports. despite moves to bring music download services to mobiles people do not want to trade multimedia services with size and battery life said jupiter. a separate study by gartner has also said real-time tv broadcasts to mobiles is unlikely in europe until 2007. technical issues and standards must be resolved first said the r...\nmobiles not media players yet mobiles are not yet ready to be allsinging alldancing multimedia devices which will replace portable media players say two reports despite moves to bring music download services to mobiles people do not want to trade multimedia services with size and battery life said jupiter a separate study by gartner has also said realtime tv broadcasts to mobiles is unlikely in europe until 2007 technical issues and standards must be resolved first said the report ...\n[mobiles, not, media, players, yet, mobiles, are, not, yet, ready, to, be, allsinging, alldancing, multimedia, devices, which, will, replace, portable, media, players, say, two, reports, despite, moves, to, bring, music, download, services, to, mobiles, people, do, not, want, to, trade, multimedia, services, with, size, and, battery, life, said, jupiter, a, separate, study, by, gartner, has, also, said, realtime, tv, broadcasts, to, mobiles, is, unlikely, in, europe, until, 2007, technical, ...\n[mobiles, media, players, yet, mobiles, yet, ready, allsinging, alldancing, multimedia, devices, replace, portable, media, players, say, two, reports, despite, moves, bring, music, download, services, mobiles, people, want, trade, multimedia, services, size, battery, life, said, jupiter, separate, study, gartner, also, said, realtime, tv, broadcasts, mobiles, unlikely, europe, 2007, technical, issues, standards, must, resolved, first, said, report, batteries, already, cope, services, operato...\n[mobil, media, player, yet, mobil, yet, readi, allsing, alldanc, multimedia, devic, replac, portabl, media, player, say, two, report, despit, move, bring, music, download, servic, mobil, peopl, want, trade, multimedia, servic, size, batteri, life, said, jupit, separ, studi, gartner, also, said, realtim, tv, broadcast, mobil, unlik, europ, 2007, technic, issu, standard, must, resolv, first, said, report, batteri, alreadi, cope, servic, oper, offer, like, video, playback, video, messag, megapi...\n[mobile, medium, player, yet, mobile, yet, ready, allsinging, alldancing, multimedia, device, replace, portable, medium, player, say, two, report, despite, move, bring, music, download, service, mobile, people, want, trade, multimedia, service, size, battery, life, said, jupiter, separate, study, gartner, also, said, realtime, tv, broadcast, mobile, unlikely, europe, 2007, technical, issue, standard, must, resolved, first, said, report, battery, already, cope, service, operator, offer, like,..."
  },
  {
    "objectID": "Course_Content/Week_4&5/3/Notebook_1.html#sources",
    "href": "Course_Content/Week_4&5/3/Notebook_1.html#sources",
    "title": "Lecture 4.3 (Part 1) : Data Preprocessing - Part 2",
    "section": "Sources :",
    "text": "Sources :\nFrom UofA 💛 ✨: Deep Learning for Natural Language Processing: A Gentle Introduction : Mihai Surdeanu and Marco A. Valenzuela-Escárcega\nBooks :\n\nIntroduction to Information Retrieval By Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze\nSpeech and Language Processing\nNatural Language Processing in Action\n\nBlogs: - https://www.kaggle.com/code/longtng/nlp-preprocessing-feature-extraction-methods-a-z - http://hunterheidenreich.com/blog/stemming-lemmatization-what/\nWill be continued either in a ML example or on the week of Applications"
  },
  {
    "objectID": "Course_Content/Week_4&5/2/home.html",
    "href": "Course_Content/Week_4&5/2/home.html",
    "title": "4.2 - Data Preprocessing - Part 1 | (Tabular Data, Audio and Image)",
    "section": "",
    "text": "Identifying and Learning to remove noisy bits in your dataset\nConverting your data to a form that can be used for downstream analysis and machine learning.\nAn idea of data preprocessing functions used for Audio and Image Data Type [Details not discussed, functions just mentioned.]\n\n\n\n\n\n\nTabular Data - Audible Dataset\nAudio - Example\nImage - Arizona State Museum\nRegex Cheatsheet \nJupyter Notebook\n\n\n\n\n\n\n\n\n\n4.3 - Data Preprocessing - Part 2 & Data Visualization)"
  },
  {
    "objectID": "Course_Content/Week_4&5/2/home.html#objectives",
    "href": "Course_Content/Week_4&5/2/home.html#objectives",
    "title": "4.2 - Data Preprocessing - Part 1 | (Tabular Data, Audio and Image)",
    "section": "",
    "text": "Identifying and Learning to remove noisy bits in your dataset\nConverting your data to a form that can be used for downstream analysis and machine learning.\nAn idea of data preprocessing functions used for Audio and Image Data Type [Details not discussed, functions just mentioned.]"
  },
  {
    "objectID": "Course_Content/Week_4&5/2/home.html#materials",
    "href": "Course_Content/Week_4&5/2/home.html#materials",
    "title": "4.2 - Data Preprocessing - Part 1 | (Tabular Data, Audio and Image)",
    "section": "",
    "text": "Tabular Data - Audible Dataset\nAudio - Example\nImage - Arizona State Museum\nRegex Cheatsheet \nJupyter Notebook"
  },
  {
    "objectID": "Course_Content/Week_4&5/2/home.html#next-class",
    "href": "Course_Content/Week_4&5/2/home.html#next-class",
    "title": "4.2 - Data Preprocessing - Part 1 | (Tabular Data, Audio and Image)",
    "section": "",
    "text": "4.3 - Data Preprocessing - Part 2 & Data Visualization)"
  },
  {
    "objectID": "Course_Content/Week_4&5/2/Notebook.html#tabular-dataset",
    "href": "Course_Content/Week_4&5/2/Notebook.html#tabular-dataset",
    "title": "Lecture 4.2 | Data Preprocessing ( Tabular Data, Audio and Image)",
    "section": "1. Tabular Dataset",
    "text": "1. Tabular Dataset\nAudible Dataset\n\n\n\nColumn name\nDescription\n\n\n\n\nname\nName of the audiobook\n\n\nauthor\nAuthor of the audiobook\n\n\nnarrator\nNarrator of the audiobook\n\n\ntime\nDuration of the audiobook\n\n\nreleasedate\nRelease date of the audiobook\n\n\nlanguage\nLanguage of the audiobook\n\n\nstars\nRating and number of ratings of the audiobook\n\n\nprice\nPrice of the audiobook in INR\n\n\n\nsrc = https://www.kaggle.com/datasets/snehangsude/audible-dataset\n\nimport pandas as pd\n\n\naudible_df = pd.read_csv('data/audible-dataset/audible_uncleaned.csv')\naudible_df.sample(2)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime\nreleasedate\nlanguage\nstars\nprice\n\n\n\n\n47778\nUltratumba: Recopilación de terrores y pesadillas\nWrittenby:MiguelAguerraldeMovellán\nNarratedby:NachoGómez\n4 hrs and 8 mins\n04-10-21\nspanish\nNot rated yet\n268.00\n\n\n1361\nBuilding Log Houses\nWrittenby:HighlightsforChildren\nNarratedby:HighlightsforChildren\n1 min\n16-08-18\nEnglish\nNot rated yet\n46.00\n\n\n\n\n\n\n\n\naudible_df.sample(10)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime\nreleasedate\nlanguage\nstars\nprice\n\n\n\n\n24142\nJesus of Nazareth\nWrittenby:PopeBenedictXVIJosephRatzinger\nNarratedby:NicholasBell\n12 hrs and 4 mins\n31-10-08\nEnglish\n5 out of 5 stars1 rating\n937.00\n\n\n84030\nDie Fowl-Zwillinge und der geheimnisvolle Jäger\nWrittenby:EoinColfer\nNarratedby:RobertFrank\n9 hrs and 12 mins\n29-11-19\ngerman\nNot rated yet\n703.00\n\n\n84845\nA Taste for Love\nWrittenby:JenniferYen\nNarratedby:JosephineHuang\n8 hrs and 46 mins\n02-02-21\nEnglish\n4.5 out of 5 stars4 ratings\n1,256.00\n\n\n43863\nConfessions de Mademoiselle Sapho\nWrittenby:Mathieu-FrançoisdeMairobert\nNarratedby:FabienneProst\n1 hr and 16 mins\n07-09-11\nfrench\nNot rated yet\n375.00\n\n\n36394\nDisarming the Narcissist (Third Edition)\nWrittenby:WendyT.BeharyLCSW,JeffreyYoungPhD-fo...\nNarratedby:JoanaGarcia\n8 hrs and 56 mins\n30-11-21\nEnglish\nNot rated yet\n586.00\n\n\n61512\n#gemeckerfrei\nWrittenby:UliBott,BerndBott\nNarratedby:UliBott,BerndBott\n5 hrs and 55 mins\n29-07-21\ngerman\nNot rated yet\n535.00\n\n\n20982\nThe Short, Strange Life of Herschel Grynszpan\nWrittenby:JonathanKirsch\nNarratedby:SimonPrebble\n9 hrs and 16 mins\n20-12-13\nEnglish\nNot rated yet\n836.00\n\n\n81887\nRich Dad's Escape the Rat Race\nWrittenby:RobertT.Kiyosaki\nNarratedby:LukeDaniels,NickPodehl,BenjaminL.Da...\n54 mins\n12-07-13\nEnglish\n5 out of 5 stars7 ratings\n1,005.00\n\n\n23392\nThe Innocent Man\nWrittenby:JohnGrisham\nNarratedby:DennisBoutsikaris\n5 hrs and 53 mins\n10-10-06\nEnglish\n4.5 out of 5 stars11 ratings\n615.00\n\n\n42708\nLove Is the Best Medicine\nWrittenby:NicholasTrout\nNarratedby:JonathanCowley\n8 hrs and 30 mins\n02-03-10\nEnglish\nNot rated yet\n879.00\n\n\n\n\n\n\n\n\nDescribe()\n\naudible_df.describe()\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime\nreleasedate\nlanguage\nstars\nprice\n\n\n\n\ncount\n87489\n87489\n87489\n87489\n87489\n87489\n87489\n87489\n\n\nunique\n82767\n48374\n29717\n2284\n5058\n36\n665\n1011\n\n\ntop\nThe Art of War\nWrittenby:矢島雅弘,石橋遊\nNarratedby:anonymous\n2 mins\n16-05-18\nEnglish\nNot rated yet\n586.00\n\n\nfreq\n20\n874\n1034\n372\n773\n61884\n72417\n5533\n\n\n\n\n\n\n\n\n\nInfo()\n\naudible_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 87489 entries, 0 to 87488\nData columns (total 8 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   name         87489 non-null  object\n 1   author       87489 non-null  object\n 2   narrator     87489 non-null  object\n 3   time         87489 non-null  object\n 4   releasedate  87489 non-null  object\n 5   language     87489 non-null  object\n 6   stars        87489 non-null  object\n 7   price        87489 non-null  object\ndtypes: object(8)\nmemory usage: 5.3+ MB\n\n\nName\n\naudible_df['name'].sample(3).to_frame()\n\n\n\n\n\n\n\n\nname\n\n\n\n\n35287\nThe End of the Beginning\n\n\n47069\nJason and the Golden Fleece\n\n\n40200\nOn War\n\n\n\n\n\n\n\n\naudible_df[audible_df['name']==' ']\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime\nreleasedate\nlanguage\nstars\nprice\n\n\n\n\n\n\n\n\n\nAuthor\n\naudible_df['author'].sample(3).to_frame()\n\n\n\n\n\n\n\n\nauthor\n\n\n\n\n19179\nWrittenby:DavidJiménez\n\n\n86993\nWrittenby:CarloPizzati\n\n\n46399\nWrittenby:T.C.Littles\n\n\n\n\n\n\n\n\n#Remove written by\n\n\nsample = audible_df['author'][0]\nsample\n\n'Writtenby:GeronimoStilton'\n\n\n\nsample.split(\":\")[1]\n\n'GeronimoStilton'\n\n\n\nsample.replace(\"Writtenby:\",'')\n\n'GeronimoStilton'"
  },
  {
    "objectID": "Course_Content/Week_4&5/2/Notebook.html#regex",
    "href": "Course_Content/Week_4&5/2/Notebook.html#regex",
    "title": "Lecture 4.2 | Data Preprocessing ( Tabular Data, Audio and Image)",
    "section": "1. Regex",
    "text": "1. Regex\nLike find , but way better\nA Regular Expressions (RegEx) is a special sequence of characters that uses a search pattern to find a string or set of strings. It can detect the presence or absence of a text by matching it with a particular pattern, and also can split a pattern into one or more sub-patterns.\ncheatsheet link = https://cheatography.com/mutanclan/cheat-sheets/python-regular-expression-regex/\n\n\n\n# Find all emails ( .com, .edu, .net)\nimport re\n\n\nemail_regex = \"[a-zA-Z0-9]+@[a-zA-Z]+\\.(com|net|edu)\"\n\n\nemail_input = input()\nif re.search(email_regex, email_input):\n    print(\"Valid Email\")\nelse:\n    print(\"Invalid Email\")\n\nabc@hello.com\nValid Email\n\n\n\nuppercase_letter_pattern = '.[^A-Z]*'\nre.findall(uppercase_letter_pattern, sample.replace('Writtenby:',''))\n\n['Geronimo', 'Stilton']\n\n\n\nmodified_sample = sample.replace('Writtenby:','')\nnames = modified_sample.split(',')\n[' '.join(re.findall(uppercase_letter_pattern,name)) for name in sample.replace('Writtenby:','').split(',')]\n    \n\n['Geronimo Stilton']\n\n\n\ndef format_names(input_string):\n    cleaned_string = input_string.replace('Writtenby:','').replace('Narratedby:','')\n    formatted_names = []\n    names = cleaned_string.split(',')\n    for name in names:\n        formatted_names.append(' '.join(re.findall(uppercase_letter_pattern,name)))\n    return formatted_names\n\n\naudible_df['author'] = audible_df['author'].apply(lambda input_string : format_names(input_string))\n\nNarrator\n\naudible_df['narrator'] = audible_df['narrator'].apply(lambda input_string : format_names(input_string))\n\n\naudible_df.sample(2)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime\nreleasedate\nlanguage\nstars\nprice\n\n\n\n\n25051\nHarold Larwood\n[Duncan Hamilton]\n[Alex Jennings]\n11 hrs and 30 mins\n16-12-10\nEnglish\n5 out of 5 stars1 rating\n835.00\n\n\n84543\nIn Her Skin\n[Kim Savage]\n[Sandy Rustin]\n6 hrs and 42 mins\n17-04-18\nEnglish\nNot rated yet\n703.00\n\n\n\n\n\n\n\nTime\n\naudible_df['time'].to_list()[:5]\n\n['2 hrs and 20 mins',\n '13 hrs and 8 mins',\n '2 hrs and 3 mins',\n '11 hrs and 16 mins',\n '10 hrs']\n\n\n\ntime_str = audible_df['time'].to_list()[0]\ntime_str\n\n'2 hrs and 20 mins'\n\n\n\ndef convert_strtime_minutes(time_str):\n    if(time_str=='Less than 1 minute'):\n        return 0\n    time_str = time_str.replace(' hrs and','h').replace(' mins','m').replace(' min','m').replace(' hrs','h').replace(' hr','h')\n    h,m = 0,0\n    for item in time_str.split(' '):\n        if 'h' in item:\n            h = int(item.replace('h',''))\n        if 'm' in item:\n            m = int(item.replace('m',''))\n    return (h*60)+m\n\n\naudible_df['time'] = audible_df['time'].apply(lambda time_str : convert_strtime_minutes(time_str))\n\n\naudible_df.sample(3)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime\nreleasedate\nlanguage\nstars\nprice\n\n\n\n\n46690\nÈdip Rei [Oedipus Rex] (Audiolibro en Catalán)\n[Sófocles]\n[Nuria Samsó]\n91\n02-01-19\ncatalan\nNot rated yet\n268.00\n\n\n66730\nLibro Di Michea\n[Autori Vari]\n[Simone Bedetti]\n28\n12-08-21\nitalian\nNot rated yet\n152.00\n\n\n66409\nTrue Sound of Sacred Name of God\n[Brother Arnold Bowen]\n[Hank Hart]\n109\n06-12-21\nEnglish\nNot rated yet\n188.00\n\n\n\n\n\n\n\n\naudible_df.rename({'time':'time in minutes'}, axis=1, inplace = True)\n\n\naudible_df.sample(2)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime in minutes\nreleasedate\nlanguage\nstars\nprice\n\n\n\n\n24830\nPlaying to Win\n[Saina Nehwal]\n[Akanksha Sharma]\n202\n19-11-19\nEnglish\n5 out of 5 stars1 rating\n1,005.00\n\n\n70146\nThe Investigator\n[Natalie Wrye]\n[Grace Grant, Alexander Cendese]\n406\n10-11-20\nEnglish\nNot rated yet\n1,172.00\n\n\n\n\n\n\n\nRelease date\n\naudible_df['releasedate']=pd.to_datetime(audible_df['releasedate'])\n\n\naudible_df.sample(2)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime in minutes\nreleasedate\nlanguage\nstars\nprice\n\n\n\n\n56257\nUna noche y nada más [One Night and Nothing Else]\n[Whitney G.]\n[Marcel Navarro, Maria Gil]\n623\n2019-02-01\nspanish\nNot rated yet\n883.00\n\n\n27491\n48-Hour Start-Up: From Idea to Launch in 1 Wee...\n[Fraser Doherty M B E]\n[Fraser Doherty M B E]\n292\n2016-08-25\nEnglish\n4.5 out of 5 stars6 ratings\n303.00\n\n\n\n\n\n\n\nLanguage\n\naudible_df['language'].value_counts().sort_values()\n\nukrainian               1\nhebrew                  2\nlithuanian              2\ntelugu                  2\nbasque                  2\nslovene                 4\nkorean                  4\nbulgarian               9\ngalician               10\narabic                 16\nnorwegian              16\ngreek                  18\nturkish                20\nczech                  23\nafrikaans              28\nurdu                   34\nhungarian              36\nromanian               50\nicelandic              52\nmandarin_chinese       97\ncatalan               153\ntamil                 161\ndutch                 190\nfinnish               197\npolish                224\nHindi                 436\nswedish               515\nportuguese            526\ndanish                935\nrussian              1804\nfrench               2386\nitalian              2694\njapanese             3167\nspanish              3496\ngerman               8295\nEnglish             61884\nName: language, dtype: int64\n\n\n\naudible_df['language'] = audible_df['language'].apply(str.lower)\n\n\naudible_df.sample(3)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime in minutes\nreleasedate\nlanguage\nstars\nprice\n\n\n\n\n83723\nQueen of Volts\n[Amanda Foody]\n[Shiromi Arserio]\n993\n2020-01-09\nenglish\n5 out of 5 stars4 ratings\n937.00\n\n\n46640\nThe Persian Expedition\n[Xenophon]\n[David Timson]\n555\n2020-03-13\nenglish\n5 out of 5 stars1 rating\n633.00\n\n\n33490\nZen Body-Being\n[Peter Ralston, Laura Ralston]\n[Toby Sheets]\n404\n2019-08-20\nenglish\n4 out of 5 stars1 rating\n1,254.00\n\n\n\n\n\n\n\n\nrating_str = audible_df['stars'].to_list()[0]\nrating_str\n\n'5 out of 5 stars34 ratings'\n\n\n\nimport numpy as np\ndef format_rating_text(rating_str):\n    \n    if rating_str == 'Not rated yet':\n        return [np.NaN,np.NaN,0]\n    \n    else:\n    \n        split_text = rating_str.split(' stars')\n\n        # Working on the stars and out of part \n        stars_and_out_of = split_text[0].split(' out of ')\n\n        stars = float(stars_and_out_of[0])\n        out_of = int(stars_and_out_of[1])\n\n        # Working on number of ratings\n        nos_of_rating = int(split_text[1].replace(' ratings','').replace(' rating','').replace(',',''))\n\n        return stars, out_of, nos_of_rating\n\n\nstar_rating_df = pd.DataFrame(audible_df['stars'].apply(format_rating_text).to_list(),columns=['star rating','total stars','number of ratings'])\nstar_rating_df.sample(100)\n\n\n\n\n\n\n\n\nstar rating\ntotal stars\nnumber of ratings\n\n\n\n\n32450\nNaN\nNaN\n0\n\n\n11881\nNaN\nNaN\n0\n\n\n31393\nNaN\nNaN\n0\n\n\n85528\nNaN\nNaN\n0\n\n\n12790\n4.0\n5.0\n1\n\n\n...\n...\n...\n...\n\n\n26034\nNaN\nNaN\n0\n\n\n40108\nNaN\nNaN\n0\n\n\n25646\nNaN\nNaN\n0\n\n\n65040\nNaN\nNaN\n0\n\n\n41445\nNaN\nNaN\n0\n\n\n\n\n100 rows × 3 columns\n\n\n\n\naudible_df[['star rating','total stars','number of ratings']] = star_rating_df\n\n\naudible_df.sample(3)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime in minutes\nreleasedate\nlanguage\nstars\nprice\nstar rating\ntotal stars\nnumber of ratings\n\n\n\n\n47520\nHighland Hunger\n[Hannah Howell, Michele Sinclair, Jackie Ivie]\n[Jayne Entwistle]\n630\n2021-11-30\nenglish\nNot rated yet\n703.00\nNaN\nNaN\n0\n\n\n73285\nMoon's Web\n[C. T. Adams, Kathy Clamp]\n[Adam Epstein]\n827\n2013-09-24\nenglish\nNot rated yet\n836.00\nNaN\nNaN\n0\n\n\n24509\nFrère François\n[Julien Green]\n[François Montagut]\n723\n2013-06-12\nfrench\nNot rated yet\n679.00\nNaN\nNaN\n0\n\n\n\n\n\n\n\n\ndel audible_df['stars']\n\n\naudible_df.sample(2)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime in minutes\nreleasedate\nlanguage\nprice\nstar rating\ntotal stars\nnumber of ratings\n\n\n\n\n53683\nPablo Neruda Lee a Pablo Neruda [Pablo Neruda ...\n[Pablo Neruda]\n[Pablo Neruda]\n62\n2005-09-14\nspanish\n325.00\nNaN\nNaN\n0\n\n\n82141\nWait for Me\n[An Na]\n[Kim Mai Guest]\n260\n2006-01-01\nenglish\n754.00\nNaN\nNaN\n0\n\n\n\n\n\n\n\nPrice\n\ndef format_pricing(price_text):\n    \n    if price_text=='Free':\n        return 0\n    else:\n        return float(price_text.replace(\",\",''))\n\n\naudible_df['price in indian rupees'] = audible_df['price'].apply(lambda price_text : format_pricing(price_text))\n\n\naudible_df.sample(4)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime in minutes\nreleasedate\nlanguage\nprice\nstar rating\ntotal stars\nnumber of ratings\nprice in indian rupees\n\n\n\n\n68583\n言葉の魔法でハッピーに！聴いて できる タロットカウンセリング\n[石川小百合]\n[石川小百合, 吉田渚]\n250\n2021-07-07\njapanese\n697.00\nNaN\nNaN\n0\n697.0\n\n\n61099\nШкола на краю света. Драконий дар [School at t...\n[Uliya Arharova]\n[Natalya Frolova]\n526\n2022-01-27\nrussian\n234.00\nNaN\nNaN\n0\n234.0\n\n\n6664\nThe House That George Built\n[Suzanne Slade]\n[Lauren Mc Cullough]\n9\n2019-11-21\nenglish\n63.00\nNaN\nNaN\n0\n63.0\n\n\n25143\nNever Settle\n[Marty Smith]\n[Marty Smith]\n494\n2019-06-08\nenglish\n500.00\nNaN\nNaN\n0\n500.0\n\n\n\n\n\n\n\n\ndel audible_df['price']\n\n\naudible_df.sample(20)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime in minutes\nreleasedate\nlanguage\nstar rating\ntotal stars\nnumber of ratings\nprice in indian rupees\n\n\n\n\n58712\nThe Science of Getting Rich\n[Wallace Wattles]\n[Dan Strutzel]\n135\n2018-06-13\nenglish\n5.0\n5.0\n1\n1003.0\n\n\n37191\nDestruction of Black Civilization\n[Chancellor Williams]\n[Joseph Kent]\n767\n2019-07-11\nenglish\nNaN\nNaN\n0\n836.0\n\n\n74262\nOnly a Promise\n[Mary Balogh]\n[Rosalyn Landor]\n674\n2015-09-06\nenglish\n5.0\n5.0\n1\n938.0\n\n\n37760\nRevolutionary Backlash\n[Rosmarie Zagarri]\n[Kirsten Potter]\n554\n2021-06-22\nenglish\nNaN\nNaN\n0\n586.0\n\n\n31917\nMrs D Is Going Without\n[Lotta Dann]\n[Cat Gould]\n419\n2019-01-29\nenglish\nNaN\nNaN\n0\n586.0\n\n\n72769\nДве судьбы [Two Destinies]\n[Wilkie Collins]\n[Lyudmila Bykov]\n626\n2021-02-26\nrussian\nNaN\nNaN\n0\n70.0\n\n\n16541\n第375回 新刊ラジオ第2部プレミアム\n[矢島雅弘, 石橋遊]\n[矢島雅弘, 石橋遊]\n16\n2018-05-16\njapanese\nNaN\nNaN\n0\n139.0\n\n\n82784\nティアムーン帝国物語7 ～断頭台から始まる、姫の転生逆転ストーリー～\n[餅月望]\n[斎藤楓子]\n744\n2022-02-25\njapanese\nNaN\nNaN\n0\n976.0\n\n\n66892\nKundalini\n[Om Swami]\n[Jagdish Raja]\n250\n2019-03-25\nenglish\n4.5\n5.0\n335\n288.0\n\n\n78993\nThe Shipwreck Hunter\n[David L. Mearns]\n[Dan Woren]\n1011\n2018-06-26\nenglish\nNaN\nNaN\n0\n1003.0\n\n\n18793\nYours, for Probably Always\n[Janet Somerville]\n[Ellen Barkin]\n1197\n2021-05-18\nenglish\nNaN\nNaN\n0\n1382.0\n\n\n9931\nMagic Tree House: Books 31 & 32\n[Mary Pope Osborne]\n[Mary Pope Osborne]\n132\n2019-09-07\nenglish\n5.0\n5.0\n1\n502.0\n\n\n18346\nLetter to My Daughter\n[Maya Angelou]\n[Maya Angelou]\n152\n2008-10-13\nenglish\n4.5\n5.0\n24\n452.0\n\n\n43359\nClinch\n[Martin Holmén]\n[Barnaby Jago]\n506\n2020-02-18\nenglish\nNaN\nNaN\n0\n636.0\n\n\n71800\nFace Offs & Cheap Shots\n[Eden Finley, Saxon James]\n[Alexander Cendese, Iggy Toma]\n425\n2021-01-26\nenglish\nNaN\nNaN\n0\n586.0\n\n\n72432\nХоровод Невест [Round Dance of Brides]\n[Sergey Efanov]\n[Alexey Semenov]\n747\n2021-08-10\nrussian\nNaN\nNaN\n0\n234.0\n\n\n24086\nPassport to Heaven\n[Micah Wilder]\n[Micah Wilder]\n615\n2021-01-06\nenglish\nNaN\nNaN\n0\n702.0\n\n\n40991\nShe Votes\n[Bridget Quinn]\n[Bridget Quinn, Donna Allen]\n301\n2020-11-08\nenglish\nNaN\nNaN\n0\n500.0\n\n\n83763\nThe Tenth Girl\n[Sara Faring]\n[Frankie Corzo, Mark Sanderlin]\n935\n2019-09-24\nenglish\nNaN\nNaN\n0\n221.0\n\n\n6775\nI Bicchierini e la Missione di Natale [The Bic...\n[E. J. Pitch]\n[E. J. Pitch]\n14\n2022-03-23\nitalian\nNaN\nNaN\n0\n117.0\n\n\n\n\n\n\n\n\naudible_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 87489 entries, 0 to 87488\nData columns (total 10 columns):\n #   Column                  Non-Null Count  Dtype         \n---  ------                  --------------  -----         \n 0   name                    87489 non-null  object        \n 1   author                  87489 non-null  object        \n 2   narrator                87489 non-null  object        \n 3   time in minutes         87489 non-null  int64         \n 4   releasedate             87489 non-null  datetime64[ns]\n 5   language                87489 non-null  object        \n 6   star rating             15072 non-null  float64       \n 7   total stars             15072 non-null  float64       \n 8   number of ratings       87489 non-null  int64         \n 9   price in indian rupees  87489 non-null  float64       \ndtypes: datetime64[ns](1), float64(3), int64(2), object(4)\nmemory usage: 6.7+ MB\n\n\n\naudible_df.describe()\n\n\n\n\n\n\n\n\ntime in minutes\nstar rating\ntotal stars\nnumber of ratings\nprice in indian rupees\n\n\n\n\ncount\n87489.000000\n15072.00000\n15072.0\n87489.000000\n87489.000000\n\n\nmean\n417.496965\n4.45694\n5.0\n3.723371\n559.009246\n\n\nstd\n364.560197\n0.72394\n0.0\n86.499601\n336.096642\n\n\nmin\n0.000000\n1.00000\n5.0\n0.000000\n0.000000\n\n\n25%\n142.000000\n4.00000\n5.0\n0.000000\n268.000000\n\n\n50%\n386.000000\n4.50000\n5.0\n0.000000\n585.000000\n\n\n75%\n584.000000\n5.00000\n5.0\n0.000000\n755.000000\n\n\nmax\n8595.000000\n5.00000\n5.0\n12573.000000\n7198.000000\n\n\n\n\n\n\n\nQuestions\n\noriginal_audible_df = pd.read_csv('data/audible-dataset/audible_uncleaned.csv')\noriginal_audible_df.sample(2)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime\nreleasedate\nlanguage\nstars\nprice\n\n\n\n\n68702\nLa porte secrète du succès\nWrittenby:FlorenceScovelSchinn\nNarratedby:SarahA.\n1 hr and 36 mins\n25-01-21\nfrench\nNot rated yet\n650.00\n\n\n73341\nDer Rebell\nWrittenby:J.R.Ward\nNarratedby:UweBüschken\n14 hrs and 53 mins\n13-06-12\ngerman\nNot rated yet\n837.00\n\n\n\n\n\n\n\n\naudible_df.sample(2)\n\n\n\n\n\n\n\n\nname\nauthor\nnarrator\ntime in minutes\nreleasedate\nlanguage\nstar rating\ntotal stars\nnumber of ratings\nprice in indian rupees\n\n\n\n\n47803\nLife Without Children\n[Roddy Doyle]\n[Roddy Doyle]\n293\n2021-07-10\nenglish\nNaN\nNaN\n0\n615.0\n\n\n28466\nContent Marketing, Content Management & Conten...\n[Paul Reichenbach]\n[Angelo Aufderheide]\n159\n2022-03-29\ngerman\nNaN\nNaN\n0\n267.0\n\n\n\n\n\n\n\nWhat is the average number of ratings a listing in audible.in receives\n\n#original_audible_df['']\n\n## Multiple lines of code\n\naudible_df['number of ratings'].mean()\n\n3.7233709380607847"
  },
  {
    "objectID": "Course_Content/Week_4&5/2/Notebook.html#sound-data",
    "href": "Course_Content/Week_4&5/2/Notebook.html#sound-data",
    "title": "Lecture 4.2 | Data Preprocessing ( Tabular Data, Audio and Image)",
    "section": "2. Sound data",
    "text": "2. Sound data"
  },
  {
    "objectID": "Course_Content/Week_4&5/2/Notebook.html#what-is-sound",
    "href": "Course_Content/Week_4&5/2/Notebook.html#what-is-sound",
    "title": "Lecture 4.2 | Data Preprocessing ( Tabular Data, Audio and Image)",
    "section": "What is sound?",
    "text": "What is sound?\n\nLink\n\n!pip install librosa\n\nRequirement already satisfied: librosa in /Users/enfageorge/miniconda/lib/python3.10/site-packages (0.10.0.post2)\nRequirement already satisfied: audioread&gt;=2.1.9 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (3.0.0)\nRequirement already satisfied: typing-extensions&gt;=4.1.1 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (4.5.0)\nRequirement already satisfied: joblib&gt;=0.14 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (1.2.0)\nRequirement already satisfied: decorator&gt;=4.3.0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (5.1.1)\nRequirement already satisfied: msgpack&gt;=1.0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (1.0.5)\nRequirement already satisfied: scipy&gt;=1.2.0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (1.9.3)\nRequirement already satisfied: numba&gt;=0.51.0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (0.56.4)\nRequirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,&gt;=1.20.3 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (1.23.5)\nRequirement already satisfied: pooch&lt;1.7,&gt;=1.0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (1.6.0)\nRequirement already satisfied: soxr&gt;=0.3.2 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (0.3.5)\nRequirement already satisfied: scikit-learn&gt;=0.20.0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (1.2.2)\nRequirement already satisfied: lazy-loader&gt;=0.1 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (0.3)\nRequirement already satisfied: soundfile&gt;=0.12.1 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from librosa) (0.12.1)\nRequirement already satisfied: llvmlite&lt;0.40,&gt;=0.39.0dev0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from numba&gt;=0.51.0-&gt;librosa) (0.39.1)\nRequirement already satisfied: setuptools in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from numba&gt;=0.51.0-&gt;librosa) (67.4.0)\nRequirement already satisfied: requests&gt;=2.19.0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from pooch&lt;1.7,&gt;=1.0-&gt;librosa) (2.28.2)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from pooch&lt;1.7,&gt;=1.0-&gt;librosa) (23.1)\nRequirement already satisfied: appdirs&gt;=1.3.0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from pooch&lt;1.7,&gt;=1.0-&gt;librosa) (1.4.4)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from scikit-learn&gt;=0.20.0-&gt;librosa) (3.1.0)\nRequirement already satisfied: cffi&gt;=1.0 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from soundfile&gt;=0.12.1-&gt;librosa) (1.15.1)\nRequirement already satisfied: pycparser in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from cffi&gt;=1.0-&gt;soundfile&gt;=0.12.1-&gt;librosa) (2.21)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;pooch&lt;1.7,&gt;=1.0-&gt;librosa) (2022.12.7)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;pooch&lt;1.7,&gt;=1.0-&gt;librosa) (3.0.1)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;pooch&lt;1.7,&gt;=1.0-&gt;librosa) (1.26.14)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from requests&gt;=2.19.0-&gt;pooch&lt;1.7,&gt;=1.0-&gt;librosa) (3.4)\n\n\nSound: sequence of vibrations in varying pressure strengths (y)  Sample Rate: (sr) is the number of samples of audio carried per second, measured in Hz or kHz\n\n# Importing 1 file\n\nimport librosa\naudio_signal, sample_rate = librosa.load('data/sound/sound_example.wav',sr=2200)\n\nprint('signal:', audio_signal, '\\n')\nprint('signal shape:', np.shape(audio_signal), '\\n')\nprint('Sample Rate (KHz):', sample_rate, '\\n')\n\n# Verify length of the audio\nprint('Check Len of Audio:', np.shape(audio_signal)[0]/sample_rate)\n\nsignal: [ 7.67160710e-11 -6.01338909e-11  3.56031073e-11 ...  4.21123696e-06\n  5.02657258e-07  1.06644375e-05] \n\nsignal shape: (8295,) \n\nSample Rate (KHz): 2200 \n\nCheck Len of Audio: 3.7704545454545455\n\n\nTrim silence\n\n# Trim leading and trailing silence from an audio signal (silence before and after the actual audio)\naudio_signal_trimmed, _ = librosa.effects.trim(audio_signal)\n\n# the result is an numpy ndarray\nprint('Audio File:', audio_signal_trimmed, '\\n')\nprint('Audio File shape:', np.shape(audio_signal_trimmed))\n\nAudio File: [ 1.69425641e-06 -5.26718111e-07 -1.19822676e-06 ...  4.21123696e-06\n  5.02657258e-07  1.06644375e-05] \n\nAudio File shape: (6759,)\n\n\nSound Waves- 2D Representation\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(16, 9))\nlibrosa.display.waveshow(y = audio_signal, sr = sample_rate, color = \"#A300F9\")\nfig.suptitle('Sound Waves', fontsize=16)\n\nText(0.5, 0.98, 'Sound Waves')\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(16, 9))\nlibrosa.display.waveshow(y = audio_signal_trimmed, sr = sample_rate, color = \"#A300F9\")\n\n&lt;librosa.display.AdaptiveWaveplot at 0x15bcb34c0&gt;\n\n\n\n\n\n\nFourier Transform\nFunction that gets a signal in the time domain as input, and outputs its decomposition into frequencies. Transform both the y-axis (frequency) to log scale, and the “color” axis (amplitude) to Decibels, which is approx. the log scale of amplitudes.\n\n# Default FFT window size\nn_fft = 2048 # FFT window size\nhop_length = 512 # number audio of frames between STFT columns (looks like a good default)\n\n# Short-time Fourier transform (STFT)\naudio_fft = np.abs(librosa.stft(audio_signal_trimmed, n_fft = n_fft, hop_length = hop_length))\n\nprint('Shape of D object:', np.shape(audio_fft))\n\nShape of D object: (1025, 14)\n\n\n\n\nSpectrogram\nA spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams (wiki).\n\ndb_audio = librosa.amplitude_to_db(audio_fft, ref = np.max)\n\n# === PLOT ===\nfig, ax = plt.subplots(figsize=(16, 9))\nfig.suptitle('Spectrogram', fontsize=16)\n\nlibrosa.display.specshow(db_audio, sr = sample_rate, hop_length = hop_length, x_axis = 'time', \n                         y_axis = 'log', cmap = 'cool')\n\n&lt;matplotlib.collections.QuadMesh at 0x15bd83eb0&gt;\n\n\n\n\n\nRefer :\n\nAudio Signal Processing for Machine Learning | Valerio Velardo - The Sound of AI\nBirdcall Recognition: EDA and Audio FE\nAudio Deep Learning Made Simple (Part 1): State-of-the-Art Techniques\nMachine Learning on Sound and Audio data"
  },
  {
    "objectID": "Course_Content/Week_4&5/2/Notebook.html#image-data",
    "href": "Course_Content/Week_4&5/2/Notebook.html#image-data",
    "title": "Lecture 4.2 | Data Preprocessing ( Tabular Data, Audio and Image)",
    "section": "3. Image Data",
    "text": "3. Image Data\n\nimport cv2\nimport matplotlib.pyplot as plt\n\nLink\n\ndata_path = \"data/image/University_of_Arizona_May_2019_04_(Arizona_State_Museum).jpg\"\nimage = cv2.imread(data_path)\n\n#Show the image with matplotlib\nplt.imshow(image)\nplt.show()\n\n\n\n\n\nColorspaces\nA color space is a specific organization of colors.\n\nRGB(Red Green Blue)\nHSL(Hue Saturation Lightness)\nHSV(Hue Saturation Value)\nYUV(Luminance, blue–luminance, red–luminance)\nCMYK(Cyan, Magenta, Yellow, Key)\n\n\nimg_color = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n#Show the image with matplotlib\nplt.imshow(img_color)\nplt.show()\n\n\n\n\n\n\nConvert to GreyScale\n\nimg_color.shape\n\n(4000, 6000, 3)\n\n\n\nimg_gray = cv2.cvtColor(img_color, cv2.COLOR_RGB2GRAY)\n\n#Show the image with matplotlib\nplt.imshow(img_gray,cmap='gray')\nplt.show()\n\n\n\n\n\nimg_gray.shape\n\n(4000, 6000)\n\n\n\n\nResizing\n\nresized_pic = cv2.resize(img_color,(300,200))\n\nplt.imshow(resized_pic)\nplt.show()\n\n\n\n\n\n\nRotate\n\nimg_rotated = cv2.flip(img_color,0)\n\nplt.imshow(img_rotated)\nplt.show()\n\n\n\n\n\n\nCrop\n\nplt.imshow(img_color[1150:2350,800:5200])\n\n&lt;matplotlib.image.AxesImage at 0x16df7dde0&gt;\n\n\n\n\n\n\nNext Lecture - Final part of data preprocessing and Data visualisation"
  },
  {
    "objectID": "Course_Content/Week_2/home.html",
    "href": "Course_Content/Week_2/home.html",
    "title": "Week 2 : Applied Probability and Statistics ( Part One )",
    "section": "",
    "text": "Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nExplore the ethical considerations associated with data-driven decision-making.\n\n\n\n\n\nExplain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.)\nDemonstrate awareness of bias and ethics in data science.\n\n\n\n\n\n\n2.1 - Random Events and Probability\n2.2 - Moments and Independence\n2.3 - Statistics & Bayesian Probability\n2.x.1 - Binomial Probability\n\n\n\n\n\n\n\nHomework 1: Due 5 pm, June 26th,Monday\n\n\n\n\n\nWeekly Checkin for Week 2 : Due 5 pm, June 25th, Sunday."
  },
  {
    "objectID": "Course_Content/Week_2/home.html#objectives",
    "href": "Course_Content/Week_2/home.html#objectives",
    "title": "Week 2 : Applied Probability and Statistics ( Part One )",
    "section": "",
    "text": "Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nExplore the ethical considerations associated with data-driven decision-making.\n\n\n\n\n\nExplain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.)\nDemonstrate awareness of bias and ethics in data science."
  },
  {
    "objectID": "Course_Content/Week_2/home.html#lectures",
    "href": "Course_Content/Week_2/home.html#lectures",
    "title": "Week 2 : Applied Probability and Statistics ( Part One )",
    "section": "",
    "text": "2.1 - Random Events and Probability\n2.2 - Moments and Independence\n2.3 - Statistics & Bayesian Probability\n2.x.1 - Binomial Probability"
  },
  {
    "objectID": "Course_Content/Week_2/home.html#activities",
    "href": "Course_Content/Week_2/home.html#activities",
    "title": "Week 2 : Applied Probability and Statistics ( Part One )",
    "section": "",
    "text": "Homework 1: Due 5 pm, June 26th,Monday\n\n\n\n\n\nWeekly Checkin for Week 2 : Due 5 pm, June 25th, Sunday."
  },
  {
    "objectID": "Course_Content/Week_2/1/home.html",
    "href": "Course_Content/Week_2/1/home.html",
    "title": "2.1 Random Events and Probability",
    "section": "",
    "text": "Random Events\nWhat is Probability?\n\nProbability Axioms\nLemma\n\nOperations\nRandom Variable\nProbability Distribution:\nProbability Mass Function and Probability Density Function.\nJoint, Marginal and Conditional Probability\n\n\n\n\n\nN/A\n\n\n\n\n\n\n\n\n\nMoments and Independence"
  },
  {
    "objectID": "Course_Content/Week_2/1/home.html#class-objectives",
    "href": "Course_Content/Week_2/1/home.html#class-objectives",
    "title": "2.1 Random Events and Probability",
    "section": "",
    "text": "Random Events\nWhat is Probability?\n\nProbability Axioms\nLemma\n\nOperations\nRandom Variable\nProbability Distribution:\nProbability Mass Function and Probability Density Function.\nJoint, Marginal and Conditional Probability"
  },
  {
    "objectID": "Course_Content/Week_2/1/home.html#materials",
    "href": "Course_Content/Week_2/1/home.html#materials",
    "title": "2.1 Random Events and Probability",
    "section": "",
    "text": "N/A"
  },
  {
    "objectID": "Course_Content/Week_2/1/home.html#next-class",
    "href": "Course_Content/Week_2/1/home.html#next-class",
    "title": "2.1 Random Events and Probability",
    "section": "",
    "text": "Moments and Independence"
  },
  {
    "objectID": "Course_Content/Week_2/3/home.html",
    "href": "Course_Content/Week_2/3/home.html",
    "title": "2.3 Statistics & Bayesian Probability",
    "section": "",
    "text": "Introduction to Statistics\nKey Statistical Concepts\nThe Monty Hall Problem\nBayesian Probability\nBaye’s Theorm\n\n\n\n\n\nN/A\n\n\n\n\n\n\n\n\n\nWeek 3 - Data Collection and Data Processing, Exploratory Analysis"
  },
  {
    "objectID": "Course_Content/Week_2/3/home.html#class-objectives",
    "href": "Course_Content/Week_2/3/home.html#class-objectives",
    "title": "2.3 Statistics & Bayesian Probability",
    "section": "",
    "text": "Introduction to Statistics\nKey Statistical Concepts\nThe Monty Hall Problem\nBayesian Probability\nBaye’s Theorm"
  },
  {
    "objectID": "Course_Content/Week_2/3/home.html#materials",
    "href": "Course_Content/Week_2/3/home.html#materials",
    "title": "2.3 Statistics & Bayesian Probability",
    "section": "",
    "text": "N/A"
  },
  {
    "objectID": "Course_Content/Week_2/3/home.html#next-class",
    "href": "Course_Content/Week_2/3/home.html#next-class",
    "title": "2.3 Statistics & Bayesian Probability",
    "section": "",
    "text": "Week 3 - Data Collection and Data Processing, Exploratory Analysis"
  },
  {
    "objectID": "Course_Content/Week_2/2/home.html",
    "href": "Course_Content/Week_2/2/home.html",
    "title": "2.2 Moments and Independence",
    "section": "",
    "text": "Expectation of a Random Variable\nMoments of Random Variable\nVariance and Standard Deviation\nCorelation\nCovariance\nIndependence of Random Variable\n\n\n\n\n\nN/A"
  },
  {
    "objectID": "Course_Content/Week_2/2/home.html#class-objectives",
    "href": "Course_Content/Week_2/2/home.html#class-objectives",
    "title": "2.2 Moments and Independence",
    "section": "",
    "text": "Expectation of a Random Variable\nMoments of Random Variable\nVariance and Standard Deviation\nCorelation\nCovariance\nIndependence of Random Variable"
  },
  {
    "objectID": "Course_Content/Week_2/2/home.html#materials",
    "href": "Course_Content/Week_2/2/home.html#materials",
    "title": "2.2 Moments and Independence",
    "section": "",
    "text": "N/A"
  },
  {
    "objectID": "Course_Content/Week_3/home.html",
    "href": "Course_Content/Week_3/home.html",
    "title": "Week 3 : Data Collection and Data Processing, Exploratory Analysis",
    "section": "",
    "text": "Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nApply data analysis and visualization techniques to derive insights from diverse datasets.\nDevelop proficiency in using data science tools and programming languages.\nExplore the ethical considerations associated with data-driven decision-making.\n\n\n\n\n\nConvert a raw data source into a version appropriate for downstream analysis using Python.\nDemonstrate awareness of bias and ethics in data science.\n\n\n\n\n\n\n3.1 - Introduction to Pandas\n3.2 - Data Collection (Part 1 of 2)\n3.3 - Data Collection (Part 2 of 2) & Data Processing (Part 1)\n\n\n\n\nHomework\n\nHomework 1: Due 5 pm, June 26th,Monday\n\nWeekly Checkin\n\nWeekly Checkin for Week 2 : Due 5 pm, June 25th, Sunday.\nWeekly Checkin for Week 3 : Due 5 pm, July 2, Sunday\n\n\n\n\n\nStrikethrough text is changes in plan.\nGreen is new items added after planning.\nChecked boxes are completed items.\n\n\n\n\n\n\n\nTopic: Statistics and Probability - Lecture 4 (Part 2) Will continue in Week 8\nLecture 2.x.1 Binomial Distribution\nTopic: Data processing and exploratory Analysis ( 2 or 3 Lectures ):\n\n3.1 - Introduction to Pandas\n3.2 - Data Collection (Part 1 of 2)\n3.3 - Data Collection (Part 2 of 2) & Data Processing (Part 1)\n\n\n\n\n\n\nHomework 1 due June 25 June 26, 5 pm\n\nHomework 1 Question 3.2 dropped. \n\n\n\n\n\n\nCheck-in for week 2 is due on June 25, 5 pm.\nCheck-in for week 3 opens.\n\n\n\n\n\nWeek 3’s assigned reading will be announced and available on D2L on or before Monday."
  },
  {
    "objectID": "Course_Content/Week_3/home.html#objectives",
    "href": "Course_Content/Week_3/home.html#objectives",
    "title": "Week 3 : Data Collection and Data Processing, Exploratory Analysis",
    "section": "",
    "text": "Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nApply data analysis and visualization techniques to derive insights from diverse datasets.\nDevelop proficiency in using data science tools and programming languages.\nExplore the ethical considerations associated with data-driven decision-making.\n\n\n\n\n\nConvert a raw data source into a version appropriate for downstream analysis using Python.\nDemonstrate awareness of bias and ethics in data science."
  },
  {
    "objectID": "Course_Content/Week_3/home.html#lectures",
    "href": "Course_Content/Week_3/home.html#lectures",
    "title": "Week 3 : Data Collection and Data Processing, Exploratory Analysis",
    "section": "",
    "text": "3.1 - Introduction to Pandas\n3.2 - Data Collection (Part 1 of 2)\n3.3 - Data Collection (Part 2 of 2) & Data Processing (Part 1)"
  },
  {
    "objectID": "Course_Content/Week_3/home.html#activities",
    "href": "Course_Content/Week_3/home.html#activities",
    "title": "Week 3 : Data Collection and Data Processing, Exploratory Analysis",
    "section": "",
    "text": "Homework\n\nHomework 1: Due 5 pm, June 26th,Monday\n\nWeekly Checkin\n\nWeekly Checkin for Week 2 : Due 5 pm, June 25th, Sunday.\nWeekly Checkin for Week 3 : Due 5 pm, July 2, Sunday"
  },
  {
    "objectID": "Course_Content/Week_3/home.html#summary-plan-versus-achievements",
    "href": "Course_Content/Week_3/home.html#summary-plan-versus-achievements",
    "title": "Week 3 : Data Collection and Data Processing, Exploratory Analysis",
    "section": "",
    "text": "Strikethrough text is changes in plan.\nGreen is new items added after planning.\nChecked boxes are completed items.\n\n\n\n\n\n\n\nTopic: Statistics and Probability - Lecture 4 (Part 2) Will continue in Week 8\nLecture 2.x.1 Binomial Distribution\nTopic: Data processing and exploratory Analysis ( 2 or 3 Lectures ):\n\n3.1 - Introduction to Pandas\n3.2 - Data Collection (Part 1 of 2)\n3.3 - Data Collection (Part 2 of 2) & Data Processing (Part 1)\n\n\n\n\n\n\nHomework 1 due June 25 June 26, 5 pm\n\nHomework 1 Question 3.2 dropped. \n\n\n\n\n\n\nCheck-in for week 2 is due on June 25, 5 pm.\nCheck-in for week 3 opens.\n\n\n\n\n\nWeek 3’s assigned reading will be announced and available on D2L on or before Monday."
  },
  {
    "objectID": "Course_Content/Week_3/1/home.html",
    "href": "Course_Content/Week_3/1/home.html",
    "title": "3.1 - Introduction to Pandas.",
    "section": "",
    "text": "Understand how to use pandas to read a dataset and basic functionality.\n\nIntro to Pandas\nSeries, DataFrames, and Indices\nSlicing with loc, iloc, and []\n\n\n\n\n\n\nSpotify Top 50 Playlist Songs | @anxods | Kaggle \nNotes\nJupyter Notebook\n\n\n\n\n\n\n\n\n\n3.2 - Data Collection (Part 1 of 2)"
  },
  {
    "objectID": "Course_Content/Week_3/1/home.html#objectives",
    "href": "Course_Content/Week_3/1/home.html#objectives",
    "title": "3.1 - Introduction to Pandas.",
    "section": "",
    "text": "Understand how to use pandas to read a dataset and basic functionality.\n\nIntro to Pandas\nSeries, DataFrames, and Indices\nSlicing with loc, iloc, and []"
  },
  {
    "objectID": "Course_Content/Week_3/1/home.html#materials",
    "href": "Course_Content/Week_3/1/home.html#materials",
    "title": "3.1 - Introduction to Pandas.",
    "section": "",
    "text": "Spotify Top 50 Playlist Songs | @anxods | Kaggle \nNotes\nJupyter Notebook"
  },
  {
    "objectID": "Course_Content/Week_3/1/home.html#next-class",
    "href": "Course_Content/Week_3/1/home.html#next-class",
    "title": "3.1 - Introduction to Pandas.",
    "section": "",
    "text": "3.2 - Data Collection (Part 1 of 2)"
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html",
    "href": "Course_Content/Week_3/1/Notebook.html",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "",
    "text": "# importing the zipfile module\nfrom zipfile import ZipFile\n  \n# loading the temp.zip and creating a zip object\nwith ZipFile(\"spotify-top-50-playlist-songs-anxods.zip\", 'r') as zObject:\n  \n    # Extracting all the members of the zip \n    # into a specific location.\n    zObject.extractall(\"spotify-top-50\")\n\n\nglobal_50_path = \"spotify-top-50/data/spotify-streaming-top-50-world.csv\""
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#load-the-dataset",
    "href": "Course_Content/Week_3/1/Notebook.html#load-the-dataset",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "",
    "text": "# importing the zipfile module\nfrom zipfile import ZipFile\n  \n# loading the temp.zip and creating a zip object\nwith ZipFile(\"spotify-top-50-playlist-songs-anxods.zip\", 'r') as zObject:\n  \n    # Extracting all the members of the zip \n    # into a specific location.\n    zObject.extractall(\"spotify-top-50\")\n\n\nglobal_50_path = \"spotify-top-50/data/spotify-streaming-top-50-world.csv\""
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#read-the-dataset",
    "href": "Course_Content/Week_3/1/Notebook.html#read-the-dataset",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "1. Read the dataset",
    "text": "1. Read the dataset\n\n!pip install pandas\n\n\nimport pandas as pd\n\n\nglobal_50_df = pd.read_csv(global_50_path)\n\n\n#to write (except the index)\n\n# global_50_df.to_csv('temp_file.csv',index=False)\n\n\nglobal_50_df\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n0\n2023-05-18\n1\nElla Baila Sola\nEslabon Armado\n89\n165671\nalbum\n16\n2023-04-28\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273dfddf1...\n\n\n1\n2023-05-18\n2\nun x100to\nGrupo Frontera & Bad Bunny\n99\n194563\nsingle\n1\n2023-04-17\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273716c0b...\n\n\n2\n2023-05-18\n3\nLa Bebe - Remix\nYng Lvcas & Peso Pluma\n99\n234352\nsingle\n2\n2023-03-17\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273a04be3...\n\n\n3\n2023-05-18\n4\nCupid - Twin Ver.\nFIFTY FIFTY\n97\n174253\nsingle\n3\n2023-02-24\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27337c0b3...\n\n\n4\n2023-05-18\n5\nFlowers\nMiley Cyrus\n91\n200600\nalbum\n13\n2023-03-10\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27358039b...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1795\n2023-06-22\n46\nHeat Waves\nGlass Animals\n93\n238805\nalbum\n16\n2020-08-07\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273712701...\n\n\n1796\n2023-06-22\n47\nFrágil\nYahritza Y Su Esencia & Grupo Frontera\n93\n160517\nsingle\n1\n2023-04-07\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27357ed58...\n\n\n1797\n2023-06-22\n48\nPRC\nPeso Pluma & Natanael Cano\n95\n184066\nsingle\n1\n2023-01-23\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737be314...\n\n\n1798\n2023-06-22\n49\nDance The Night (From Barbie The Album)\nDua Lipa\n92\n176579\nsingle\n1\n2023-05-25\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2737dd3ba...\n\n\n1799\n2023-06-22\n50\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n\n\n1800 rows × 11 columns\n\n\n\n\nglobal_50_df.columns\n\nIndex(['date', 'position', 'song', 'artist', 'popularity', 'duration_ms',\n       'album_type', 'total_tracks', 'release_date', 'is_explicit',\n       'album_cover_url'],\n      dtype='object')\n\n\nWho is the top artist of the day\n\ndate = '2023-06-22'\nglobal_50_df[(global_50_df['date']==date) & (global_50_df['position']==1)]\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n1750\n2023-06-22\n1\nElla Baila Sola\nEslabon Armado\n93\n165671\nalbum\n16\n2023-04-28\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273dfddf1..."
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#create-a-dataframe",
    "href": "Course_Content/Week_3/1/Notebook.html#create-a-dataframe",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "2. Create a dataframe",
    "text": "2. Create a dataframe\nSynatx is pd.DataFrame(data, index, columns)\n\npd.DataFrame(['csc380','csv'])\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\ncsc380\n\n\n1\ncsv\n\n\n\n\n\n\n\n\npd.DataFrame(['csc380','csv'], ['csc380','pandas'])\n\n\n\n\n\n\n\n\n0\n\n\n\n\ncsc380\ncsc380\n\n\npandas\ncsv\n\n\n\n\n\n\n\n\npd.DataFrame([['csc380','csv'], ['csc380','pandas']])\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\ncsc380\ncsv\n\n\n1\ncsc380\npandas\n\n\n\n\n\n\n\n\npd.DataFrame([['csc380','csv'], ['csc380','pandas']], columns=['class','method'])\n\n\n\n\n\n\n\n\nclass\nmethod\n\n\n\n\n0\ncsc380\ncsv\n\n\n1\ncsc380\npandas\n\n\n\n\n\n\n\n\npd.DataFrame(['csv','pandas'],['P','P'],['CSC380'])\n\n\n\n\n\n\n\n\nCSC380\n\n\n\n\nP\ncsv\n\n\nP\npandas\n\n\n\n\n\n\n\n\nsample_df = pd.DataFrame(['csv','pandas'],['P','P'],['CSC380']).reset_index()\nsample_df\n\n\n\n\n\n\n\n\nindex\nCSC380\n\n\n\n\n0\nP\ncsv\n\n\n1\nP\npandas\n\n\n\n\n\n\n\n\nsample_df.drop('index',axis = 'columns',inplace=True)\nsample_df\n\n\n\n\n\n\n\n\nCSC380\n\n\n\n\n0\ncsv\n\n\n1\npandas\n\n\n\n\n\n\n\n\nsample_df['random_count'] = [89,52]\nsample_df\n\n\n\n\n\n\n\n\nCSC380\nrandom_count\n\n\n\n\n0\ncsv\n89\n\n\n1\npandas\n52\n\n\n\n\n\n\n\nWe explored creating dataframes using list, you can use other formats too. Below is a dict.\n\npd.DataFrame([\n    {\n        'class': 'csc380',\n        'method' : 'csv'\n    }, \n    {\n        'class': 'csc380',\n        'method':'pandas'\n    }\n])\n\n\n\n\n\n\n\n\nclass\nmethod\n\n\n\n\n0\ncsc380\ncsv\n\n\n1\ncsc380\npandas"
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#simple-operations",
    "href": "Course_Content/Week_3/1/Notebook.html#simple-operations",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "3. Simple Operations",
    "text": "3. Simple Operations\n\nsample_df['random_count'] = sample_df['random_count'] - 10\nsample_df\n\n\n\n\n\n\n\n\nCSC380\nrandom_count\n\n\n\n\n0\ncsv\n79\n\n\n1\npandas\n42\n\n\n\n\n\n\n\n\nsample_df['CSC380'] = sample_df['CSC380'].apply(lambda x : x.upper())\nsample_df\n\n\n\n\n\n\n\n\nCSC380\nrandom_count\n\n\n\n\n0\nCSV\n79\n\n\n1\nPANDAS\n42\n\n\n\n\n\n\n\n\nglobal_50_df.shape\n\n(1800, 11)"
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#head-tail-and-sample",
    "href": "Course_Content/Week_3/1/Notebook.html#head-tail-and-sample",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "4. Head, tail and sample",
    "text": "4. Head, tail and sample\n\nglobal_50_df.head()\n\n\nglobal_50_df.tail(3)\n\n\nglobal_50_df.sample(5)"
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#ilocloc",
    "href": "Course_Content/Week_3/1/Notebook.html#ilocloc",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "5. [ ],iloc,loc",
    "text": "5. [ ],iloc,loc\n\nglobal_50_df[2:5]\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n2\n2023-05-18\n3\nLa Bebe - Remix\nYng Lvcas & Peso Pluma\n99\n234352\nsingle\n2\n2023-03-17\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273a04be3...\n\n\n3\n2023-05-18\n4\nCupid - Twin Ver.\nFIFTY FIFTY\n97\n174253\nsingle\n3\n2023-02-24\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27337c0b3...\n\n\n4\n2023-05-18\n5\nFlowers\nMiley Cyrus\n91\n200600\nalbum\n13\n2023-03-10\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27358039b...\n\n\n\n\n\n\n\n\nglobal_50_df.iloc[15:19]\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n15\n2023-05-18\n16\nBESO\nROSALÍA & Rauw Alejandro\n96\n194543\nsingle\n3\n2023-03-24\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2734d6cf0...\n\n\n16\n2023-05-18\n17\nDie For You (with Ariana Grande) - Remix\nThe Weeknd\n88\n232857\nalbum\n21\n2023-03-14\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2738ad8f5...\n\n\n17\n2023-05-18\n18\nPRC\nPeso Pluma & Natanael Cano\n96\n184066\nsingle\n1\n2023-01-23\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737be314...\n\n\n18\n2023-05-18\n19\nCalm Down (with Selena Gomez)\nRema\n78\n239317\nalbum\n22\n2023-04-27\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273963265...\n\n\n\n\n\n\n\n\nglobal_50_df.loc[15:19]\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n15\n2023-05-18\n16\nBESO\nROSALÍA & Rauw Alejandro\n96\n194543\nsingle\n3\n2023-03-24\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2734d6cf0...\n\n\n16\n2023-05-18\n17\nDie For You (with Ariana Grande) - Remix\nThe Weeknd\n88\n232857\nalbum\n21\n2023-03-14\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2738ad8f5...\n\n\n17\n2023-05-18\n18\nPRC\nPeso Pluma & Natanael Cano\n96\n184066\nsingle\n1\n2023-01-23\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737be314...\n\n\n18\n2023-05-18\n19\nCalm Down (with Selena Gomez)\nRema\n78\n239317\nalbum\n22\n2023-04-27\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273963265...\n\n\n19\n2023-05-18\n20\nYandel 150\nYandel\n90\n216148\nalbum\n17\n2023-01-13\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273b2aec0...\n\n\n\n\n\n\n\n\nglobal_50_subset_df = global_50_df.sample(10)\nglobal_50_subset_df\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n1588\n2023-06-18\n39\nTá OK\nDENNIS & MC Kevin o Chris\n89\n92093\nsingle\n1\n2023-05-04\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2736b19d4...\n\n\n93\n2023-05-19\n44\nPor las Noches\nPeso Pluma\n93\n239845\nsingle\n1\n2021-06-11\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273427d9a...\n\n\n1367\n2023-06-14\n18\nClassy 101\nFeid & Young Miko\n96\n195986\nsingle\n1\n2023-03-31\nTrue\nhttps://i.scdn.co/image/ab67616d0000b27329ebee...\n\n\n213\n2023-05-22\n14\nPRC\nPeso Pluma & Natanael Cano\n96\n184066\nsingle\n1\n2023-01-23\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737be314...\n\n\n124\n2023-05-20\n25\nLike Crazy\nJimin\n93\n212241\nsingle\n6\n2023-03-24\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2732b4607...\n\n\n1043\n2023-06-07\n44\nBye\nPeso Pluma\n86\n212976\nsingle\n1\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b27310c8a0...\n\n\n938\n2023-06-05\n39\nFrágil\nYahritza Y Su Esencia & Grupo Frontera\n91\n160517\nsingle\n1\n2023-04-07\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27357ed58...\n\n\n65\n2023-05-19\n16\nDie For You (with Ariana Grande) - Remix\nThe Weeknd\n88\n232857\nalbum\n21\n2023-03-14\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2738ad8f5...\n\n\n1168\n2023-06-10\n19\nSee You Again (feat. Kali Uchis)\nTyler, The Creator\n95\n180386\nalbum\n14\n2017-07-21\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2738940ac...\n\n\n1277\n2023-06-12\n28\nSee You Again (feat. Kali Uchis)\nTyler, The Creator\n95\n180386\nalbum\n14\n2017-07-21\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2738940ac...\n\n\n\n\n\n\n\n\nglobal_50_subset_df[3:5]\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n213\n2023-05-22\n14\nPRC\nPeso Pluma & Natanael Cano\n96\n184066\nsingle\n1\n2023-01-23\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737be314...\n\n\n124\n2023-05-20\n25\nLike Crazy\nJimin\n93\n212241\nsingle\n6\n2023-03-24\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2732b4607...\n\n\n\n\n\n\n\n\nglobal_50_subset_df.iloc[3:5]\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n213\n2023-05-22\n14\nPRC\nPeso Pluma & Natanael Cano\n96\n184066\nsingle\n1\n2023-01-23\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737be314...\n\n\n124\n2023-05-20\n25\nLike Crazy\nJimin\n93\n212241\nsingle\n6\n2023-03-24\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2732b4607...\n\n\n\n\n\n\n\n\nglobal_50_subset_df.loc[3:5]\n\nKeyError: 3\n\n\n\nglobal_50_df.iloc[5:15]['song','artist']\n\nKeyError: ('song', 'artist')\n\n\n\nglobal_50_df.iloc[5:15][['song','artist']]\n\n\n\n\n\n\n\n\nsong\nartist\n\n\n\n\n5\nDaylight\nDavid Kushner\n\n\n6\nKill Bill\nSZA\n\n\n7\nTattoo\nLoreen\n\n\n8\nAs It Was\nHarry Styles\n\n\n9\nTQG\nKAROL G\n\n\n10\nCha Cha Cha\nKäärijä\n\n\n11\nClassy 101\nFeid & Young Miko\n\n\n12\nAcróstico\nShakira\n\n\n13\nCreepin' (with The Weeknd & 21 Savage)\nMetro Boomin\n\n\n14\nSee You Again (feat. Kali Uchis)\nTyler, The Creator"
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#dataframes-and-series",
    "href": "Course_Content/Week_3/1/Notebook.html#dataframes-and-series",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "6. Dataframes and Series",
    "text": "6. Dataframes and Series\n\nglobal_50_df.iloc[5:15]['song']\n\n5                                   Daylight\n6                                  Kill Bill\n7                                     Tattoo\n8                                  As It Was\n9                                        TQG\n10                               Cha Cha Cha\n11                                Classy 101\n12                                 Acróstico\n13    Creepin' (with The Weeknd & 21 Savage)\n14          See You Again (feat. Kali Uchis)\nName: song, dtype: object\n\n\n\nglobal_50_df.iloc[5:15]['song'].to_frame()\n\n\n\n\n\n\n\n\nsong\n\n\n\n\n5\nDaylight\n\n\n6\nKill Bill\n\n\n7\nTattoo\n\n\n8\nAs It Was\n\n\n9\nTQG\n\n\n10\nCha Cha Cha\n\n\n11\nClassy 101\n\n\n12\nAcróstico\n\n\n13\nCreepin' (with The Weeknd & 21 Savage)\n\n\n14\nSee You Again (feat. Kali Uchis)"
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#conditionals",
    "href": "Course_Content/Week_3/1/Notebook.html#conditionals",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "7. Conditionals",
    "text": "7. Conditionals\nQ : What are the top 50 artists of June 5th 2023?\n\ndate = '2023-06-05'\nglobal_50_df[global_50_df['date']== date]\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n900\n2023-06-05\n1\nElla Baila Sola\nEslabon Armado\n92\n165671\nalbum\n16\n2023-04-28\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273dfddf1...\n\n\n901\n2023-06-05\n2\nPeso Pluma: Bzrp Music Sessions, Vol. 55\nBizarrap & Peso Pluma\n89\n188361\nsingle\n1\n2023-06-01\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273155830...\n\n\n902\n2023-06-05\n3\nWHERE SHE GOES\nBad Bunny\n96\n231704\nsingle\n1\n2023-05-18\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273ab5c9c...\n\n\n903\n2023-06-05\n4\nLa Bebe - Remix\nYng Lvcas & Peso Pluma\n99\n234352\nsingle\n2\n2023-03-17\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273a04be3...\n\n\n904\n2023-06-05\n5\nun x100to\nGrupo Frontera & Bad Bunny\n100\n194563\nsingle\n1\n2023-04-17\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273716c0b...\n\n\n905\n2023-06-05\n6\nFlowers\nMiley Cyrus\n91\n200600\nalbum\n13\n2023-03-10\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27358039b...\n\n\n906\n2023-06-05\n7\nCupid - Twin Ver.\nFIFTY FIFTY\n98\n174253\nsingle\n3\n2023-02-24\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27337c0b3...\n\n\n907\n2023-06-05\n8\nTQM\nFuerza Regida\n91\n158965\nsingle\n1\n2023-05-19\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273832ea5...\n\n\n908\n2023-06-05\n9\nAs It Was\nHarry Styles\n92\n167303\nalbum\n13\n2022-05-20\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2732e8ed7...\n\n\n909\n2023-06-05\n10\nDaylight\nDavid Kushner\n97\n212953\nsingle\n1\n2023-04-14\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27395ca6a...\n\n\n910\n2023-06-05\n11\nKill Bill\nSZA\n94\n153946\nalbum\n23\n2022-12-08\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2730c471c...\n\n\n911\n2023-06-05\n12\nSprinter\nDave & Central Cee\n81\n229133\nsingle\n1\n2023-06-01\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273e3a09a...\n\n\n912\n2023-06-05\n13\nClassy 101\nFeid & Young Miko\n96\n195986\nsingle\n1\n2023-03-31\nTrue\nhttps://i.scdn.co/image/ab67616d0000b27329ebee...\n\n\n913\n2023-06-05\n14\nTQG\nKAROL G\n96\n197933\nalbum\n17\n2023-02-24\nTrue\nhttps://i.scdn.co/image/ab67616d0000b27382de1c...\n\n\n914\n2023-06-05\n15\nEl Azul\nJunior H & Peso Pluma\n95\n187225\nsingle\n1\n2023-02-10\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27333ed35...\n\n\n915\n2023-06-05\n16\nCalm Down (with Selena Gomez)\nRema\n81\n239317\nalbum\n22\n2023-04-27\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273963265...\n\n\n916\n2023-06-05\n17\nPopular (with Playboi Carti & Madonna)\nThe Weeknd & Playboi Carti & Madonna\n79\n215466\nsingle\n1\n2023-06-02\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273eac1ac...\n\n\n917\n2023-06-05\n18\nBESO\nROSALÍA & Rauw Alejandro\n96\n194543\nsingle\n3\n2023-03-24\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2734d6cf0...\n\n\n918\n2023-06-05\n19\nDance The Night (From Barbie The Album)\nDua Lipa\n88\n176579\nsingle\n1\n2023-05-25\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2737dd3ba...\n\n\n919\n2023-06-05\n20\nPRC\nPeso Pluma & Natanael Cano\n95\n184066\nsingle\n1\n2023-01-23\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737be314...\n\n\n920\n2023-06-05\n21\nYandel 150\nYandel\n90\n216148\nalbum\n17\n2023-01-13\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273b2aec0...\n\n\n921\n2023-06-05\n22\nAnti-Hero\nTaylor Swift\n94\n200690\nalbum\n13\n2022-10-21\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273bb54dd...\n\n\n922\n2023-06-05\n23\nCreepin' (with The Weeknd & 21 Savage)\nMetro Boomin\n95\n221520\nalbum\n15\n2022-12-02\nTrue\nhttps://i.scdn.co/image/ab67616d0000b27313e54d...\n\n\n923\n2023-06-05\n24\nSee You Again (feat. Kali Uchis)\nTyler, The Creator\n95\n180386\nalbum\n14\n2017-07-21\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2738940ac...\n\n\n924\n2023-06-05\n25\nCruel Summer\nTaylor Swift\n93\n178426\nalbum\n18\n2019-08-23\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273e787cf...\n\n\n925\n2023-06-05\n26\nAll My Life (feat. J. Cole)\nLil Durk\n83\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n926\n2023-06-05\n27\nLast Night\nMorgan Wallen\n89\n163854\nalbum\n36\n2023-03-03\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273705079...\n\n\n927\n2023-06-05\n28\nDie For You (with Ariana Grande) - Remix\nThe Weeknd\n88\n232857\nalbum\n21\n2023-03-14\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2738ad8f5...\n\n\n928\n2023-06-05\n29\nI'm Good (Blue)\nDavid Guetta & Bebe Rexha\n94\n175238\nsingle\n2\n2022-08-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273933c03...\n\n\n929\n2023-06-05\n30\nAngels Like You\nMiley Cyrus\n93\n196453\nalbum\n15\n2020-11-27\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2738cffb7...\n\n\n930\n2023-06-05\n31\nSay Yes To Heaven\nLana Del Rey\n90\n209156\nsingle\n2\n2023-05-19\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273aa2770...\n\n\n931\n2023-06-05\n32\nBye\nPeso Pluma\n85\n212976\nsingle\n1\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b27310c8a0...\n\n\n932\n2023-06-05\n33\nAcróstico\nShakira\n92\n170785\nsingle\n1\n2023-05-11\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273bd6bbd...\n\n\n933\n2023-06-05\n34\nAngel Pt. 1 (feat. Jimin of BTS, JVKE & Muni L...\nFast & Furious: The Fast Saga & Jimin & BTS\n87\n175802\nsingle\n2\n2023-05-17\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273445afb...\n\n\n934\n2023-06-05\n35\nAMG\nNatanael Cano & Peso Pluma & Gabito Ballesteros\n94\n174942\nsingle\n1\n2022-11-24\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273fbf10e...\n\n\n935\n2023-06-05\n36\nBoy's a Liar Pt. 2\nPinkPantheress & Ice Spice\n95\n131013\nsingle\n2\n2023-02-03\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27342c5ba...\n\n\n936\n2023-06-05\n37\nI Wanna Be Yours\nArctic Monkeys\n94\n183956\nalbum\n12\n2013-09-09\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2734ae1c4...\n\n\n937\n2023-06-05\n38\nLos del Espacio\nLIT killah\n77\n338000\nsingle\n1\n2023-06-01\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27352c8b9...\n\n\n938\n2023-06-05\n39\nFrágil\nYahritza Y Su Esencia & Grupo Frontera\n91\n160517\nsingle\n1\n2023-04-07\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27357ed58...\n\n\n939\n2023-06-05\n40\nAnnihilate (Spider-Man: Across the Spider-Vers...\nMetro Boomin\n78\n231746\nalbum\n13\n2023-06-02\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2736ed9ae...\n\n\n940\n2023-06-05\n41\nDie For You\nThe Weeknd\n91\n260253\nalbum\n18\n2016-11-24\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273a04841...\n\n\n941\n2023-06-05\n42\nI Ain't Worried\nOneRepublic\n55\n148120\nalbum\n18\n2023-02-15\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273fedd3d...\n\n\n942\n2023-06-05\n43\nCalling (Spider-Man: Across the Spider-Verse) ...\nMetro Boomin\n77\n219453\nalbum\n13\n2023-06-02\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2736ed9ae...\n\n\n943\n2023-06-05\n44\nPeople\nLibianca\n93\n184791\nsingle\n1\n2022-12-06\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273fc342f...\n\n\n944\n2023-06-05\n45\nTattoo\nLoreen\n93\n183374\nsingle\n1\n2023-02-25\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2732b0ba8...\n\n\n945\n2023-06-05\n46\nTá OK\nDENNIS & MC Kevin o Chris\n82\n92093\nsingle\n1\n2023-05-04\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2736b19d4...\n\n\n946\n2023-06-05\n47\nLa Bachata\nManuel Turizo\n87\n162637\nalbum\n15\n2023-03-17\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2734dd995...\n\n\n947\n2023-06-05\n48\nMoonlight\nKali Uchis\n87\n187723\nalbum\n15\n2023-03-03\nFalse\nhttps://i.scdn.co/image/ab67616d0000b27381fccd...\n\n\n948\n2023-06-05\n49\nPor las Noches\nPeso Pluma\n93\n239845\nsingle\n1\n2021-06-11\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273427d9a...\n\n\n949\n2023-06-05\n50\nStarboy\nThe Weeknd\n93\n230453\nalbum\n18\n2016-11-25\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2734718e2...\n\n\n\n\n\n\n\nQ : What are the top 5 artists of June 5th 2023?\n\ndate = '2023-06-05'\nglobal_50_df[(global_50_df['date']== date) & (global_50_df['position']&lt;6)]\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n900\n2023-06-05\n1\nElla Baila Sola\nEslabon Armado\n92\n165671\nalbum\n16\n2023-04-28\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273dfddf1...\n\n\n901\n2023-06-05\n2\nPeso Pluma: Bzrp Music Sessions, Vol. 55\nBizarrap & Peso Pluma\n89\n188361\nsingle\n1\n2023-06-01\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273155830...\n\n\n902\n2023-06-05\n3\nWHERE SHE GOES\nBad Bunny\n96\n231704\nsingle\n1\n2023-05-18\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273ab5c9c...\n\n\n903\n2023-06-05\n4\nLa Bebe - Remix\nYng Lvcas & Peso Pluma\n99\n234352\nsingle\n2\n2023-03-17\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273a04be3...\n\n\n904\n2023-06-05\n5\nun x100to\nGrupo Frontera & Bad Bunny\n100\n194563\nsingle\n1\n2023-04-17\nFalse\nhttps://i.scdn.co/image/ab67616d0000b273716c0b..."
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#unique",
    "href": "Course_Content/Week_3/1/Notebook.html#unique",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "8. Unique()",
    "text": "8. Unique()\nWho are the artists who have entered top 50 spotify playlist? ( over the entire time of the dataset)\n\nglobal_50_df['artist'].to_list()\n\n['Eslabon Armado',\n 'Grupo Frontera & Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Loreen',\n 'Harry Styles',\n 'KAROL G',\n 'Käärijä',\n 'Feid & Young Miko',\n 'Shakira',\n 'Metro Boomin',\n 'Tyler, The Creator',\n 'ROSALÍA & Rauw Alejandro',\n 'The Weeknd',\n 'Peso Pluma & Natanael Cano',\n 'Rema',\n 'Yandel',\n 'Junior H & Peso Pluma',\n 'PinkPantheress & Ice Spice',\n 'Taylor Swift',\n 'The Weeknd',\n 'Arctic Monkeys',\n 'Lil Durk & J. Cole',\n 'Morgan Wallen',\n 'David Guetta & Bebe Rexha',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'd4vd',\n 'Miley Cyrus',\n 'JVKE',\n 'Stephen Sanchez & Em Beihold',\n 'Jimin',\n 'Kali Uchis',\n 'Taylor Swift',\n 'Bizarrap & Shakira',\n 'Sam Smith',\n 'Tom Odell',\n 'The Weeknd',\n 'Peso Pluma',\n 'Alessandra',\n 'Ozuna',\n 'Libianca',\n 'OneRepublic',\n 'Manuel Turizo',\n 'Eminem',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'JISOO',\n 'Chino Pacas',\n 'Eslabon Armado',\n 'Grupo Frontera & Bad Bunny',\n 'FIFTY FIFTY',\n 'Yng Lvcas & Peso Pluma',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Harry Styles',\n 'Loreen',\n 'KAROL G',\n 'Shakira',\n 'Feid & Young Miko',\n 'Käärijä',\n 'Metro Boomin',\n 'ROSALÍA & Rauw Alejandro',\n 'The Weeknd',\n 'Rema',\n 'Tyler, The Creator',\n 'Yandel',\n 'Peso Pluma & Natanael Cano',\n 'Junior H & Peso Pluma',\n 'PinkPantheress & Ice Spice',\n 'Taylor Swift',\n 'The Weeknd',\n 'David Guetta & Bebe Rexha',\n 'Morgan Wallen',\n 'Arctic Monkeys',\n 'Lil Durk & J. Cole',\n 'Miley Cyrus',\n 'JVKE',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'd4vd',\n 'Bizarrap & Shakira',\n 'Tom Odell',\n 'Kali Uchis',\n 'Stephen Sanchez & Em Beihold',\n 'Sam Smith',\n 'The Weeknd',\n 'Taylor Swift',\n 'Jimin',\n 'Libianca',\n 'Ozuna',\n 'OneRepublic',\n 'Peso Pluma',\n 'Manuel Turizo',\n 'Eminem',\n 'Alessandra',\n 'Coldplay',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Chino Pacas',\n 'Eslabon Armado',\n 'Grupo Frontera & Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Harry Styles',\n 'Shakira',\n 'KAROL G',\n 'Feid & Young Miko',\n 'Loreen',\n 'ROSALÍA & Rauw Alejandro',\n 'Metro Boomin',\n 'The Weeknd',\n 'Yandel',\n 'Rema',\n 'Tyler, The Creator',\n 'Peso Pluma & Natanael Cano',\n 'Junior H & Peso Pluma',\n 'Käärijä',\n 'PinkPantheress & Ice Spice',\n 'Fast & Furious: The Fast Saga',\n 'Taylor Swift',\n 'Jimin',\n 'Morgan Wallen',\n 'Lil Durk & J. Cole',\n 'The Weeknd',\n 'Arctic Monkeys',\n 'David Guetta & Bebe Rexha',\n 'Miley Cyrus',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'd4vd',\n 'JVKE',\n 'Stephen Sanchez & Em Beihold',\n 'Bizarrap & Shakira',\n 'Kali Uchis',\n 'Tom Odell',\n 'The Weeknd',\n 'Sam Smith',\n 'Ozuna',\n 'Taylor Swift',\n 'OneRepublic',\n 'Manuel Turizo',\n 'Peso Pluma',\n 'Libianca',\n 'Coldplay',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Eminem',\n 'Lil Mabu',\n 'Bad Bunny',\n 'Eslabon Armado',\n 'Grupo Frontera & Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'Lana Del Rey',\n 'David Kushner',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'SZA',\n 'Shakira',\n 'Harry Styles',\n 'Feid & Young Miko',\n 'KAROL G',\n 'Loreen',\n 'ROSALÍA & Rauw Alejandro',\n 'Yandel',\n 'Rema',\n 'Peso Pluma & Natanael Cano',\n 'Metro Boomin',\n 'Junior H & Peso Pluma',\n 'The Weeknd',\n 'Tyler, The Creator',\n 'PinkPantheress & Ice Spice',\n 'Käärijä',\n 'Morgan Wallen',\n 'Lil Durk & J. Cole',\n 'Taylor Swift',\n 'David Guetta & Bebe Rexha',\n 'Post Malone',\n 'The Weeknd',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'Arctic Monkeys',\n 'Miley Cyrus',\n 'Bizarrap & Shakira',\n 'Jimin',\n 'Ozuna',\n 'Peso Pluma',\n 'The Weeknd',\n 'Manuel Turizo',\n 'JVKE',\n 'Taylor Swift',\n 'd4vd',\n 'Tom Odell',\n 'Stephen Sanchez & Em Beihold',\n 'Kali Uchis',\n 'OneRepublic',\n 'Sam Smith',\n 'Libianca',\n 'Eladio Carrion',\n 'Eslabon Armado',\n 'Bad Bunny',\n 'Grupo Frontera & Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'SZA',\n 'David Kushner',\n 'Harry Styles',\n 'Shakira',\n 'KAROL G',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Feid & Young Miko',\n 'Peso Pluma & Natanael Cano',\n 'Junior H & Peso Pluma',\n 'Lana Del Rey',\n 'ROSALÍA & Rauw Alejandro',\n 'Yandel',\n 'Rema',\n 'Loreen',\n 'Metro Boomin',\n 'The Weeknd',\n 'PinkPantheress & Ice Spice',\n 'Tyler, The Creator',\n 'Fuerza Regida',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'Morgan Wallen',\n 'Taylor Swift',\n 'David Guetta & Bebe Rexha',\n 'Arctic Monkeys',\n 'Lil Durk & J. Cole',\n 'The Weeknd',\n 'Käärijä',\n 'Miley Cyrus',\n 'Peso Pluma',\n 'Bizarrap & Shakira',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Taylor Swift',\n 'OneRepublic',\n 'Manuel Turizo',\n 'The Weeknd',\n 'd4vd',\n 'Ozuna',\n 'Stephen Sanchez & Em Beihold',\n 'JVKE',\n 'Eden Muñoz & Junior H',\n 'Libianca',\n 'Kali Uchis',\n 'Tom Odell',\n 'Jimin',\n 'Eslabon Armado',\n 'Bad Bunny',\n 'Grupo Frontera & Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Harry Styles',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'KAROL G',\n 'Feid & Young Miko',\n 'Shakira',\n 'Junior H & Peso Pluma',\n 'Peso Pluma & Natanael Cano',\n 'Tyler, The Creator',\n 'The Weeknd',\n 'Rema',\n 'Fuerza Regida',\n 'Metro Boomin',\n 'Lana Del Rey',\n 'PinkPantheress & Ice Spice',\n 'ROSALÍA & Rauw Alejandro',\n 'Loreen',\n 'Yandel',\n 'Morgan Wallen',\n 'Arctic Monkeys',\n 'Taylor Swift',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'Miley Cyrus',\n 'The Weeknd',\n 'Lil Durk & J. Cole',\n 'd4vd',\n 'Taylor Swift',\n 'David Guetta & Bebe Rexha',\n 'The Weeknd',\n 'JVKE',\n 'Stephen Sanchez & Em Beihold',\n 'OneRepublic',\n 'Tom Odell',\n 'Peso Pluma',\n 'Kali Uchis',\n 'Jimin',\n 'Libianca',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Chino Pacas',\n 'Bizarrap & Shakira',\n 'Coldplay',\n 'Käärijä',\n 'Manuel Turizo',\n 'Bad Bunny',\n 'Eslabon Armado',\n 'Grupo Frontera & Bad Bunny',\n 'FIFTY FIFTY',\n 'Yng Lvcas & Peso Pluma',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Harry Styles',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Shakira',\n 'KAROL G',\n 'Feid & Young Miko',\n 'Tyler, The Creator',\n 'Metro Boomin',\n 'ROSALÍA & Rauw Alejandro',\n 'Rema',\n 'Fuerza Regida',\n 'The Weeknd',\n 'Loreen',\n 'Lana Del Rey',\n 'PinkPantheress & Ice Spice',\n 'Arctic Monkeys',\n 'Yandel',\n 'Junior H & Peso Pluma',\n 'Peso Pluma & Natanael Cano',\n 'Morgan Wallen',\n 'Taylor Swift',\n 'Lil Durk & J. Cole',\n 'The Weeknd',\n 'Miley Cyrus',\n 'Taylor Swift',\n 'David Guetta & Bebe Rexha',\n 'd4vd',\n 'The Weeknd',\n 'Stephen Sanchez & Em Beihold',\n 'Tom Odell',\n 'Libianca',\n 'Kali Uchis',\n 'JVKE',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'OneRepublic',\n 'Beyoncé & Kendrick Lamar',\n 'Peso Pluma',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Sam Smith',\n 'Jimin',\n 'Coldplay',\n 'Bizarrap & Shakira',\n 'Manuel Turizo',\n 'Eslabon Armado',\n 'Bad Bunny',\n 'Grupo Frontera & Bad Bunny',\n 'FIFTY FIFTY',\n 'Yng Lvcas & Peso Pluma',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Harry Styles',\n 'Shakira',\n 'KAROL G',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Feid & Young Miko',\n 'Metro Boomin',\n 'Rema',\n 'ROSALÍA & Rauw Alejandro',\n 'Tyler, The Creator',\n 'The Weeknd',\n 'Fuerza Regida',\n 'Junior H & Peso Pluma',\n 'Yandel',\n 'Loreen',\n 'Peso Pluma & Natanael Cano',\n 'PinkPantheress & Ice Spice',\n 'Arctic Monkeys',\n 'Lil Durk & J. Cole',\n 'Miley Cyrus',\n 'Morgan Wallen',\n 'Taylor Swift',\n 'Lana Del Rey',\n 'The Weeknd',\n 'Taylor Swift',\n 'David Guetta & Bebe Rexha',\n 'd4vd',\n 'ENHYPEN',\n 'Tom Odell',\n 'Kali Uchis',\n 'Libianca',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'JVKE',\n 'Stephen Sanchez & Em Beihold',\n 'The Weeknd',\n 'OneRepublic',\n 'Manuel Turizo',\n 'Beyoncé & Kendrick Lamar',\n 'Sam Smith',\n 'Bizarrap & Shakira',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Coldplay',\n 'Peso Pluma',\n 'Eslabon Armado',\n 'Bad Bunny',\n 'Grupo Frontera & Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Harry Styles',\n 'KAROL G',\n 'Feid & Young Miko',\n 'Shakira',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Metro Boomin',\n 'ROSALÍA & Rauw Alejandro',\n 'Rema',\n 'The Weeknd',\n 'Tyler, The Creator',\n 'Fuerza Regida',\n 'Yandel',\n 'Junior H & Peso Pluma',\n 'PinkPantheress & Ice Spice',\n 'Peso Pluma & Natanael Cano',\n 'Loreen',\n 'Arctic Monkeys',\n 'Taylor Swift',\n 'Miley Cyrus',\n 'Lil Durk & J. Cole',\n 'Morgan Wallen',\n 'The Weeknd',\n 'David Guetta & Bebe Rexha',\n 'Taylor Swift',\n 'Lana Del Rey',\n 'd4vd',\n 'Tom Odell',\n 'The Weeknd',\n 'Libianca',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'Stephen Sanchez & Em Beihold',\n 'Kali Uchis',\n 'JVKE',\n 'OneRepublic',\n 'Manuel Turizo',\n 'Bizarrap & Shakira',\n 'Sam Smith',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Coldplay',\n 'Peso Pluma',\n 'Ozuna',\n 'Eden Muñoz & Junior H',\n 'Eslabon Armado',\n 'Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'Grupo Frontera & Bad Bunny',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Harry Styles',\n 'KAROL G',\n 'Feid & Young Miko',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Shakira',\n 'Fuerza Regida',\n 'Metro Boomin',\n 'Rema',\n 'ROSALÍA & Rauw Alejandro',\n 'Yandel',\n 'Junior H & Peso Pluma',\n 'Tyler, The Creator',\n 'The Weeknd',\n 'PinkPantheress & Ice Spice',\n 'Peso Pluma & Natanael Cano',\n 'Taylor Swift',\n 'The Weeknd',\n 'Arctic Monkeys',\n 'Morgan Wallen',\n 'Lil Durk & J. Cole',\n 'Loreen',\n 'Miley Cyrus',\n 'David Guetta & Bebe Rexha',\n 'Tina Turner',\n 'Tina Turner',\n 'd4vd',\n 'Lana Del Rey',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'Libianca',\n 'Taylor Swift',\n 'The Weeknd',\n 'Tom Odell',\n 'JVKE',\n 'Kali Uchis',\n 'Peso Pluma',\n 'OneRepublic',\n 'Bizarrap & Shakira',\n 'Stephen Sanchez & Em Beihold',\n 'Manuel Turizo',\n 'Sam Smith',\n 'Ozuna',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Eslabon Armado',\n 'Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'Grupo Frontera & Bad Bunny',\n 'Taylor Swift',\n 'Taylor Swift',\n 'FIFTY FIFTY',\n 'Taylor Swift',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Harry Styles',\n 'KAROL G',\n 'Lil Durk',\n 'Feid & Young Miko',\n 'Taylor Swift',\n 'Fuerza Regida',\n 'Rema',\n 'Shakira',\n 'Dua Lipa',\n 'Metro Boomin',\n 'ROSALÍA & Rauw Alejandro',\n 'Junior H & Peso Pluma',\n 'Yandel',\n 'The Weeknd',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Tyler, The Creator',\n 'Morgan Wallen',\n 'PinkPantheress & Ice Spice',\n 'Peso Pluma & Natanael Cano',\n 'David Guetta & Bebe Rexha',\n 'Loreen',\n 'Taylor Swift',\n 'The Weeknd',\n 'Arctic Monkeys',\n 'Miley Cyrus',\n 'Lana Del Rey',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'd4vd',\n 'Libianca',\n 'OneRepublic',\n 'Peso Pluma',\n 'Manuel Turizo',\n 'The Weeknd',\n 'JVKE',\n 'Tom Odell',\n 'Kali Uchis',\n 'Ozuna',\n 'Bizarrap & Shakira',\n 'Stephen Sanchez & Em Beihold',\n 'Eslabon Armado',\n 'Yng Lvcas & Peso Pluma',\n 'Grupo Frontera & Bad Bunny',\n 'Bad Bunny',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'Harry Styles',\n 'SZA',\n 'David Kushner',\n 'KAROL G',\n 'Feid & Young Miko',\n 'Fuerza Regida',\n 'Rema',\n 'Taylor Swift',\n 'Taylor Swift',\n 'Junior H & Peso Pluma',\n 'Shakira',\n 'Lil Durk',\n 'ROSALÍA & Rauw Alejandro',\n 'Metro Boomin',\n 'Yandel',\n 'Peso Pluma & Natanael Cano',\n 'The Weeknd',\n 'Tyler, The Creator',\n 'Morgan Wallen',\n 'PinkPantheress & Ice Spice',\n 'Taylor Swift',\n 'David Guetta & Bebe Rexha',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Arctic Monkeys',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'Dua Lipa',\n 'The Weeknd',\n 'Taylor Swift',\n 'Taylor Swift',\n 'Miley Cyrus',\n 'Loreen',\n 'Libianca',\n 'OneRepublic',\n 'd4vd',\n 'Peso Pluma',\n 'Manuel Turizo',\n 'Bizarrap & Shakira',\n 'Ozuna',\n 'Lana Del Rey',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Kali Uchis',\n 'The Weeknd',\n 'JVKE',\n 'Stephen Sanchez & Em Beihold',\n 'Eslabon Armado',\n 'Yng Lvcas & Peso Pluma',\n 'Bad Bunny',\n 'Grupo Frontera & Bad Bunny',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'SZA',\n 'Harry Styles',\n 'David Kushner',\n 'Fuerza Regida',\n 'KAROL G',\n 'Junior H & Peso Pluma',\n 'Feid & Young Miko',\n 'Taylor Swift',\n 'Rema',\n 'The Weeknd',\n 'Metro Boomin',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Tyler, The Creator',\n 'Peso Pluma & Natanael Cano',\n 'Morgan Wallen',\n 'Yandel',\n 'Taylor Swift',\n 'Miley Cyrus',\n 'PinkPantheress & Ice Spice',\n 'Arctic Monkeys',\n 'ROSALÍA & Rauw Alejandro',\n 'Lil Durk',\n 'Shakira',\n 'The Weeknd',\n 'd4vd',\n 'Lana Del Rey',\n 'Taylor Swift',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'David Guetta & Bebe Rexha',\n 'Libianca',\n 'Peso Pluma',\n 'Dua Lipa',\n 'OneRepublic',\n 'Kali Uchis',\n 'JVKE',\n 'Loreen',\n 'Stephen Sanchez & Em Beihold',\n 'The Weeknd',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Taylor Swift',\n 'Tom Odell',\n 'Manuel Turizo',\n 'Bizarrap & Shakira',\n 'Jimin',\n 'Eslabon Armado',\n 'Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'FIFTY FIFTY',\n 'Grupo Frontera & Bad Bunny',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Harry Styles',\n 'KAROL G',\n 'Feid & Young Miko',\n 'Fuerza Regida',\n 'Taylor Swift',\n 'Rema',\n 'Metro Boomin',\n 'The Weeknd',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Tyler, The Creator',\n 'Miley Cyrus',\n 'Morgan Wallen',\n 'ROSALÍA & Rauw Alejandro',\n 'Junior H & Peso Pluma',\n 'Arctic Monkeys',\n 'Lil Durk',\n 'The Weeknd',\n 'Yandel',\n 'Shakira',\n 'PinkPantheress & Ice Spice',\n 'Lana Del Rey',\n 'Taylor Swift',\n 'David Guetta & Bebe Rexha',\n 'd4vd',\n 'Taylor Swift',\n 'Peso Pluma & Natanael Cano',\n 'Libianca',\n 'OneRepublic',\n 'Dua Lipa',\n 'Kali Uchis',\n 'Loreen',\n 'Tom Odell',\n 'JVKE',\n 'Stephen Sanchez & Em Beihold',\n 'The Weeknd',\n 'Jimin',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'Coldplay',\n 'Peso Pluma',\n 'Manuel Turizo',\n 'Sam Smith',\n 'Bizarrap & Shakira',\n 'Eslabon Armado',\n 'Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'Grupo Frontera & Bad Bunny',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Harry Styles',\n 'Feid & Young Miko',\n 'KAROL G',\n 'Fuerza Regida',\n 'Rema',\n 'Taylor Swift',\n 'Metro Boomin',\n 'The Weeknd',\n 'Lil Durk',\n 'Tyler, The Creator',\n 'ROSALÍA & Rauw Alejandro',\n 'Junior H & Peso Pluma',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Miley Cyrus',\n 'Yandel',\n 'Lana Del Rey',\n 'The Weeknd',\n 'Morgan Wallen',\n 'Arctic Monkeys',\n 'PinkPantheress & Ice Spice',\n 'Shakira',\n 'David Guetta & Bebe Rexha',\n 'Taylor Swift',\n 'Dua Lipa',\n 'd4vd',\n 'Peso Pluma & Natanael Cano',\n 'Libianca',\n 'Taylor Swift',\n 'Loreen',\n 'Kali Uchis',\n 'JVKE',\n 'Tom Odell',\n 'OneRepublic',\n 'Stephen Sanchez & Em Beihold',\n 'The Weeknd',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'Jimin',\n 'Manuel Turizo',\n 'Peso Pluma',\n 'Sam Smith',\n 'Ozuna',\n 'Coldplay',\n 'Eslabon Armado',\n 'Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'Grupo Frontera & Bad Bunny',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'David Kushner',\n 'SZA',\n 'Harry Styles',\n 'Feid & Young Miko',\n 'KAROL G',\n 'Fuerza Regida',\n 'Rema',\n 'Metro Boomin',\n 'Taylor Swift',\n 'Junior H & Peso Pluma',\n 'The Weeknd',\n 'ROSALÍA & Rauw Alejandro',\n 'Tyler, The Creator',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Lil Durk',\n 'Miley Cyrus',\n 'Yandel',\n 'Lana Del Rey',\n 'Arctic Monkeys',\n 'Morgan Wallen',\n 'The Weeknd',\n 'Shakira',\n 'David Guetta & Bebe Rexha',\n 'Dua Lipa',\n 'Peso Pluma & Natanael Cano',\n 'Taylor Swift',\n 'd4vd',\n 'Libianca',\n 'PinkPantheress & Ice Spice',\n 'Loreen',\n 'Kali Uchis',\n 'OneRepublic',\n 'Tom Odell',\n 'JVKE',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'Jimin',\n 'Stephen Sanchez & Em Beihold',\n 'The Weeknd',\n 'Peso Pluma',\n 'Manuel Turizo',\n 'Ozuna',\n 'Sam Smith',\n 'Peso Pluma',\n 'Bizarrap & Shakira',\n 'Bizarrap & Peso Pluma',\n 'Eslabon Armado',\n 'Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'Grupo Frontera & Bad Bunny',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'David Kushner',\n 'Fuerza Regida',\n 'SZA',\n 'Harry Styles',\n 'Feid & Young Miko',\n 'KAROL G',\n 'Rema',\n 'Dua Lipa',\n 'ROSALÍA & Rauw Alejandro',\n 'Junior H & Peso Pluma',\n 'Taylor Swift',\n 'Lana Del Rey',\n 'Tyler, The Creator',\n 'Metro Boomin',\n 'Peso Pluma & Natanael Cano',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Lil Durk',\n 'The Weeknd',\n 'Shakira',\n 'Yandel',\n 'Miley Cyrus',\n 'Arctic Monkeys',\n 'Morgan Wallen',\n 'Taylor Swift',\n 'David Guetta & Bebe Rexha',\n 'PinkPantheress & Ice Spice',\n 'Peso Pluma',\n 'The Weeknd',\n 'Libianca',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'OneRepublic',\n 'Kali Uchis',\n 'Loreen',\n 'd4vd',\n 'Tom Odell',\n 'The Weeknd',\n 'JVKE',\n 'Stephen Sanchez & Em Beihold',\n 'Jimin',\n 'Manuel Turizo',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Bizarrap & Quevedo',\n 'Sam Smith',\n 'Bizarrap & Peso Pluma',\n 'Eslabon Armado',\n 'Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'Grupo Frontera & Bad Bunny',\n 'Miley Cyrus',\n 'FIFTY FIFTY',\n 'Fuerza Regida',\n 'Dave & Central Cee',\n 'The Weeknd & Playboi Carti & Madonna',\n 'David Kushner',\n 'Harry Styles',\n 'SZA',\n 'Feid & Young Miko',\n 'KAROL G',\n 'Rema',\n 'Dua Lipa',\n 'ROSALÍA & Rauw Alejandro',\n 'Junior H & Peso Pluma',\n 'Metro Boomin',\n 'Taylor Swift',\n 'Tyler, The Creator',\n 'Yandel',\n 'Lana Del Rey',\n 'Peso Pluma & Natanael Cano',\n 'Taylor Swift',\n 'Lil Durk',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'The Weeknd',\n 'Miley Cyrus',\n 'Morgan Wallen',\n 'David Guetta & Bebe Rexha',\n 'Shakira',\n 'Metro Boomin',\n 'PinkPantheress & Ice Spice',\n 'Arctic Monkeys',\n 'Peso Pluma',\n 'The Weeknd',\n 'Libianca',\n 'Loreen',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'Stray Kids',\n 'OneRepublic',\n 'Metro Boomin',\n 'LIT killah',\n 'Kali Uchis',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Tom Odell',\n 'Jimin',\n 'The Weeknd',\n 'Eslabon Armado',\n 'Bizarrap & Peso Pluma',\n 'Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'Grupo Frontera & Bad Bunny',\n 'Miley Cyrus',\n 'FIFTY FIFTY',\n 'Fuerza Regida',\n 'Harry Styles',\n 'David Kushner',\n 'SZA',\n 'Dave & Central Cee',\n 'Feid & Young Miko',\n 'KAROL G',\n 'Junior H & Peso Pluma',\n 'Rema',\n 'The Weeknd & Playboi Carti & Madonna',\n 'ROSALÍA & Rauw Alejandro',\n 'Dua Lipa',\n 'Peso Pluma & Natanael Cano',\n 'Yandel',\n 'Taylor Swift',\n 'Metro Boomin',\n 'Tyler, The Creator',\n 'Taylor Swift',\n 'Lil Durk',\n 'Morgan Wallen',\n 'The Weeknd',\n 'David Guetta & Bebe Rexha',\n 'Miley Cyrus',\n 'Lana Del Rey',\n 'Peso Pluma',\n 'Shakira',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'PinkPantheress & Ice Spice',\n 'Arctic Monkeys',\n 'LIT killah',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Metro Boomin',\n 'The Weeknd',\n 'OneRepublic',\n 'Metro Boomin',\n 'Libianca',\n 'Loreen',\n 'DENNIS & MC Kevin o Chris',\n 'Manuel Turizo',\n 'Kali Uchis',\n 'Peso Pluma',\n 'The Weeknd',\n 'Eslabon Armado',\n 'Bizarrap & Peso Pluma',\n 'Bad Bunny',\n 'Grupo Frontera & Bad Bunny',\n 'Yng Lvcas & Peso Pluma',\n 'FIFTY FIFTY',\n 'Miley Cyrus',\n 'Fuerza Regida',\n 'David Kushner',\n 'Harry Styles',\n 'SZA',\n 'Dave & Central Cee',\n 'Feid & Young Miko',\n 'Junior H & Peso Pluma',\n 'Rema',\n 'KAROL G',\n 'Tyler, The Creator',\n 'Peso Pluma & Natanael Cano',\n 'Metro Boomin',\n 'Miley Cyrus',\n 'Taylor Swift',\n 'Taylor Swift',\n 'Dua Lipa',\n 'Lana Del Rey',\n 'The Weeknd',\n 'Fast & Furious: The Fast Saga & Jimin & BTS',\n 'ROSALÍA & Rauw Alejandro',\n 'The Weeknd & Playboi Carti & Madonna',\n 'Arctic Monkeys',\n 'Yandel',\n 'Morgan Wallen',\n 'Metro Boomin',\n 'Lil Durk',\n 'Metro Boomin',\n 'The Weeknd',\n 'PinkPantheress & Ice Spice',\n 'David Guetta & Bebe Rexha',\n 'Natanael Cano & Peso Pluma & Gabito Ballesteros',\n 'Peso Pluma',\n 'LIT killah',\n 'Post Malone',\n 'Kali Uchis',\n 'Yahritza Y Su Esencia & Grupo Frontera',\n 'Libianca',\n 'Shakira',\n 'Metro Boomin',\n 'Jimin',\n 'OneRepublic',\n 'The Weeknd',\n 'Peso Pluma',\n ...]\n\n\n\ntop_50_artists_of_all_time = global_50_df['artist'].unique()"
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#length-average-median",
    "href": "Course_Content/Week_3/1/Notebook.html#length-average-median",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "9. Length, Average, Median",
    "text": "9. Length, Average, Median\n\ntop_50_artists_of_all_time\n\narray(['Eslabon Armado', 'Grupo Frontera & Bad Bunny',\n       'Yng Lvcas & Peso Pluma', 'FIFTY FIFTY', 'Miley Cyrus',\n       'David Kushner', 'SZA', 'Loreen', 'Harry Styles', 'KAROL G',\n       'Käärijä', 'Feid & Young Miko', 'Shakira', 'Metro Boomin',\n       'Tyler, The Creator', 'ROSALÍA & Rauw Alejandro', 'The Weeknd',\n       'Peso Pluma & Natanael Cano', 'Rema', 'Yandel',\n       'Junior H & Peso Pluma', 'PinkPantheress & Ice Spice',\n       'Taylor Swift', 'Arctic Monkeys', 'Lil Durk & J. Cole',\n       'Morgan Wallen', 'David Guetta & Bebe Rexha',\n       'Natanael Cano & Peso Pluma & Gabito Ballesteros', 'd4vd', 'JVKE',\n       'Stephen Sanchez & Em Beihold', 'Jimin', 'Kali Uchis',\n       'Bizarrap & Shakira', 'Sam Smith', 'Tom Odell', 'Peso Pluma',\n       'Alessandra', 'Ozuna', 'Libianca', 'OneRepublic', 'Manuel Turizo',\n       'Eminem', 'Yahritza Y Su Esencia & Grupo Frontera', 'JISOO',\n       'Chino Pacas', 'Coldplay', 'Fast & Furious: The Fast Saga',\n       'Lil Mabu', 'Bad Bunny', 'Lana Del Rey',\n       'Fast & Furious: The Fast Saga & Jimin & BTS', 'Post Malone',\n       'Eladio Carrion', 'Fuerza Regida', 'Eden Muñoz & Junior H',\n       'Beyoncé & Kendrick Lamar', 'ENHYPEN', 'Tina Turner', 'Lil Durk',\n       'Dua Lipa', 'Bizarrap & Peso Pluma', 'Bizarrap & Quevedo',\n       'Dave & Central Cee', 'The Weeknd & Playboi Carti & Madonna',\n       'Stray Kids', 'LIT killah', 'DENNIS & MC Kevin o Chris',\n       'Halsey & SUGA', 'Sky Rompiendo & Feid & Myke Towers', 'BTS',\n       'J Hus & Drake', 'Saiko & Feid & Quevedo', 'Glass Animals',\n       'Doja Cat'], dtype=object)\n\n\n\nlen(top_50_artists_of_all_time)\n\n75\n\n\nQ : How long are the song usually?\nAverage , Median\n\nglobal_50_df.columns\n\nIndex(['date', 'position', 'song', 'artist', 'popularity', 'duration_ms',\n       'album_type', 'total_tracks', 'release_date', 'is_explicit',\n       'album_cover_url'],\n      dtype='object')\n\n\nglobal_50_df.duration_ms.mean()\n\nglobal_50_df.duration_ms.median()\n\n194563.0\n\n\nWho are the artists with explicit music that were in the top 50 daily spotify playlists?\n\nglobal_50_df[global_50_df['is_explicit']]['artist'].unique()\n\narray(['Yng Lvcas & Peso Pluma', 'KAROL G', 'Feid & Young Miko',\n       'Metro Boomin', 'Tyler, The Creator', 'Peso Pluma & Natanael Cano',\n       'Yandel', 'Lil Durk & J. Cole', 'Morgan Wallen',\n       'David Guetta & Bebe Rexha',\n       'Natanael Cano & Peso Pluma & Gabito Ballesteros', 'Tom Odell',\n       'The Weeknd', 'Eminem', 'Chino Pacas', 'Lil Mabu', 'Bad Bunny',\n       'Post Malone', 'Eladio Carrion', 'Fuerza Regida',\n       'Beyoncé & Kendrick Lamar', 'Taylor Swift', 'Lil Durk',\n       'Peso Pluma', 'Bizarrap & Peso Pluma', 'Dave & Central Cee',\n       'The Weeknd & Playboi Carti & Madonna', 'Halsey & SUGA',\n       'Sky Rompiendo & Feid & Myke Towers', 'Saiko & Feid & Quevedo',\n       'Doja Cat'], dtype=object)"
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#value-counts-sort-values",
    "href": "Course_Content/Week_3/1/Notebook.html#value-counts-sort-values",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "10. Value Counts, Sort Values",
    "text": "10. Value Counts, Sort Values\nthe artists with explicit music that were in the top 50 daily spotify playlists, How many spots did they take?\n\nglobal_50_df[global_50_df['is_explicit']]['artist'].value_counts()\n\nYng Lvcas & Peso Pluma                             36\nThe Weeknd                                         36\nFeid & Young Miko                                  36\nMetro Boomin                                       36\nTyler, The Creator                                 36\nPeso Pluma & Natanael Cano                         36\nYandel                                             36\nMorgan Wallen                                      36\nDavid Guetta & Bebe Rexha                          36\nKAROL G                                            36\nBad Bunny                                          33\nFuerza Regida                                      32\nTom Odell                                          29\nNatanael Cano & Peso Pluma & Gabito Ballesteros    22\nLil Durk                                           20\nBizarrap & Peso Pluma                              20\nDave & Central Cee                                 19\nThe Weeknd & Playboi Carti & Madonna               14\nSky Rompiendo & Feid & Myke Towers                 13\nLil Durk & J. Cole                                 10\nPeso Pluma                                         10\nTaylor Swift                                       10\nDoja Cat                                            5\nChino Pacas                                         3\nHalsey & SUGA                                       3\nEminem                                              3\nBeyoncé & Kendrick Lamar                            2\nEladio Carrion                                      1\nPost Malone                                         1\nSaiko & Feid & Quevedo                              1\nLil Mabu                                            1\nName: artist, dtype: int64\n\n\nMean duration of songs by each artist\n\nglobal_50_df[['artist','duration_ms']].groupby('artist').mean()\n\n\n\n\n\n\n\n\nduration_ms\n\n\nartist\n\n\n\n\n\nAlessandra\n147979.0\n\n\nArctic Monkeys\n183956.0\n\n\nBTS\n229953.0\n\n\nBad Bunny\n231704.0\n\n\nBeyoncé & Kendrick Lamar\n260962.0\n\n\n...\n...\n\n\nTyler, The Creator\n180386.0\n\n\nYahritza Y Su Esencia & Grupo Frontera\n160517.0\n\n\nYandel\n216148.0\n\n\nYng Lvcas & Peso Pluma\n234352.0\n\n\nd4vd\n242484.0\n\n\n\n\n75 rows × 1 columns\n\n\n\n\nglobal_50_df[['artist','duration_ms']].groupby('artist').mean().sort_values(by='duration_ms')\n\n\n\n\n\n\n\n\nduration_ms\n\n\nartist\n\n\n\n\n\nLil Mabu\n88304.0\n\n\nDENNIS & MC Kevin o Chris\n92093.0\n\n\nChino Pacas\n112087.0\n\n\nPinkPantheress & Ice Spice\n131013.0\n\n\nAlessandra\n147979.0\n\n\n...\n...\n\n\nColdplay\n266773.0\n\n\nDoja Cat\n277043.0\n\n\nTina Turner\n280020.0\n\n\nSaiko & Feid & Quevedo\n288000.0\n\n\nLIT killah\n338000.0\n\n\n\n\n75 rows × 1 columns\n\n\n\n\nglobal_50_df[['artist','duration_ms']].groupby('artist').mean().sort_values(by='duration_ms').head(5)\n\n\n\n\n\n\n\n\nduration_ms\n\n\nartist\n\n\n\n\n\nLil Mabu\n88304.0\n\n\nDENNIS & MC Kevin o Chris\n92093.0\n\n\nChino Pacas\n112087.0\n\n\nPinkPantheress & Ice Spice\n131013.0\n\n\nAlessandra\n147979.0\n\n\n\n\n\n\n\n\nglobal_50_df[['artist','duration_ms']].groupby('artist').mean().sort_values(by='duration_ms').tail(5)\n\n\n\n\n\n\n\n\nduration_ms\n\n\nartist\n\n\n\n\n\nColdplay\n266773.0\n\n\nDoja Cat\n277043.0\n\n\nTina Turner\n280020.0\n\n\nSaiko & Feid & Quevedo\n288000.0\n\n\nLIT killah\n338000.0"
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#str.startswith",
    "href": "Course_Content/Week_3/1/Notebook.html#str.startswith",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "11. str.startswith",
    "text": "11. str.startswith\nQ : Who are the artists whose name starts with Lil\n\nglobal_50_df[global_50_df['artist'].str.startswith('Lil')]['artist'].unique()\n\narray(['Lil Durk & J. Cole', 'Lil Mabu', 'Lil Durk'], dtype=object)\n\n\nQ : Spots that the above artists took?\n\nlil_artists = global_50_df[global_50_df['artist'].str.startswith('Lil')]['artist'].unique().tolist()\n\n\nglobal_50_df[global_50_df['artist'].isin(lil_artists)]\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n25\n2023-05-18\n26\nAll My Life (feat. J. Cole)\nLil Durk & J. Cole\n84\n223878\nsingle\n1\n2023-05-12\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737c173b...\n\n\n77\n2023-05-19\n28\nAll My Life (feat. J. Cole)\nLil Durk & J. Cole\n85\n223878\nsingle\n1\n2023-05-12\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737c173b...\n\n\n126\n2023-05-20\n27\nAll My Life (feat. J. Cole)\nLil Durk & J. Cole\n86\n223878\nsingle\n1\n2023-05-12\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737c173b...\n\n\n149\n2023-05-20\n50\nMATHEMATICAL DISRESPECT\nLil Mabu\n85\n88304\nsingle\n1\n2023-05-05\nTrue\nhttps://i.scdn.co/image/ab67616d0000b273a56e7d...\n\n\n176\n2023-05-21\n27\nAll My Life (feat. J. Cole)\nLil Durk & J. Cole\n87\n223878\nsingle\n1\n2023-05-12\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737c173b...\n\n\n230\n2023-05-22\n31\nAll My Life (feat. J. Cole)\nLil Durk & J. Cole\n87\n223878\nsingle\n1\n2023-05-12\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737c173b...\n\n\n281\n2023-05-23\n32\nAll My Life (feat. J. Cole)\nLil Durk & J. Cole\n88\n223878\nsingle\n1\n2023-05-12\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737c173b...\n\n\n328\n2023-05-24\n29\nAll My Life (feat. J. Cole)\nLil Durk & J. Cole\n88\n223878\nsingle\n1\n2023-05-12\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737c173b...\n\n\n375\n2023-05-25\n26\nAll My Life (feat. J. Cole)\nLil Durk & J. Cole\n89\n223878\nsingle\n1\n2023-05-12\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737c173b...\n\n\n427\n2023-05-26\n28\nAll My Life (feat. J. Cole)\nLil Durk & J. Cole\n89\n223878\nsingle\n1\n2023-05-12\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737c173b...\n\n\n477\n2023-05-27\n28\nAll My Life (feat. J. Cole)\nLil Durk & J. Cole\n89\n223878\nsingle\n1\n2023-05-12\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2737c173b...\n\n\n513\n2023-05-28\n14\nAll My Life (feat. J. Cole)\nLil Durk\n72\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n567\n2023-05-29\n18\nAll My Life (feat. J. Cole)\nLil Durk\n77\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n627\n2023-05-30\n28\nAll My Life (feat. J. Cole)\nLil Durk\n79\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n673\n2023-05-31\n24\nAll My Life (feat. J. Cole)\nLil Durk\n80\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n716\n2023-06-01\n17\nAll My Life (feat. J. Cole)\nLil Durk\n81\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n770\n2023-06-02\n21\nAll My Life (feat. J. Cole)\nLil Durk\n82\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n823\n2023-06-03\n24\nAll My Life (feat. J. Cole)\nLil Durk\n82\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n876\n2023-06-04\n27\nAll My Life (feat. J. Cole)\nLil Durk\n83\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n925\n2023-06-05\n26\nAll My Life (feat. J. Cole)\nLil Durk\n83\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n982\n2023-06-06\n33\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n1027\n2023-06-07\n28\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n1077\n2023-06-08\n28\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n1131\n2023-06-09\n32\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n1185\n2023-06-10\n36\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n1244\n2023-06-11\n45\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n1398\n2023-06-14\n49\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n1444\n2023-06-15\n45\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n1493\n2023-06-16\n44\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n1540\n2023-06-17\n41\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n1799\n2023-06-22\n50\nAll My Life (feat. J. Cole)\nLil Durk\n84\n223878\nalbum\n21\n2023-05-26\nTrue\nhttps://i.scdn.co/image/ab67616d0000b2736234c2...\n\n\n\n\n\n\n\n\nglobal_50_df[global_50_df['artist'].isin(lil_artists)].shape\n\n(31, 11)"
  },
  {
    "objectID": "Course_Content/Week_3/1/Notebook.html#identifying-and-removing-duplicates",
    "href": "Course_Content/Week_3/1/Notebook.html#identifying-and-removing-duplicates",
    "title": "Lecture 3.1 : Introduction to Pandas",
    "section": "12. Identifying and removing duplicates",
    "text": "12. Identifying and removing duplicates\nQ : Unique songs by artists whose names starts with lil\n\nlil_artists_songs_df = global_50_df[global_50_df['artist'].isin(lil_artists)][['artist','song']]\nlil_artists_songs_df\n\n\n\n\n\n\n\n\nartist\nsong\n\n\n\n\n25\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n77\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n126\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n149\nLil Mabu\nMATHEMATICAL DISRESPECT\n\n\n176\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n230\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n281\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n328\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n375\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n427\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n477\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n513\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n567\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n627\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n673\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n716\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n770\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n823\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n876\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n925\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n982\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n1027\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n1077\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n1131\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n1185\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n1244\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n1398\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n1444\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n1493\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n1540\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n1799\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n\n\n\n\n\n\nlil_artists_songs_df[['artist','song']].duplicated()\n\n25      False\n77       True\n126      True\n149     False\n176      True\n230      True\n281      True\n328      True\n375      True\n427      True\n477      True\n513     False\n567      True\n627      True\n673      True\n716      True\n770      True\n823      True\n876      True\n925      True\n982      True\n1027     True\n1077     True\n1131     True\n1185     True\n1244     True\n1398     True\n1444     True\n1493     True\n1540     True\n1799     True\ndtype: bool\n\n\n\nlil_artists_songs_df[lil_artists_songs_df[['artist','song']].duplicated() != True]\n\n\n\n\n\n\n\n\nartist\nsong\n\n\n\n\n25\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n149\nLil Mabu\nMATHEMATICAL DISRESPECT\n\n\n513\nLil Durk\nAll My Life (feat. J. Cole)\n\n\n\n\n\n\n\n\nlil_artists_songs_df.drop_duplicates()\n\n\n\n\n\n\n\n\nartist\nsong\n\n\n\n\n25\nLil Durk & J. Cole\nAll My Life (feat. J. Cole)\n\n\n149\nLil Mabu\nMATHEMATICAL DISRESPECT\n\n\n513\nLil Durk\nAll My Life (feat. J. Cole)"
  },
  {
    "objectID": "Course_Content/Week_3/3/home.html",
    "href": "Course_Content/Week_3/3/home.html",
    "title": "3.3 - Data Collection (Part 2 of 2) & Data Processing (Part 1)",
    "section": "",
    "text": "Continuing from SQL\n\nData Collection\n\nAPI:\n\nWhat is an API?\nRequests Library\n\nScraping\n\nBeautifulSoup Library\nRobots.txt\n\n\nData File Types\nData Processing\n\nLibraries\nNumpy\nScipy\nRestart pandas .. till datetime\n\n\n\n\n\n\nSlides\nNotebook"
  },
  {
    "objectID": "Course_Content/Week_3/3/home.html#objectives",
    "href": "Course_Content/Week_3/3/home.html#objectives",
    "title": "3.3 - Data Collection (Part 2 of 2) & Data Processing (Part 1)",
    "section": "",
    "text": "Continuing from SQL\n\nData Collection\n\nAPI:\n\nWhat is an API?\nRequests Library\n\nScraping\n\nBeautifulSoup Library\nRobots.txt\n\n\nData File Types\nData Processing\n\nLibraries\nNumpy\nScipy\nRestart pandas .. till datetime"
  },
  {
    "objectID": "Course_Content/Week_3/3/home.html#materials",
    "href": "Course_Content/Week_3/3/home.html#materials",
    "title": "3.3 - Data Collection (Part 2 of 2) & Data Processing (Part 1)",
    "section": "",
    "text": "Slides\nNotebook"
  },
  {
    "objectID": "Course_Content/Week_3/3/Notebook.html",
    "href": "Course_Content/Week_3/3/Notebook.html",
    "title": "Lecture 3.3",
    "section": "",
    "text": "!pip install beautifulsoup4\n\nRequirement already satisfied: beautifulsoup4 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (4.12.2)\nRequirement already satisfied: soupsieve&gt;1.2 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from beautifulsoup4) (2.4.1)\n\n\n\n\n\nimport requests\n\n\nURL = \"https://csc380.beingenfa.com/Syllabus/Key_Info.html\"\n\n\nr = requests.get(URL)\n\n\nr.status_code\n\n200\n\n\n\nr.text\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\\n\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"generator\" content=\"quarto-1.3.361\"&gt;\\n\\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\\n\\n\\n&lt;title&gt;CSC 380 – key_info&lt;/title&gt;\\n&lt;style&gt;\\ncode{white-space: pre-wrap;}\\nspan.smallcaps{font-variant: small-caps;}\\ndiv.columns{display: flex; gap: min(4vw, 1.5em);}\\ndiv.column{flex: auto; overflow-x: auto;}\\ndiv.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}\\nul.task-list{list-style: none;}\\nul.task-list li input[type=\"checkbox\"] {\\n  width: 0.8em;\\n  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ \\n  vertical-align: middle;\\n}\\n&lt;/style&gt;\\n\\n\\n&lt;script src=\"../site_libs/quarto-nav/quarto-nav.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-nav/headroom.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/clipboard/clipboard.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/autocomplete.umd.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/fuse.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/quarto-search.js\"&gt;&lt;/script&gt;\\n&lt;meta name=\"quarto:offset\" content=\"../\"&gt;\\n&lt;script src=\"../site_libs/quarto-html/quarto.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/popper.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/tippy.umd.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/anchor.min.js\"&gt;&lt;/script&gt;\\n&lt;link href=\"../site_libs/quarto-html/tippy.css\" rel=\"stylesheet\"&gt;\\n&lt;link href=\"../site_libs/quarto-html/quarto-syntax-highlighting.css\" rel=\"stylesheet\" id=\"quarto-text-highlighting-styles\"&gt;\\n&lt;script src=\"../site_libs/bootstrap/bootstrap.min.js\"&gt;&lt;/script&gt;\\n&lt;link href=\"../site_libs/bootstrap/bootstrap-icons.css\" rel=\"stylesheet\"&gt;\\n&lt;link href=\"../site_libs/bootstrap/bootstrap.min.css\" rel=\"stylesheet\" id=\"quarto-bootstrap\" data-mode=\"light\"&gt;\\n&lt;script id=\"quarto-search-options\" type=\"application/json\"&gt;{\\n  \"location\": \"sidebar\",\\n  \"copy-button\": false,\\n  \"collapse-after\": 3,\\n  \"panel-placement\": \"start\",\\n  \"type\": \"textbox\",\\n  \"limit\": 20,\\n  \"language\": {\\n    \"search-no-results-text\": \"No results\",\\n    \"search-matching-documents-text\": \"matching documents\",\\n    \"search-copy-link-title\": \"Copy link to search\",\\n    \"search-hide-matches-text\": \"Hide additional matches\",\\n    \"search-more-match-text\": \"more match in this document\",\\n    \"search-more-matches-text\": \"more matches in this document\",\\n    \"search-clear-button-title\": \"Clear\",\\n    \"search-detached-cancel-button-title\": \"Cancel\",\\n    \"search-submit-button-title\": \"Submit\",\\n    \"search-label\": \"Search\"\\n  }\\n}&lt;/script&gt;\\n\\n\\n&lt;link rel=\"stylesheet\" href=\"../styles.css\"&gt;\\n&lt;/head&gt;\\n\\n&lt;body class=\"nav-sidebar docked\"&gt;\\n\\n&lt;div id=\"quarto-search-results\"&gt;&lt;/div&gt;\\n  &lt;header id=\"quarto-header\" class=\"headroom fixed-top\"&gt;\\n  &lt;nav class=\"quarto-secondary-nav\"&gt;\\n    &lt;div class=\"container-fluid d-flex\"&gt;\\n      &lt;button type=\"button\" class=\"quarto-btn-toggle btn\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;\\n        &lt;i class=\"bi bi-layout-text-sidebar-reverse\"&gt;&lt;/i&gt;\\n      &lt;/button&gt;\\n      &lt;nav class=\"quarto-page-breadcrumbs\" aria-label=\"breadcrumb\"&gt;&lt;ol class=\"breadcrumb\"&gt;&lt;li class=\"breadcrumb-item\"&gt;&lt;a href=\"../Syllabus/Key_Info.html\"&gt;Syllabus&lt;/a&gt;&lt;/li&gt;&lt;li class=\"breadcrumb-item\"&gt;&lt;a href=\"../Syllabus/Key_Info.html\"&gt;Key Info&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/nav&gt;\\n      &lt;a class=\"flex-grow-1\" role=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;      \\n      &lt;/a&gt;\\n      &lt;button type=\"button\" class=\"btn quarto-search-button\" aria-label=\"\" onclick=\"window.quartoOpenSearch();\"&gt;\\n        &lt;i class=\"bi bi-search\"&gt;&lt;/i&gt;\\n      &lt;/button&gt;\\n    &lt;/div&gt;\\n  &lt;/nav&gt;\\n&lt;/header&gt;\\n&lt;!-- content --&gt;\\n&lt;div id=\"quarto-content\" class=\"quarto-container page-columns page-rows-contents page-layout-article\"&gt;\\n&lt;!-- sidebar --&gt;\\n  &lt;nav id=\"quarto-sidebar\" class=\"sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto\"&gt;\\n    &lt;div class=\"pt-lg-2 mt-2 text-left sidebar-header\"&gt;\\n    &lt;div class=\"sidebar-title mb-0 py-0\"&gt;\\n      &lt;a href=\"../\"&gt;CSC 380&lt;/a&gt; \\n    &lt;/div&gt;\\n      &lt;/div&gt;\\n        &lt;div class=\"mt-2 flex-shrink-0 align-items-center\"&gt;\\n        &lt;div class=\"sidebar-search\"&gt;\\n        &lt;div id=\"quarto-search\" class=\"\" title=\"Search\"&gt;&lt;/div&gt;\\n        &lt;/div&gt;\\n        &lt;/div&gt;\\n    &lt;div class=\"sidebar-menu-container\"&gt; \\n    &lt;ul class=\"list-unstyled mt-1\"&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-1\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Course Content&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-1\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-1\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_1/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 1&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_2/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 2&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_3/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 3&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_4/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 4&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-2\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Homework&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-2\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-2\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Homework/HW1.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;HW1: Probability&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-3\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Ethics Discussions&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-3\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-3\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_2.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W2: Political Content&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_3.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W3: Creative Work&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_4.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W4: Mental Health Support&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-4\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Syllabus&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-4\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-4\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Syllabus/Key_Info.html\" class=\"sidebar-item-text sidebar-link active\"&gt;\\n &lt;span class=\"menu-text\"&gt;Key Info&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Syllabus/Syllabus.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Official Syllabus&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n    &lt;/ul&gt;\\n    &lt;/div&gt;\\n&lt;/nav&gt;\\n&lt;div id=\"quarto-sidebar-glass\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\"&gt;&lt;/div&gt;\\n&lt;!-- margin-sidebar --&gt;\\n    &lt;div id=\"quarto-margin-sidebar\" class=\"sidebar margin-sidebar\"&gt;\\n        &lt;nav id=\"TOC\" role=\"doc-toc\" class=\"toc-active\"&gt;\\n    &lt;h2 id=\"toc-title\"&gt;On this page&lt;/h2&gt;\\n   \\n  &lt;ul&gt;\\n  &lt;li&gt;&lt;a href=\"#syllabus-key-info\" id=\"toc-syllabus-key-info\" class=\"nav-link active\" data-scroll-target=\"#syllabus-key-info\"&gt;Syllabus (Key Info)&lt;/a&gt;\\n  &lt;ul class=\"collapse\"&gt;\\n  &lt;li&gt;&lt;a href=\"#description\" id=\"toc-description\" class=\"nav-link\" data-scroll-target=\"#description\"&gt;Description&lt;/a&gt;&lt;/li&gt;\\n  &lt;li&gt;&lt;a href=\"#course-objective\" id=\"toc-course-objective\" class=\"nav-link\" data-scroll-target=\"#course-objective\"&gt;Course Objective&lt;/a&gt;&lt;/li&gt;\\n  &lt;li&gt;&lt;a href=\"#expected-learning-outcomes\" id=\"toc-expected-learning-outcomes\" class=\"nav-link\" data-scroll-target=\"#expected-learning-outcomes\"&gt;Expected Learning Outcomes&lt;/a&gt;&lt;/li&gt;\\n  &lt;/ul&gt;&lt;/li&gt;\\n  &lt;/ul&gt;\\n&lt;/nav&gt;\\n    &lt;/div&gt;\\n&lt;!-- main --&gt;\\n&lt;main class=\"content\" id=\"quarto-document-content\"&gt;\\n\\n\\n\\n&lt;section id=\"syllabus-key-info\" class=\"level1\"&gt;\\n&lt;h1&gt;Syllabus (Key Info)&lt;/h1&gt;\\n&lt;section id=\"description\" class=\"level2\"&gt;\\n&lt;h2 class=\"anchored\" data-anchor-id=\"description\"&gt;Description&lt;/h2&gt;\\n&lt;p&gt;The course introduces students to the principles of data science, which are essential for computer scientists to make effective decisions in their professional careers. In today’s data-driven world, a wide range of computer science sub-disciplines heavily rely on data collection, analysis, and interpretation. With the pervasive presence of artificial intelligence (AI) in our lives, understanding the basics of how these systems work is becoming increasingly important. Additionally, it covers the basics of artificial intelligence (AI) systems and examines practical use cases, current news, and ethical considerations through readings and discussions.&lt;/p&gt;\\n&lt;/section&gt;\\n&lt;section id=\"course-objective\" class=\"level2\"&gt;\\n&lt;h2 class=\"anchored\" data-anchor-id=\"course-objective\"&gt;Course Objective&lt;/h2&gt;\\n&lt;p&gt;Course Objectives&lt;/p&gt;\\n&lt;p&gt;This course aims to introduce students to the principles and techniques of data science, enabling them to make effective decisions in their computer science careers. During this course, the student will,&lt;/p&gt;\\n&lt;ul class=\"task-list\"&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Apply data analysis and visualization techniques to derive insights from diverse datasets.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Gain familiarity with machine learning algorithms and their practical applications.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Develop proficiency in using data science tools and programming languages.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Engage in critical thinking and problem-solving through project-based assignments.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Explore the ethical considerations associated with data-driven decision-making.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Stay informed about current trends and developments in data science and artificial intelligence.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/section&gt;\\n&lt;section id=\"expected-learning-outcomes\" class=\"level2\"&gt;\\n&lt;h2 class=\"anchored\" data-anchor-id=\"expected-learning-outcomes\"&gt;Expected Learning Outcomes&lt;/h2&gt;\\n&lt;p&gt;A student who successfully completes this course will be able to:&lt;/p&gt;\\n&lt;ul class=\"task-list\"&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Explain the difference between different measures of centrality and variability (means vs.&nbsp;medians, variance vs.&nbsp;interquartile range, etc.)&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Convert a raw data source into a version appropriate for downstream analysis using Python.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Write appropriate visualizations for different sources and types of data.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Explain why we seek to build machine learning models that generalize rather than memorize their input.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Explain the different uses for training, validation, and testing datasets&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Select the appropriate evaluation measure for the dataset and task being solved&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Articulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Demonstrate awareness of bias and ethics in data science.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n\\n&lt;/section&gt;\\n&lt;/section&gt;\\n\\n&lt;/main&gt; &lt;!-- /main --&gt;\\n&lt;script id=\"quarto-html-after-body\" type=\"application/javascript\"&gt;\\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\\n    const bodyEl = window.document.querySelector(\"body\");\\n    if (mode === \"dark\") {\\n      bodyEl.classList.add(\"quarto-dark\");\\n      bodyEl.classList.remove(\"quarto-light\");\\n    } else {\\n      bodyEl.classList.add(\"quarto-light\");\\n      bodyEl.classList.remove(\"quarto-dark\");\\n    }\\n  }\\n  const toggleBodyColorPrimary = () =&gt; {\\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\\n    if (bsSheetEl) {\\n      toggleBodyColorMode(bsSheetEl);\\n    }\\n  }\\n  toggleBodyColorPrimary();  \\n  const icon = \"\\ue9cb\";\\n  const anchorJS = new window.AnchorJS();\\n  anchorJS.options = {\\n    placement: \\'right\\',\\n    icon: icon\\n  };\\n  anchorJS.add(\\'.anchored\\');\\n  const isCodeAnnotation = (el) =&gt; {\\n    for (const clz of el.classList) {\\n      if (clz.startsWith(\\'code-annotation-\\')) {                     \\n        return true;\\n      }\\n    }\\n    return false;\\n  }\\n  const clipboard = new window.ClipboardJS(\\'.code-copy-button\\', {\\n    text: function(trigger) {\\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\\n      for (const childEl of codeEl.children) {\\n        if (isCodeAnnotation(childEl)) {\\n          childEl.remove();\\n        }\\n      }\\n      return codeEl.innerText;\\n    }\\n  });\\n  clipboard.on(\\'success\\', function(e) {\\n    // button target\\n    const button = e.trigger;\\n    // don\\'t keep focus\\n    button.blur();\\n    // flash \"checked\"\\n    button.classList.add(\\'code-copy-button-checked\\');\\n    var currentTitle = button.getAttribute(\"title\");\\n    button.setAttribute(\"title\", \"Copied!\");\\n    let tooltip;\\n    if (window.bootstrap) {\\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\\n      button.setAttribute(\"data-bs-placement\", \"left\");\\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\\n      tooltip = new bootstrap.Tooltip(button, \\n        { trigger: \"manual\", \\n          customClass: \"code-copy-button-tooltip\",\\n          offset: [0, -8]});\\n      tooltip.show();    \\n    }\\n    setTimeout(function() {\\n      if (tooltip) {\\n        tooltip.hide();\\n        button.removeAttribute(\"data-bs-title\");\\n        button.removeAttribute(\"data-bs-toggle\");\\n        button.removeAttribute(\"data-bs-placement\");\\n      }\\n      button.setAttribute(\"title\", currentTitle);\\n      button.classList.remove(\\'code-copy-button-checked\\');\\n    }, 1000);\\n    // clear code selection\\n    e.clearSelection();\\n  });\\n  function tippyHover(el, contentFn) {\\n    const config = {\\n      allowHTML: true,\\n      content: contentFn,\\n      maxWidth: 500,\\n      delay: 100,\\n      arrow: false,\\n      appendTo: function(el) {\\n          return el.parentElement;\\n      },\\n      interactive: true,\\n      interactiveBorder: 10,\\n      theme: \\'quarto\\',\\n      placement: \\'bottom-start\\'\\n    };\\n    window.tippy(el, config); \\n  }\\n  const noterefs = window.document.querySelectorAll(\\'a[role=\"doc-noteref\"]\\');\\n  for (var i=0; i&lt;noterefs.length; i++) {\\n    const ref = noterefs[i];\\n    tippyHover(ref, function() {\\n      // use id or data attribute instead here\\n      let href = ref.getAttribute(\\'data-footnote-href\\') || ref.getAttribute(\\'href\\');\\n      try { href = new URL(href).hash; } catch {}\\n      const id = href.replace(/^#\\\\/?/, \"\");\\n      const note = window.document.getElementById(id);\\n      return note.innerHTML;\\n    });\\n  }\\n      let selectedAnnoteEl;\\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\\n        let cellAttr = \\'data-code-cell=\"\\' + cell + \\'\"\\';\\n        let lineAttr = \\'data-code-annotation=\"\\' +  annotation + \\'\"\\';\\n        const selector = \\'span[\\' + cellAttr + \\'][\\' + lineAttr + \\']\\';\\n        return selector;\\n      }\\n      const selectCodeLines = (annoteEl) =&gt; {\\n        const doc = window.document;\\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\\n        const lineIds = lines.map((line) =&gt; {\\n          return targetCell + \"-\" + line;\\n        })\\n        let top = null;\\n        let height = null;\\n        let parent = null;\\n        if (lineIds.length &gt; 0) {\\n            //compute the position of the single el (top and bottom and make a div)\\n            const el = window.document.getElementById(lineIds[0]);\\n            top = el.offsetTop;\\n            height = el.offsetHeight;\\n            parent = el.parentElement.parentElement;\\n          if (lineIds.length &gt; 1) {\\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\\n            height = bottom - top;\\n          }\\n          if (top !== null && height !== null && parent !== null) {\\n            // cook up a div (if necessary) and position it \\n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\\n            if (div === null) {\\n              div = window.document.createElement(\"div\");\\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\\n              div.style.position = \\'absolute\\';\\n              parent.appendChild(div);\\n            }\\n            div.style.top = top - 2 + \"px\";\\n            div.style.height = height + 4 + \"px\";\\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\\n            if (gutterDiv === null) {\\n              gutterDiv = window.document.createElement(\"div\");\\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\\n              gutterDiv.style.position = \\'absolute\\';\\n              const codeCell = window.document.getElementById(targetCell);\\n              const gutter = codeCell.querySelector(\\'.code-annotation-gutter\\');\\n              gutter.appendChild(gutterDiv);\\n            }\\n            gutterDiv.style.top = top - 2 + \"px\";\\n            gutterDiv.style.height = height + 4 + \"px\";\\n          }\\n          selectedAnnoteEl = annoteEl;\\n        }\\n      };\\n      const unselectCodeLines = () =&gt; {\\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\\n        elementsIds.forEach((elId) =&gt; {\\n          const div = window.document.getElementById(elId);\\n          if (div) {\\n            div.remove();\\n          }\\n        });\\n        selectedAnnoteEl = undefined;\\n      };\\n      // Attach click handler to the DT\\n      const annoteDls = window.document.querySelectorAll(\\'dt[data-target-cell]\\');\\n      for (const annoteDlNode of annoteDls) {\\n        annoteDlNode.addEventListener(\\'click\\', (event) =&gt; {\\n          const clickedEl = event.target;\\n          if (clickedEl !== selectedAnnoteEl) {\\n            unselectCodeLines();\\n            const activeEl = window.document.querySelector(\\'dt[data-target-cell].code-annotation-active\\');\\n            if (activeEl) {\\n              activeEl.classList.remove(\\'code-annotation-active\\');\\n            }\\n            selectCodeLines(clickedEl);\\n            clickedEl.classList.add(\\'code-annotation-active\\');\\n          } else {\\n            // Unselect the line\\n            unselectCodeLines();\\n            clickedEl.classList.remove(\\'code-annotation-active\\');\\n          }\\n        });\\n      }\\n  const findCites = (el) =&gt; {\\n    const parentEl = el.parentElement;\\n    if (parentEl) {\\n      const cites = parentEl.dataset.cites;\\n      if (cites) {\\n        return {\\n          el,\\n          cites: cites.split(\\' \\')\\n        };\\n      } else {\\n        return findCites(el.parentElement)\\n      }\\n    } else {\\n      return undefined;\\n    }\\n  };\\n  var bibliorefs = window.document.querySelectorAll(\\'a[role=\"doc-biblioref\"]\\');\\n  for (var i=0; i&lt;bibliorefs.length; i++) {\\n    const ref = bibliorefs[i];\\n    const citeInfo = findCites(ref);\\n    if (citeInfo) {\\n      tippyHover(citeInfo.el, function() {\\n        var popup = window.document.createElement(\\'div\\');\\n        citeInfo.cites.forEach(function(cite) {\\n          var citeDiv = window.document.createElement(\\'div\\');\\n          citeDiv.classList.add(\\'hanging-indent\\');\\n          citeDiv.classList.add(\\'csl-entry\\');\\n          var biblioDiv = window.document.getElementById(\\'ref-\\' + cite);\\n          if (biblioDiv) {\\n            citeDiv.innerHTML = biblioDiv.innerHTML;\\n          }\\n          popup.appendChild(citeDiv);\\n        });\\n        return popup.innerHTML;\\n      });\\n    }\\n  }\\n});\\n&lt;/script&gt;\\n&lt;/div&gt; &lt;!-- /content --&gt;\\n\\n\\n\\n&lt;/body&gt;&lt;/html&gt;'\n\n\n\n\n\n\nfrom bs4 import BeautifulSoup\n\n\nsoup = BeautifulSoup(r.text, 'html.parser')\n\n\nprint(soup.prettify()) #Output cleared for web\n\n\nprint(soup.get_text()) #Output cleared for web\n\n\n# creating a list of all common heading tags\nheading_tags = [\"h1\", \"h2\", \"h3\"]\nfor tags in soup.find_all(heading_tags):\n    print(tags.text.strip())\n\nOn this page\nSyllabus (Key Info)\nDescription\nCourse Objective\nExpected Learning Outcomes"
  },
  {
    "objectID": "Course_Content/Week_3/3/Notebook.html#data-collection",
    "href": "Course_Content/Week_3/3/Notebook.html#data-collection",
    "title": "Lecture 3.3",
    "section": "",
    "text": "!pip install beautifulsoup4\n\nRequirement already satisfied: beautifulsoup4 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (4.12.2)\nRequirement already satisfied: soupsieve&gt;1.2 in /Users/enfageorge/miniconda/lib/python3.10/site-packages (from beautifulsoup4) (2.4.1)\n\n\n\n\n\nimport requests\n\n\nURL = \"https://csc380.beingenfa.com/Syllabus/Key_Info.html\"\n\n\nr = requests.get(URL)\n\n\nr.status_code\n\n200\n\n\n\nr.text\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\\n\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"generator\" content=\"quarto-1.3.361\"&gt;\\n\\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\\n\\n\\n&lt;title&gt;CSC 380 – key_info&lt;/title&gt;\\n&lt;style&gt;\\ncode{white-space: pre-wrap;}\\nspan.smallcaps{font-variant: small-caps;}\\ndiv.columns{display: flex; gap: min(4vw, 1.5em);}\\ndiv.column{flex: auto; overflow-x: auto;}\\ndiv.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}\\nul.task-list{list-style: none;}\\nul.task-list li input[type=\"checkbox\"] {\\n  width: 0.8em;\\n  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ \\n  vertical-align: middle;\\n}\\n&lt;/style&gt;\\n\\n\\n&lt;script src=\"../site_libs/quarto-nav/quarto-nav.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-nav/headroom.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/clipboard/clipboard.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/autocomplete.umd.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/fuse.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/quarto-search.js\"&gt;&lt;/script&gt;\\n&lt;meta name=\"quarto:offset\" content=\"../\"&gt;\\n&lt;script src=\"../site_libs/quarto-html/quarto.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/popper.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/tippy.umd.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/anchor.min.js\"&gt;&lt;/script&gt;\\n&lt;link href=\"../site_libs/quarto-html/tippy.css\" rel=\"stylesheet\"&gt;\\n&lt;link href=\"../site_libs/quarto-html/quarto-syntax-highlighting.css\" rel=\"stylesheet\" id=\"quarto-text-highlighting-styles\"&gt;\\n&lt;script src=\"../site_libs/bootstrap/bootstrap.min.js\"&gt;&lt;/script&gt;\\n&lt;link href=\"../site_libs/bootstrap/bootstrap-icons.css\" rel=\"stylesheet\"&gt;\\n&lt;link href=\"../site_libs/bootstrap/bootstrap.min.css\" rel=\"stylesheet\" id=\"quarto-bootstrap\" data-mode=\"light\"&gt;\\n&lt;script id=\"quarto-search-options\" type=\"application/json\"&gt;{\\n  \"location\": \"sidebar\",\\n  \"copy-button\": false,\\n  \"collapse-after\": 3,\\n  \"panel-placement\": \"start\",\\n  \"type\": \"textbox\",\\n  \"limit\": 20,\\n  \"language\": {\\n    \"search-no-results-text\": \"No results\",\\n    \"search-matching-documents-text\": \"matching documents\",\\n    \"search-copy-link-title\": \"Copy link to search\",\\n    \"search-hide-matches-text\": \"Hide additional matches\",\\n    \"search-more-match-text\": \"more match in this document\",\\n    \"search-more-matches-text\": \"more matches in this document\",\\n    \"search-clear-button-title\": \"Clear\",\\n    \"search-detached-cancel-button-title\": \"Cancel\",\\n    \"search-submit-button-title\": \"Submit\",\\n    \"search-label\": \"Search\"\\n  }\\n}&lt;/script&gt;\\n\\n\\n&lt;link rel=\"stylesheet\" href=\"../styles.css\"&gt;\\n&lt;/head&gt;\\n\\n&lt;body class=\"nav-sidebar docked\"&gt;\\n\\n&lt;div id=\"quarto-search-results\"&gt;&lt;/div&gt;\\n  &lt;header id=\"quarto-header\" class=\"headroom fixed-top\"&gt;\\n  &lt;nav class=\"quarto-secondary-nav\"&gt;\\n    &lt;div class=\"container-fluid d-flex\"&gt;\\n      &lt;button type=\"button\" class=\"quarto-btn-toggle btn\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;\\n        &lt;i class=\"bi bi-layout-text-sidebar-reverse\"&gt;&lt;/i&gt;\\n      &lt;/button&gt;\\n      &lt;nav class=\"quarto-page-breadcrumbs\" aria-label=\"breadcrumb\"&gt;&lt;ol class=\"breadcrumb\"&gt;&lt;li class=\"breadcrumb-item\"&gt;&lt;a href=\"../Syllabus/Key_Info.html\"&gt;Syllabus&lt;/a&gt;&lt;/li&gt;&lt;li class=\"breadcrumb-item\"&gt;&lt;a href=\"../Syllabus/Key_Info.html\"&gt;Key Info&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/nav&gt;\\n      &lt;a class=\"flex-grow-1\" role=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;      \\n      &lt;/a&gt;\\n      &lt;button type=\"button\" class=\"btn quarto-search-button\" aria-label=\"\" onclick=\"window.quartoOpenSearch();\"&gt;\\n        &lt;i class=\"bi bi-search\"&gt;&lt;/i&gt;\\n      &lt;/button&gt;\\n    &lt;/div&gt;\\n  &lt;/nav&gt;\\n&lt;/header&gt;\\n&lt;!-- content --&gt;\\n&lt;div id=\"quarto-content\" class=\"quarto-container page-columns page-rows-contents page-layout-article\"&gt;\\n&lt;!-- sidebar --&gt;\\n  &lt;nav id=\"quarto-sidebar\" class=\"sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto\"&gt;\\n    &lt;div class=\"pt-lg-2 mt-2 text-left sidebar-header\"&gt;\\n    &lt;div class=\"sidebar-title mb-0 py-0\"&gt;\\n      &lt;a href=\"../\"&gt;CSC 380&lt;/a&gt; \\n    &lt;/div&gt;\\n      &lt;/div&gt;\\n        &lt;div class=\"mt-2 flex-shrink-0 align-items-center\"&gt;\\n        &lt;div class=\"sidebar-search\"&gt;\\n        &lt;div id=\"quarto-search\" class=\"\" title=\"Search\"&gt;&lt;/div&gt;\\n        &lt;/div&gt;\\n        &lt;/div&gt;\\n    &lt;div class=\"sidebar-menu-container\"&gt; \\n    &lt;ul class=\"list-unstyled mt-1\"&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-1\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Course Content&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-1\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-1\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_1/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 1&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_2/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 2&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_3/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 3&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_4/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 4&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-2\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Homework&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-2\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-2\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Homework/HW1.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;HW1: Probability&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-3\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Ethics Discussions&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-3\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-3\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_2.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W2: Political Content&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_3.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W3: Creative Work&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_4.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W4: Mental Health Support&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-4\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Syllabus&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-4\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-4\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Syllabus/Key_Info.html\" class=\"sidebar-item-text sidebar-link active\"&gt;\\n &lt;span class=\"menu-text\"&gt;Key Info&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Syllabus/Syllabus.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Official Syllabus&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n    &lt;/ul&gt;\\n    &lt;/div&gt;\\n&lt;/nav&gt;\\n&lt;div id=\"quarto-sidebar-glass\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\"&gt;&lt;/div&gt;\\n&lt;!-- margin-sidebar --&gt;\\n    &lt;div id=\"quarto-margin-sidebar\" class=\"sidebar margin-sidebar\"&gt;\\n        &lt;nav id=\"TOC\" role=\"doc-toc\" class=\"toc-active\"&gt;\\n    &lt;h2 id=\"toc-title\"&gt;On this page&lt;/h2&gt;\\n   \\n  &lt;ul&gt;\\n  &lt;li&gt;&lt;a href=\"#syllabus-key-info\" id=\"toc-syllabus-key-info\" class=\"nav-link active\" data-scroll-target=\"#syllabus-key-info\"&gt;Syllabus (Key Info)&lt;/a&gt;\\n  &lt;ul class=\"collapse\"&gt;\\n  &lt;li&gt;&lt;a href=\"#description\" id=\"toc-description\" class=\"nav-link\" data-scroll-target=\"#description\"&gt;Description&lt;/a&gt;&lt;/li&gt;\\n  &lt;li&gt;&lt;a href=\"#course-objective\" id=\"toc-course-objective\" class=\"nav-link\" data-scroll-target=\"#course-objective\"&gt;Course Objective&lt;/a&gt;&lt;/li&gt;\\n  &lt;li&gt;&lt;a href=\"#expected-learning-outcomes\" id=\"toc-expected-learning-outcomes\" class=\"nav-link\" data-scroll-target=\"#expected-learning-outcomes\"&gt;Expected Learning Outcomes&lt;/a&gt;&lt;/li&gt;\\n  &lt;/ul&gt;&lt;/li&gt;\\n  &lt;/ul&gt;\\n&lt;/nav&gt;\\n    &lt;/div&gt;\\n&lt;!-- main --&gt;\\n&lt;main class=\"content\" id=\"quarto-document-content\"&gt;\\n\\n\\n\\n&lt;section id=\"syllabus-key-info\" class=\"level1\"&gt;\\n&lt;h1&gt;Syllabus (Key Info)&lt;/h1&gt;\\n&lt;section id=\"description\" class=\"level2\"&gt;\\n&lt;h2 class=\"anchored\" data-anchor-id=\"description\"&gt;Description&lt;/h2&gt;\\n&lt;p&gt;The course introduces students to the principles of data science, which are essential for computer scientists to make effective decisions in their professional careers. In today’s data-driven world, a wide range of computer science sub-disciplines heavily rely on data collection, analysis, and interpretation. With the pervasive presence of artificial intelligence (AI) in our lives, understanding the basics of how these systems work is becoming increasingly important. Additionally, it covers the basics of artificial intelligence (AI) systems and examines practical use cases, current news, and ethical considerations through readings and discussions.&lt;/p&gt;\\n&lt;/section&gt;\\n&lt;section id=\"course-objective\" class=\"level2\"&gt;\\n&lt;h2 class=\"anchored\" data-anchor-id=\"course-objective\"&gt;Course Objective&lt;/h2&gt;\\n&lt;p&gt;Course Objectives&lt;/p&gt;\\n&lt;p&gt;This course aims to introduce students to the principles and techniques of data science, enabling them to make effective decisions in their computer science careers. During this course, the student will,&lt;/p&gt;\\n&lt;ul class=\"task-list\"&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Apply data analysis and visualization techniques to derive insights from diverse datasets.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Gain familiarity with machine learning algorithms and their practical applications.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Develop proficiency in using data science tools and programming languages.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Engage in critical thinking and problem-solving through project-based assignments.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Explore the ethical considerations associated with data-driven decision-making.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Stay informed about current trends and developments in data science and artificial intelligence.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/section&gt;\\n&lt;section id=\"expected-learning-outcomes\" class=\"level2\"&gt;\\n&lt;h2 class=\"anchored\" data-anchor-id=\"expected-learning-outcomes\"&gt;Expected Learning Outcomes&lt;/h2&gt;\\n&lt;p&gt;A student who successfully completes this course will be able to:&lt;/p&gt;\\n&lt;ul class=\"task-list\"&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Explain the difference between different measures of centrality and variability (means vs.&nbsp;medians, variance vs.&nbsp;interquartile range, etc.)&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Convert a raw data source into a version appropriate for downstream analysis using Python.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Write appropriate visualizations for different sources and types of data.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Explain why we seek to build machine learning models that generalize rather than memorize their input.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Explain the different uses for training, validation, and testing datasets&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Select the appropriate evaluation measure for the dataset and task being solved&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Articulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;input type=\"checkbox\"&gt;Demonstrate awareness of bias and ethics in data science.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n\\n&lt;/section&gt;\\n&lt;/section&gt;\\n\\n&lt;/main&gt; &lt;!-- /main --&gt;\\n&lt;script id=\"quarto-html-after-body\" type=\"application/javascript\"&gt;\\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\\n    const bodyEl = window.document.querySelector(\"body\");\\n    if (mode === \"dark\") {\\n      bodyEl.classList.add(\"quarto-dark\");\\n      bodyEl.classList.remove(\"quarto-light\");\\n    } else {\\n      bodyEl.classList.add(\"quarto-light\");\\n      bodyEl.classList.remove(\"quarto-dark\");\\n    }\\n  }\\n  const toggleBodyColorPrimary = () =&gt; {\\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\\n    if (bsSheetEl) {\\n      toggleBodyColorMode(bsSheetEl);\\n    }\\n  }\\n  toggleBodyColorPrimary();  \\n  const icon = \"\\ue9cb\";\\n  const anchorJS = new window.AnchorJS();\\n  anchorJS.options = {\\n    placement: \\'right\\',\\n    icon: icon\\n  };\\n  anchorJS.add(\\'.anchored\\');\\n  const isCodeAnnotation = (el) =&gt; {\\n    for (const clz of el.classList) {\\n      if (clz.startsWith(\\'code-annotation-\\')) {                     \\n        return true;\\n      }\\n    }\\n    return false;\\n  }\\n  const clipboard = new window.ClipboardJS(\\'.code-copy-button\\', {\\n    text: function(trigger) {\\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\\n      for (const childEl of codeEl.children) {\\n        if (isCodeAnnotation(childEl)) {\\n          childEl.remove();\\n        }\\n      }\\n      return codeEl.innerText;\\n    }\\n  });\\n  clipboard.on(\\'success\\', function(e) {\\n    // button target\\n    const button = e.trigger;\\n    // don\\'t keep focus\\n    button.blur();\\n    // flash \"checked\"\\n    button.classList.add(\\'code-copy-button-checked\\');\\n    var currentTitle = button.getAttribute(\"title\");\\n    button.setAttribute(\"title\", \"Copied!\");\\n    let tooltip;\\n    if (window.bootstrap) {\\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\\n      button.setAttribute(\"data-bs-placement\", \"left\");\\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\\n      tooltip = new bootstrap.Tooltip(button, \\n        { trigger: \"manual\", \\n          customClass: \"code-copy-button-tooltip\",\\n          offset: [0, -8]});\\n      tooltip.show();    \\n    }\\n    setTimeout(function() {\\n      if (tooltip) {\\n        tooltip.hide();\\n        button.removeAttribute(\"data-bs-title\");\\n        button.removeAttribute(\"data-bs-toggle\");\\n        button.removeAttribute(\"data-bs-placement\");\\n      }\\n      button.setAttribute(\"title\", currentTitle);\\n      button.classList.remove(\\'code-copy-button-checked\\');\\n    }, 1000);\\n    // clear code selection\\n    e.clearSelection();\\n  });\\n  function tippyHover(el, contentFn) {\\n    const config = {\\n      allowHTML: true,\\n      content: contentFn,\\n      maxWidth: 500,\\n      delay: 100,\\n      arrow: false,\\n      appendTo: function(el) {\\n          return el.parentElement;\\n      },\\n      interactive: true,\\n      interactiveBorder: 10,\\n      theme: \\'quarto\\',\\n      placement: \\'bottom-start\\'\\n    };\\n    window.tippy(el, config); \\n  }\\n  const noterefs = window.document.querySelectorAll(\\'a[role=\"doc-noteref\"]\\');\\n  for (var i=0; i&lt;noterefs.length; i++) {\\n    const ref = noterefs[i];\\n    tippyHover(ref, function() {\\n      // use id or data attribute instead here\\n      let href = ref.getAttribute(\\'data-footnote-href\\') || ref.getAttribute(\\'href\\');\\n      try { href = new URL(href).hash; } catch {}\\n      const id = href.replace(/^#\\\\/?/, \"\");\\n      const note = window.document.getElementById(id);\\n      return note.innerHTML;\\n    });\\n  }\\n      let selectedAnnoteEl;\\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\\n        let cellAttr = \\'data-code-cell=\"\\' + cell + \\'\"\\';\\n        let lineAttr = \\'data-code-annotation=\"\\' +  annotation + \\'\"\\';\\n        const selector = \\'span[\\' + cellAttr + \\'][\\' + lineAttr + \\']\\';\\n        return selector;\\n      }\\n      const selectCodeLines = (annoteEl) =&gt; {\\n        const doc = window.document;\\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\\n        const lineIds = lines.map((line) =&gt; {\\n          return targetCell + \"-\" + line;\\n        })\\n        let top = null;\\n        let height = null;\\n        let parent = null;\\n        if (lineIds.length &gt; 0) {\\n            //compute the position of the single el (top and bottom and make a div)\\n            const el = window.document.getElementById(lineIds[0]);\\n            top = el.offsetTop;\\n            height = el.offsetHeight;\\n            parent = el.parentElement.parentElement;\\n          if (lineIds.length &gt; 1) {\\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\\n            height = bottom - top;\\n          }\\n          if (top !== null && height !== null && parent !== null) {\\n            // cook up a div (if necessary) and position it \\n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\\n            if (div === null) {\\n              div = window.document.createElement(\"div\");\\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\\n              div.style.position = \\'absolute\\';\\n              parent.appendChild(div);\\n            }\\n            div.style.top = top - 2 + \"px\";\\n            div.style.height = height + 4 + \"px\";\\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\\n            if (gutterDiv === null) {\\n              gutterDiv = window.document.createElement(\"div\");\\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\\n              gutterDiv.style.position = \\'absolute\\';\\n              const codeCell = window.document.getElementById(targetCell);\\n              const gutter = codeCell.querySelector(\\'.code-annotation-gutter\\');\\n              gutter.appendChild(gutterDiv);\\n            }\\n            gutterDiv.style.top = top - 2 + \"px\";\\n            gutterDiv.style.height = height + 4 + \"px\";\\n          }\\n          selectedAnnoteEl = annoteEl;\\n        }\\n      };\\n      const unselectCodeLines = () =&gt; {\\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\\n        elementsIds.forEach((elId) =&gt; {\\n          const div = window.document.getElementById(elId);\\n          if (div) {\\n            div.remove();\\n          }\\n        });\\n        selectedAnnoteEl = undefined;\\n      };\\n      // Attach click handler to the DT\\n      const annoteDls = window.document.querySelectorAll(\\'dt[data-target-cell]\\');\\n      for (const annoteDlNode of annoteDls) {\\n        annoteDlNode.addEventListener(\\'click\\', (event) =&gt; {\\n          const clickedEl = event.target;\\n          if (clickedEl !== selectedAnnoteEl) {\\n            unselectCodeLines();\\n            const activeEl = window.document.querySelector(\\'dt[data-target-cell].code-annotation-active\\');\\n            if (activeEl) {\\n              activeEl.classList.remove(\\'code-annotation-active\\');\\n            }\\n            selectCodeLines(clickedEl);\\n            clickedEl.classList.add(\\'code-annotation-active\\');\\n          } else {\\n            // Unselect the line\\n            unselectCodeLines();\\n            clickedEl.classList.remove(\\'code-annotation-active\\');\\n          }\\n        });\\n      }\\n  const findCites = (el) =&gt; {\\n    const parentEl = el.parentElement;\\n    if (parentEl) {\\n      const cites = parentEl.dataset.cites;\\n      if (cites) {\\n        return {\\n          el,\\n          cites: cites.split(\\' \\')\\n        };\\n      } else {\\n        return findCites(el.parentElement)\\n      }\\n    } else {\\n      return undefined;\\n    }\\n  };\\n  var bibliorefs = window.document.querySelectorAll(\\'a[role=\"doc-biblioref\"]\\');\\n  for (var i=0; i&lt;bibliorefs.length; i++) {\\n    const ref = bibliorefs[i];\\n    const citeInfo = findCites(ref);\\n    if (citeInfo) {\\n      tippyHover(citeInfo.el, function() {\\n        var popup = window.document.createElement(\\'div\\');\\n        citeInfo.cites.forEach(function(cite) {\\n          var citeDiv = window.document.createElement(\\'div\\');\\n          citeDiv.classList.add(\\'hanging-indent\\');\\n          citeDiv.classList.add(\\'csl-entry\\');\\n          var biblioDiv = window.document.getElementById(\\'ref-\\' + cite);\\n          if (biblioDiv) {\\n            citeDiv.innerHTML = biblioDiv.innerHTML;\\n          }\\n          popup.appendChild(citeDiv);\\n        });\\n        return popup.innerHTML;\\n      });\\n    }\\n  }\\n});\\n&lt;/script&gt;\\n&lt;/div&gt; &lt;!-- /content --&gt;\\n\\n\\n\\n&lt;/body&gt;&lt;/html&gt;'\n\n\n\n\n\n\nfrom bs4 import BeautifulSoup\n\n\nsoup = BeautifulSoup(r.text, 'html.parser')\n\n\nprint(soup.prettify()) #Output cleared for web\n\n\nprint(soup.get_text()) #Output cleared for web\n\n\n# creating a list of all common heading tags\nheading_tags = [\"h1\", \"h2\", \"h3\"]\nfor tags in soup.find_all(heading_tags):\n    print(tags.text.strip())\n\nOn this page\nSyllabus (Key Info)\nDescription\nCourse Objective\nExpected Learning Outcomes"
  },
  {
    "objectID": "Course_Content/Week_3/3/Notebook.html#data-processing",
    "href": "Course_Content/Week_3/3/Notebook.html#data-processing",
    "title": "Lecture 3.3",
    "section": "2. Data Processing",
    "text": "2. Data Processing\n\n2.1 Numpy\nSupport for large, multi-dimensional arrays and matrices, and a large collection of high-level mathematical functions to operate on these arrays.\n\nimport numpy as np\n\nndarray object: an n-dimensional array of homogeneous data types, with many operations being performed in compiled code for performance\n\nFixed Size\nSame type of data\nMuch more effiecent mathematical operations than built in data types like lists.\n\nnumpy.dtype - intc (same as a C integer) and intp (used for indexing) - int8, int16, int32, int64 - uint8, uint16, uint32, uint64 - float16, float32, float64 - complex64, complex128\nCreate a numpy array\n\nConversion from other Python structures (e.g., lists, tuples)\nBuilt-in NumPy array creation (e.g., arange, ones, zeros, etc.)\nReading arrays from a file.\n\n\nnp.array([2,3,1,0])\n\narray([2, 3, 1, 0])\n\n\n\nnp.zeros((5, 5)) #np.zeros(shape)\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])\n\n\n\nnp.ones((6, 2))#np.ones(shape)\n\narray([[1., 1.],\n       [1., 1.],\n       [1., 1.],\n       [1., 1.],\n       [1., 1.],\n       [1., 1.]])\n\n\n\nnp.arange(15,5,-1) #Like range function in python\n\narray([15, 14, 13, 12, 11, 10,  9,  8,  7,  6])\n\n\n\n#Return evenly spaced numbers over a specified interval.\nnp.linspace(0, 100, 5) # numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)\n\narray([  0.,  25.,  50.,  75., 100.])\n\n\n\nnp.random.random() #Keeps changing\n\n0.7026417115321625\n\n\n\nrandom_obj = np.random.default_rng(seed=None) #default_rng is the recommended constructor for the random number class\nrandom_obj.random() #changes if you do not give a seed\n\n0.21864992820211548\n\n\n\nrandom_obj = np.random.default_rng(seed=42) #default_rng is the recommended constructor for the random number class\nrandom_obj.random()\n\n0.7739560485559633\n\n\n\nprint(\"Original:\\n\",np.arange(9))\nprint(\"After using reshape:\\n\",np.arange(9).reshape(3,3))\n\nOriginal:\n [0 1 2 3 4 5 6 7 8]\nAfter using reshape:\n [[0 1 2]\n [3 4 5]\n [6 7 8]]\n\n\n\nx = np.arange(2,10)\nprint(x)\nx[-1]\n\n[2 3 4 5 6 7 8 9]\n\n\n9\n\n\n\nx.shape = (1,3)\nprint(x)\nx[-1] # next slide\n\nValueError: cannot reshape array of size 8 into shape (1,3)\n\n\n\nx.shape = (2,4)\nprint(\"Array:\\n\",x,\"\\n\")\nprint(\"x[-1]: \",x[0])\nprint(\"x[1,3]: \", x[0,3])\n\nArray:\n [[2 3 4 5]\n [6 7 8 9]] \n\nx[-1]:  [2 3 4 5]\nx[1,3]:  5\n\n\n\na = np.arange(1,11)\nb = np.arange(12,22)\na+b\n\narray([13, 15, 17, 19, 21, 23, 25, 27, 29, 31])\n\n\n\na = np.arange(1,11).reshape(2,5)\nb = np.arange(12,22).reshape(5,2)\nresult = np.dot(a,b) # To multiply two arrays\nresult\n\narray([[260, 275],\n       [660, 700]])\n\n\n\nresult.transpose()\n\narray([[260, 660],\n       [275, 700]])\n\n\n\nnp.linalg.inv(result) # and finally\n\narray([[ 1.4 , -0.55],\n       [-1.32,  0.52]])\n\n\n\n\n2.2 Scipy\n\nbuilt on the NumPy\nvarious tools and functions for solving common problems in scientific computing.\n\nex: - Fourier Transforms (scipy.fftpack) - Multidimensional image processing (scipy.ndimage) - Spatial data structures and algorithms (scipy.spatial) ..\n\n\n2.3 Continue our discussion on Pandas\n\nimport pandas as pd\n\n\nWORLD_DATA_PATH = \"spotify-top-50/data/spotify-streaming-top-50-usa.csv\"\n\n\nworld_df = pd.read_csv(WORLD_DATA_PATH)\n\n\nworld_df.sample()\n\n\n\n\n\n\n\n\ndate\nposition\nsong\nartist\npopularity\nduration_ms\nalbum_type\ntotal_tracks\nrelease_date\nis_explicit\nalbum_cover_url\n\n\n\n\n269\n2023-05-23\n20\nWasted On You\nMorgan Wallen\n86\n178520\nalbum\n30\n2021-01-08\nFalse\nhttps://i.scdn.co/image/ab67616d0000b2737d6813...\n\n\n\n\n\n\n\nQ: The time range of the dataset?\n\nworld_df['date'].dtype\n\ndtype('O')\n\n\n\ntype(world_df['date'][0])\n\nstr\n\n\n\n# Convert column date to date datatype\n\nworld_df['date'] = pd.to_datetime(world_df['date'])\n\n\n# Q : What is the time range in which this dataset is recording top 50?\n# Assume that it records everyday\n\nworld_df['date'].max(), world_df['date'].min()\n\n(Timestamp('2023-06-27 00:00:00'), Timestamp('2023-05-18 00:00:00'))"
  },
  {
    "objectID": "Course_Content/Week_3/2/home.html",
    "href": "Course_Content/Week_3/2/home.html",
    "title": "3.2 - Data Collection (Part 1 of 2)",
    "section": "",
    "text": "Research Design for Statistical Analysis\n\nCausation versus Correlation\nSampling\n\nRevisit the Data Science Process\nData Collection\n\nTill SQL\n\n\n\n\n\n\nSlides\n\n\n\n\n\n\n\n\n\n3.3 - Data Collection (Part 2 of 2) & Data Processing (Part 1)"
  },
  {
    "objectID": "Course_Content/Week_3/2/home.html#objectives",
    "href": "Course_Content/Week_3/2/home.html#objectives",
    "title": "3.2 - Data Collection (Part 1 of 2)",
    "section": "",
    "text": "Research Design for Statistical Analysis\n\nCausation versus Correlation\nSampling\n\nRevisit the Data Science Process\nData Collection\n\nTill SQL"
  },
  {
    "objectID": "Course_Content/Week_3/2/home.html#materials",
    "href": "Course_Content/Week_3/2/home.html#materials",
    "title": "3.2 - Data Collection (Part 1 of 2)",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "Course_Content/Week_3/2/home.html#next-class",
    "href": "Course_Content/Week_3/2/home.html#next-class",
    "title": "3.2 - Data Collection (Part 1 of 2)",
    "section": "",
    "text": "3.3 - Data Collection (Part 2 of 2) & Data Processing (Part 1)"
  },
  {
    "objectID": "Course_Content/Week_6/home.html",
    "href": "Course_Content/Week_6/home.html",
    "title": "Week 6 : Data Visualization, Introduction to ML, Supervised Learning",
    "section": "",
    "text": "Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nApply data analysis and visualization techniques to derive insights from diverse datasets.\nDevelop proficiency in using data science tools and programming languages.\nExplore the ethical considerations associated with data-driven decision-making.\n\n\n\n\n\nWrite appropriate visualizations for different sources and types of data.\nExplain why we seek to build machine learning models that generalize rather than memorize their input\nExplain the different uses for training, validation, and testing datasets\nArticulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem\nSelect the appropriate evaluation measure for the dataset and task being solved\nDemonstrate awareness of bias and ethics in data science.\n\n\n\n\n\n\n6.1 - Data Vizualisation and Introduction to Machine Learning\n6.2 - Machine Learning - Key Concepts & Supervised ML : Linear Regression\n6.3 - Hands on Demo\n\n\n\n\nBonus : Dataset Suggestions ( 1 point)\nBonus : Viz (2 points)\nParticipation Activity : Github Setup\n\n\n\nHomework 3\n\n\n\nWeekly Checkin for Week 6 | Due 5 pm , July 23, Sunday ( Based on Lecture 6.1,6.2,6.3 )\n\n\n\n\n\n\n\n\n\nStrikethrough text is changes in plan.\nGreen is new items added after planning.\nChecked boxes are completed items.\n\n\n\n\n\n\n\n6.1 - Data Vizualisation and Introduction to Machine Learning\n6.2 - Machine Learning - Key Concepts & Supervised ML : Linear Regression\n6.3 - Hands on Demo\n\n\n\n\n\nHomework 3 released [delayed by 2 days]\nBonus Homeworks ( 3 total points )\nParticipation Activity\n\n\n\n\n\nCheck-in for week 6 opens ( Based on Lecture 6.1,6.2,6.3 )\n\n\n\n\n\nWeek 5 assigned reading discussion question to be released\n\n\n\n\n\nEthical Question start\nDataset Selection"
  },
  {
    "objectID": "Course_Content/Week_6/home.html#objectives",
    "href": "Course_Content/Week_6/home.html#objectives",
    "title": "Week 6 : Data Visualization, Introduction to ML, Supervised Learning",
    "section": "",
    "text": "Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nApply data analysis and visualization techniques to derive insights from diverse datasets.\nDevelop proficiency in using data science tools and programming languages.\nExplore the ethical considerations associated with data-driven decision-making.\n\n\n\n\n\nWrite appropriate visualizations for different sources and types of data.\nExplain why we seek to build machine learning models that generalize rather than memorize their input\nExplain the different uses for training, validation, and testing datasets\nArticulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem\nSelect the appropriate evaluation measure for the dataset and task being solved\nDemonstrate awareness of bias and ethics in data science."
  },
  {
    "objectID": "Course_Content/Week_6/home.html#lectures",
    "href": "Course_Content/Week_6/home.html#lectures",
    "title": "Week 6 : Data Visualization, Introduction to ML, Supervised Learning",
    "section": "",
    "text": "6.1 - Data Vizualisation and Introduction to Machine Learning\n6.2 - Machine Learning - Key Concepts & Supervised ML : Linear Regression\n6.3 - Hands on Demo"
  },
  {
    "objectID": "Course_Content/Week_6/home.html#activities",
    "href": "Course_Content/Week_6/home.html#activities",
    "title": "Week 6 : Data Visualization, Introduction to ML, Supervised Learning",
    "section": "",
    "text": "Bonus : Dataset Suggestions ( 1 point)\nBonus : Viz (2 points)\nParticipation Activity : Github Setup\n\n\n\nHomework 3\n\n\n\nWeekly Checkin for Week 6 | Due 5 pm , July 23, Sunday ( Based on Lecture 6.1,6.2,6.3 )"
  },
  {
    "objectID": "Course_Content/Week_6/home.html#summary-plan-versus-achievements",
    "href": "Course_Content/Week_6/home.html#summary-plan-versus-achievements",
    "title": "Week 6 : Data Visualization, Introduction to ML, Supervised Learning",
    "section": "",
    "text": "Strikethrough text is changes in plan.\nGreen is new items added after planning.\nChecked boxes are completed items.\n\n\n\n\n\n\n\n6.1 - Data Vizualisation and Introduction to Machine Learning\n6.2 - Machine Learning - Key Concepts & Supervised ML : Linear Regression\n6.3 - Hands on Demo\n\n\n\n\n\nHomework 3 released [delayed by 2 days]\nBonus Homeworks ( 3 total points )\nParticipation Activity\n\n\n\n\n\nCheck-in for week 6 opens ( Based on Lecture 6.1,6.2,6.3 )\n\n\n\n\n\nWeek 5 assigned reading discussion question to be released\n\n\n\n\n\nEthical Question start\nDataset Selection"
  },
  {
    "objectID": "Course_Content/Week_6/1/home.html",
    "href": "Course_Content/Week_6/1/home.html",
    "title": "6.1 - Data Vizualisation and Introduction to Machine Learning",
    "section": "",
    "text": "Data Viz:\n\nWhat is?\nWhy?\nDifferent types - [ Bonus Activity worth 2 points here ]\nBad Viz\nPython Libraries\n\nMatplotlib\n\nAnatomy\nSingle plot on a figure\nMultiple charts on a figure\n\n\n\nIntroduction to ML:\n\nTypes:\n\nSupervised\nUnsupervised\nSemi-supervised\nReinforcement learning\n\n\n\n\n\n\n\n\nSlides\nJupyter Notebook\n\n\n\n\n\n\n\n\n\n6.2 Machine Learning - Key Concepts & Supervised ML : Linear Regression"
  },
  {
    "objectID": "Course_Content/Week_6/1/home.html#objectives",
    "href": "Course_Content/Week_6/1/home.html#objectives",
    "title": "6.1 - Data Vizualisation and Introduction to Machine Learning",
    "section": "",
    "text": "Data Viz:\n\nWhat is?\nWhy?\nDifferent types - [ Bonus Activity worth 2 points here ]\nBad Viz\nPython Libraries\n\nMatplotlib\n\nAnatomy\nSingle plot on a figure\nMultiple charts on a figure\n\n\n\nIntroduction to ML:\n\nTypes:\n\nSupervised\nUnsupervised\nSemi-supervised\nReinforcement learning"
  },
  {
    "objectID": "Course_Content/Week_6/1/home.html#materials",
    "href": "Course_Content/Week_6/1/home.html#materials",
    "title": "6.1 - Data Vizualisation and Introduction to Machine Learning",
    "section": "",
    "text": "Slides\nJupyter Notebook"
  },
  {
    "objectID": "Course_Content/Week_6/1/home.html#next-class",
    "href": "Course_Content/Week_6/1/home.html#next-class",
    "title": "6.1 - Data Vizualisation and Introduction to Machine Learning",
    "section": "",
    "text": "6.2 Machine Learning - Key Concepts & Supervised ML : Linear Regression"
  },
  {
    "objectID": "Course_Content/Week_6/1/Notebook.html",
    "href": "Course_Content/Week_6/1/Notebook.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Graphical display of data\nUseful for\n\ndata cleaning\nexploring data structure\ndetecting outliers and unusual groups\nidentifying trends and clusters\nspotting local patterns\nevaluating modeling output\nPresenting results.\n\nUsed to convey a message across, or to understand data better. \nA graphic won’t say the whole story. \nPresentation graphics are used to convey known information and are often designed to attract attention.\nExploratory graphics are used to find new information and should direct attention to information.\nSource - Why Is Data Visualization Important? What Is Important in Data Visualization?"
  },
  {
    "objectID": "Course_Content/Week_6/1/Notebook.html#types",
    "href": "Course_Content/Week_6/1/Notebook.html#types",
    "title": "Data Visualization",
    "section": "1. Types",
    "text": "1. Types\nsome below"
  },
  {
    "objectID": "Course_Content/Week_6/1/Notebook.html#chart",
    "href": "Course_Content/Week_6/1/Notebook.html#chart",
    "title": "Data Visualization",
    "section": "Chart:",
    "text": "Chart:\n\nSource - Tableau.com"
  },
  {
    "objectID": "Course_Content/Week_6/1/Notebook.html#geospatial",
    "href": "Course_Content/Week_6/1/Notebook.html#geospatial",
    "title": "Data Visualization",
    "section": "GeoSpatial",
    "text": "GeoSpatial\n\nSource"
  },
  {
    "objectID": "Course_Content/Week_6/1/Notebook.html#heatmaps",
    "href": "Course_Content/Week_6/1/Notebook.html#heatmaps",
    "title": "Data Visualization",
    "section": "Heatmaps",
    "text": "Heatmaps\n Bonus Point activity worth 2 points.- https://www.tableau.com/learn/articles/data-visualization#big-data"
  },
  {
    "objectID": "Course_Content/Week_6/1/Notebook.html#good-and-bad-vizualisation",
    "href": "Course_Content/Week_6/1/Notebook.html#good-and-bad-vizualisation",
    "title": "Data Visualization",
    "section": "2. Good and Bad Vizualisation",
    "text": "2. Good and Bad Vizualisation\nExamples from : https://www.codeconquest.com/blog/12-bad-data-visualization-examples-explained/#htoc-what-does-this-graph-show"
  },
  {
    "objectID": "Course_Content/Week_6/1/Notebook.html#key-principles-of-effective-data-visualization",
    "href": "Course_Content/Week_6/1/Notebook.html#key-principles-of-effective-data-visualization",
    "title": "Data Visualization",
    "section": "3. Key Principles of Effective Data Visualization",
    "text": "3. Key Principles of Effective Data Visualization\n\nChoosing the right visual representation for different types of data.\nUnderstanding the target audience and their needs.\nDesigning for clarity, simplicity, and accuracy.\nHighlighting the main message and avoiding clutter.\nUsing color, size, and shape effectively."
  },
  {
    "objectID": "Course_Content/Week_6/1/Notebook.html#python-libraries-tools",
    "href": "Course_Content/Week_6/1/Notebook.html#python-libraries-tools",
    "title": "Data Visualization",
    "section": "4. Python Libraries / Tools",
    "text": "4. Python Libraries / Tools\n\nmatplotlib\nplotly\nseaborn\n\nCheckout -\n\nhttps://datavizcatalogue.com/search.html\nhttps://python-graph-gallery.com/\n\n\nMatplotlib\nExcellent Start - https://matplotlib.org/stable/tutorials/introductory/quick_start.html#\n\n\nPlot linear, quadratic, and cubic functions from 0 to 2, with 100 equally spaced intervals\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nx = np.linspace(0, 2, 100)  # Sample data.\nx\n\narray([0.        , 0.02020202, 0.04040404, 0.06060606, 0.08080808,\n       0.1010101 , 0.12121212, 0.14141414, 0.16161616, 0.18181818,\n       0.2020202 , 0.22222222, 0.24242424, 0.26262626, 0.28282828,\n       0.3030303 , 0.32323232, 0.34343434, 0.36363636, 0.38383838,\n       0.4040404 , 0.42424242, 0.44444444, 0.46464646, 0.48484848,\n       0.50505051, 0.52525253, 0.54545455, 0.56565657, 0.58585859,\n       0.60606061, 0.62626263, 0.64646465, 0.66666667, 0.68686869,\n       0.70707071, 0.72727273, 0.74747475, 0.76767677, 0.78787879,\n       0.80808081, 0.82828283, 0.84848485, 0.86868687, 0.88888889,\n       0.90909091, 0.92929293, 0.94949495, 0.96969697, 0.98989899,\n       1.01010101, 1.03030303, 1.05050505, 1.07070707, 1.09090909,\n       1.11111111, 1.13131313, 1.15151515, 1.17171717, 1.19191919,\n       1.21212121, 1.23232323, 1.25252525, 1.27272727, 1.29292929,\n       1.31313131, 1.33333333, 1.35353535, 1.37373737, 1.39393939,\n       1.41414141, 1.43434343, 1.45454545, 1.47474747, 1.49494949,\n       1.51515152, 1.53535354, 1.55555556, 1.57575758, 1.5959596 ,\n       1.61616162, 1.63636364, 1.65656566, 1.67676768, 1.6969697 ,\n       1.71717172, 1.73737374, 1.75757576, 1.77777778, 1.7979798 ,\n       1.81818182, 1.83838384, 1.85858586, 1.87878788, 1.8989899 ,\n       1.91919192, 1.93939394, 1.95959596, 1.97979798, 2.        ])\n\n\n\nplt.figure(figsize=(5, 2.7), layout='constrained')\n\n&lt;Figure size 500x270 with 0 Axes&gt;\n\n\n&lt;Figure size 500x270 with 0 Axes&gt;\n\n\n\nplt.figure(figsize=(5, 2.7), layout='constrained')\nplt.plot(x, x, label='linear')  # Plot some data on the (implicit) axes.\nplt.plot(x, x**2, label='quadratic')  # etc.\nplt.plot(x, x**3, label='cubic')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x117cadcc0&gt;\n\n\n\n\n\n\nplt.figure(figsize=(5, 2.7), layout='constrained')\nplt.plot(x, x, label='linear')  # Plot some data on the (implicit) axes.\nplt.plot(x, x**2, label='quadratic')  # etc.\nplt.plot(x, x**3, label='cubic')\nplt.xlabel('x label')\nplt.ylabel('y label')\nplt.title(\"Simple Plot\")\nplt.legend()\nplt.show()\n\n\n\n\n\nMultiple graphs in one figure\n\n\nfig, axs = plt.subplots(2, 2, layout='constrained')\naxs[0][0].plot(x,x)\naxs[0][0].set_title(\"Linear\")\n\naxs[0][1].plot(x,x**2)\naxs[0][1].set_xlabel(\"X Label\")\n\naxs[1][0].plot(x,x**3)\naxs[1][0].set_ylabel(\"Y Label\")\naxs[1][1].plot(x,x**4, label=\"The Curve\")\naxs[1][1].legend()\n\nplt.show()\n\n\n\n\n\nfig, axd = plt.subplot_mosaic([['upleft', 'right'],\n                               ['lowleft', 'right']], layout='constrained')\naxd['upleft'].set_title('upleft')\naxd['lowleft'].set_title('lowleft')\naxd['right'].set_title('right')\n\nText(0.5, 1.0, 'right')"
  },
  {
    "objectID": "Course_Content/Week_6/3/home.html",
    "href": "Course_Content/Week_6/3/home.html",
    "title": "6.2 - Hands on Demo with Linear Regression",
    "section": "",
    "text": "How to work through a data science problem?\nSmall discussions around Feature Selection, Statistical Tests etc ( We will come back to this later. Don’t worry :))\n\n\n\n\n\n\nNotebook\nDataset\n\n\n\n\n\n\n\n\n\n7.1 Supervised Learning cont …"
  },
  {
    "objectID": "Course_Content/Week_6/3/home.html#objectives",
    "href": "Course_Content/Week_6/3/home.html#objectives",
    "title": "6.2 - Hands on Demo with Linear Regression",
    "section": "",
    "text": "How to work through a data science problem?\nSmall discussions around Feature Selection, Statistical Tests etc ( We will come back to this later. Don’t worry :))"
  },
  {
    "objectID": "Course_Content/Week_6/3/home.html#materials",
    "href": "Course_Content/Week_6/3/home.html#materials",
    "title": "6.2 - Hands on Demo with Linear Regression",
    "section": "",
    "text": "Notebook\nDataset"
  },
  {
    "objectID": "Course_Content/Week_6/3/home.html#next-class",
    "href": "Course_Content/Week_6/3/home.html#next-class",
    "title": "6.2 - Hands on Demo with Linear Regression",
    "section": "",
    "text": "7.1 Supervised Learning cont …"
  },
  {
    "objectID": "Course_Content/Week_6/3/Notebook.html",
    "href": "Course_Content/Week_6/3/Notebook.html",
    "title": "Sales Prediction",
    "section": "",
    "text": "Problem Statement\nBuild a model which predicts sales based on the money spent on different platforms for marketing.\nDataset - https://www.kaggle.com/datasets/ashydv/advertising-dataset"
  },
  {
    "objectID": "Course_Content/Week_6/3/Notebook.html#reading-the-data",
    "href": "Course_Content/Week_6/3/Notebook.html#reading-the-data",
    "title": "Sales Prediction",
    "section": "1. Reading the Data",
    "text": "1. Reading the Data\n::: {.cell _uuid=‘1365d38deb407ea9c0f4e93830c5f9d4d65ebd9d’ execution_count=2}\nadvertising = pd.DataFrame(pd.read_csv(\"advertising.csv\"))\nadvertising.head()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n12.0\n\n\n3\n151.5\n41.3\n58.5\n16.5\n\n\n4\n180.8\n10.8\n58.4\n17.9\n\n\n\n\n\n\n:::\n::: {.cell _uuid=‘4f36948806d235d179b1a5c6b6c990a41afc6e4a’ scrolled=‘true’ execution_count=3}\nadvertising.shape\n\n(200, 4)\n\n:::\n::: {.cell _uuid=‘9578033b7d507aa4d901b48de36931066cc00241’ execution_count=4}\nadvertising.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 4 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   TV         200 non-null    float64\n 1   Radio      200 non-null    float64\n 2   Newspaper  200 non-null    float64\n 3   Sales      200 non-null    float64\ndtypes: float64(4)\nmemory usage: 6.4 KB\n\n:::\n::: {.cell _uuid=‘b817b9601c376627448453b03d79bf8f9dd02eac’ execution_count=5}\nadvertising.describe()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n147.042500\n23.264000\n30.554000\n15.130500\n\n\nstd\n85.854236\n14.846809\n21.778621\n5.283892\n\n\nmin\n0.700000\n0.000000\n0.300000\n1.600000\n\n\n25%\n74.375000\n9.975000\n12.750000\n11.000000\n\n\n50%\n149.750000\n22.900000\n25.750000\n16.000000\n\n\n75%\n218.825000\n36.525000\n45.100000\n19.050000\n\n\nmax\n296.400000\n49.600000\n114.000000\n27.000000\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Course_Content/Week_6/3/Notebook.html#data-cleaning",
    "href": "Course_Content/Week_6/3/Notebook.html#data-cleaning",
    "title": "Sales Prediction",
    "section": "2. Data Cleaning",
    "text": "2. Data Cleaning\n::: {.cell _uuid=‘cf9580e58b78c0558d96f54272701b6d2d32a018’ execution_count=9}\nadvertising.isnull().sum()\n\nTV           0\nRadio        0\nNewspaper    0\nSales        0\ndtype: int64\n\n:::\nThere are no NULL values in the dataset, hence it is clean."
  },
  {
    "objectID": "Course_Content/Week_6/3/Notebook.html#exploratory-data-analysis",
    "href": "Course_Content/Week_6/3/Notebook.html#exploratory-data-analysis",
    "title": "Sales Prediction",
    "section": "3. Exploratory Data Analysis",
    "text": "3. Exploratory Data Analysis\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=2, ncols=2,figsize=(10,10))\n\nadvertising['TV'].plot.box(ax=axes[0,0])\nadvertising['Radio'].plot.box(ax=axes[0,1])\nadvertising['Newspaper'].plot.box(ax=axes[1,0])\nadvertising['Sales'].plot.box(ax=axes[1,1])\nplt.show()\n\n\n\n\n::: {.cell _uuid=‘2d6f716ebe182a58f9941c059256a09cc7f03703’ execution_count=11}\ng = pd.plotting.scatter_matrix(advertising, figsize=(10,10))\nplt.show()\n\n\n\n:::\n\nadvertising.corr()\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\nTV\n1.000000\n0.054809\n0.056648\n0.901208\n\n\nRadio\n0.054809\n1.000000\n0.354104\n0.349631\n\n\nNewspaper\n0.056648\n0.354104\n1.000000\n0.157960\n\n\nSales\n0.901208\n0.349631\n0.157960\n1.000000\n\n\n\n\n\n\n\n\nf = plt.figure(figsize=(5, 5))\nplt.matshow(advertising.corr(), fignum=f.number , )\nplt.xticks(range(advertising.select_dtypes(['number']).shape[1]), advertising.select_dtypes(['number']).columns, fontsize=14, rotation=45)\nplt.yticks(range(advertising.select_dtypes(['number']).shape[1]), advertising.select_dtypes(['number']).columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title('Correlation Matrix', fontsize=16);\n\n\n\n\n\nf_stats, p_values = f_regression(advertising[['TV','Radio','Newspaper']].to_numpy(),advertising['Sales'].to_numpy())\nf_stats, p_values\n\n(array([856.17671282,  27.57467815,   5.0667947 ]),\n array([0.        , 0.00000039, 0.02548744]))"
  },
  {
    "objectID": "Course_Content/Week_6/3/Notebook.html#model-building",
    "href": "Course_Content/Week_6/3/Notebook.html#model-building",
    "title": "Sales Prediction",
    "section": "Model Building",
    "text": "Model Building\n\nDataset Preparation\nWe first assign the feature variable, TV, in this case, to the variable X and the response variable, Sales, to the variable y.\n::: {.cell _uuid=‘ae7285c79fd678fad0ee4fb18f8923daf024838b’ execution_count=15}\nX = advertising['TV'].to_numpy()\ny = advertising['Sales'].to_numpy()\n:::\n\nTrain-Val-Test Split\nYou now need to split our variable into training and testing sets. You’ll perform this by importing train_test_split from the sklearn.model_selection library. It is usually a good practice to keep 70% of the data in your train dataset and the rest 30% in your test dataset\n::: {.cell _uuid=‘997311202075aaa98631ef95c1a0d91cdbefa2af’ execution_count=16}\nX_train, X_test_and_val, y_train, y_test_and_val = train_test_split(X, y, train_size = 0.7,random_state = 100)\nX_val, X_test, y_val, y_test = train_test_split(X_test_and_val, y_test_and_val, test_size = 0.5, random_state = 100)\n:::\n\nprint(len(X_train),len(X_val),len(X_test))\n\n140 30 30\n\n\n\n\nBuilding a Linear Model\n\nmodel = LinearRegression()\n\n\nX_train.reshape(-1, 1)\n\narray([[213.4],\n       [151.5],\n       [205. ],\n       [142.9],\n       [134.3],\n       [ 80.2],\n       [239.8],\n       [ 88.3],\n       [ 19.4],\n       [225.8],\n       [136.2],\n       [ 25.1],\n       [ 38. ],\n       [172.5],\n       [109.8],\n       [240.1],\n       [232.1],\n       [ 66.1],\n       [218.4],\n       [234.5],\n       [ 23.8],\n       [ 67.8],\n       [296.4],\n       [141.3],\n       [175.1],\n       [220.5],\n       [ 76.4],\n       [253.8],\n       [191.1],\n       [287.6],\n       [100.4],\n       [228. ],\n       [125.7],\n       [ 74.7],\n       [ 57.5],\n       [262.7],\n       [262.9],\n       [237.4],\n       [227.2],\n       [199.8],\n       [228.3],\n       [290.7],\n       [276.9],\n       [199.8],\n       [239.3],\n       [ 73.4],\n       [284.3],\n       [147.3],\n       [224. ],\n       [198.9],\n       [276.7],\n       [ 13.2],\n       [ 11.7],\n       [280.2],\n       [ 39.5],\n       [265.6],\n       [ 27.5],\n       [280.7],\n       [ 78.2],\n       [163.3],\n       [213.5],\n       [293.6],\n       [ 18.7],\n       [ 75.5],\n       [166.8],\n       [ 44.7],\n       [109.8],\n       [  8.7],\n       [266.9],\n       [206.9],\n       [149.8],\n       [ 19.6],\n       [ 36.9],\n       [199.1],\n       [265.2],\n       [165.6],\n       [140.3],\n       [230.1],\n       [  5.4],\n       [ 17.9],\n       [237.4],\n       [286. ],\n       [ 93.9],\n       [292.9],\n       [ 25. ],\n       [ 97.5],\n       [ 26.8],\n       [281.4],\n       [ 69.2],\n       [ 43.1],\n       [255.4],\n       [239.9],\n       [209.6],\n       [  7.3],\n       [240.1],\n       [102.7],\n       [243.2],\n       [137.9],\n       [ 18.8],\n       [ 17.2],\n       [ 76.4],\n       [139.5],\n       [261.3],\n       [ 66.9],\n       [ 48.3],\n       [177. ],\n       [ 28.6],\n       [180.8],\n       [222.4],\n       [193.7],\n       [ 59.6],\n       [131.7],\n       [  8.4],\n       [ 13.1],\n       [  4.1],\n       [  0.7],\n       [ 76.3],\n       [250.9],\n       [273.7],\n       [ 96.2],\n       [210.8],\n       [ 53.5],\n       [ 90.4],\n       [104.6],\n       [283.6],\n       [ 95.7],\n       [204.1],\n       [ 31.5],\n       [182.6],\n       [289.7],\n       [156.6],\n       [107.4],\n       [ 43. ],\n       [248.4],\n       [116. ],\n       [110.7],\n       [187.9],\n       [139.3],\n       [ 62.3],\n       [  8.6]])\n\n\n::: {.cell _uuid=‘b80a766082e6c9c40c3f09499fec4cfc51f62763’ execution_count=25}\nmodel.fit(X_train.reshape(-1, 1),y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n:::\n::: {.cell _uuid=‘fd4287b550d2f05555ae3e18d6f497912424f8cf’ execution_count=27}\n# Print the parameters, i.e. the intercept and the slope of the regression line fitted\nb = model.intercept_ \nw1 = model.coef_[0]\n\nprint(str(w1)+\"x\"+\"+\"+str(b))\n\n0.05454575291590793x+6.948683200001362\n\n:::\n\nprediction = model.predict(X_val.reshape(-1, 1))\nprediction\n\narray([13.52144643, 18.86693021, 13.1068987 , 17.1923756 , 19.94148154,\n       10.71234015, 20.13239168, 11.05597839,  9.03233096, 17.60692332,\n       13.66326538, 18.77420243, 15.11418241, 12.25053038, 18.44147334,\n       17.72692398, 14.09963141, 11.70507285, 17.99419817, 14.32326899,\n        9.67597085, 10.6796127 , 13.34144544, 10.79961336, 12.08689312,\n       16.60328147, 17.48692266, 18.82329361, 17.03419291, 18.75238413])\n\n\n\nmean_squared_error(y_val.reshape(-1, 1),prediction)\n\n4.42099969589266\n\n\n\n\n\nR2\nR-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit)\n\nr2_score(y_val.reshape(-1, 1),prediction)\n\n0.7587703509647371\n\n\n::: {.cell _uuid=‘6e0dc97a88b9fc1d4e975c2fe511e59bd0cd2b8a’ execution_count=31}\nplt.scatter(X_train, y_train)\nplt.plot(X_train, (w1 * X_train )+ b, 'r')\nplt.show()\n\n\n\n:::"
  },
  {
    "objectID": "Course_Content/Week_6/3/Notebook.html#model-evaluation",
    "href": "Course_Content/Week_6/3/Notebook.html#model-evaluation",
    "title": "Sales Prediction",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n::: {.cell _uuid=‘0b64c5e3173c685b0715a93f0a77c759e90b2dff’ execution_count=32}\npredictions = model.predict(X_test.reshape(-1,1))\n:::\n::: {.cell _uuid=‘58863bc73dfa751e6bade66b3b71f80be51d9ca6’ execution_count=33}\nmean_squared_error(y_test, predictions)\n\n3.734113047761241\n\n:::\n\nChecking the R-squared on the test set\n::: {.cell _uuid=‘6ce19fc28741a4d2b558a377f2fd39c81abdb72e’ execution_count=34}\nr_squared = r2_score(y_test, predictions)\nr_squared\n\n0.8149944458734971\n\n:::\n\n\nVisualizing the fit on the test set\n::: {.cell _uuid=‘eb08ac34d4e148e3221adfe126072f108adbfa24’ execution_count=35}\nplt.scatter(X_test, y_test)\nplt.plot(X_test, (w1 * X_test )+ b, 'r')\nplt.show()\n\n\n\n:::"
  },
  {
    "objectID": "Course_Content/Week_6/2/home.html",
    "href": "Course_Content/Week_6/2/home.html",
    "title": "6.2 - Machine Learning - Key Concepts & Supervised ML : Linear Regression",
    "section": "",
    "text": "ML Terminology:\n\nLabels, Features, Examples, Models\nTraining, Inference/Testing,\n\nLinear Regression:\n\nEquation, Loss, Update weights based on Loss\nLASSO, RIDGE\n\nGradient Descent\n\nStochastic, Batch, Mini-batch stochastic\n\nLoss Functions :\n\nEsp MAE, MSE\n\nLearning rate\nOverfitting v/s Underfitting\nData -\n\nSplit as (Train, Val, test)\nK Cross validation\n\nRegularisation :\n\nReducing Model Complexity:\n\nL1/L2 Regularisation\nDropout\nEarly stopping\n\nData Augmentation\n\n\n\n\n\n\n\nSlides\nBoard I used\n\n\n\n\n\n\n\n\n\n6.3 Supervised Learning - Logistic Regression & Getting to code"
  },
  {
    "objectID": "Course_Content/Week_6/2/home.html#objectives",
    "href": "Course_Content/Week_6/2/home.html#objectives",
    "title": "6.2 - Machine Learning - Key Concepts & Supervised ML : Linear Regression",
    "section": "",
    "text": "ML Terminology:\n\nLabels, Features, Examples, Models\nTraining, Inference/Testing,\n\nLinear Regression:\n\nEquation, Loss, Update weights based on Loss\nLASSO, RIDGE\n\nGradient Descent\n\nStochastic, Batch, Mini-batch stochastic\n\nLoss Functions :\n\nEsp MAE, MSE\n\nLearning rate\nOverfitting v/s Underfitting\nData -\n\nSplit as (Train, Val, test)\nK Cross validation\n\nRegularisation :\n\nReducing Model Complexity:\n\nL1/L2 Regularisation\nDropout\nEarly stopping\n\nData Augmentation"
  },
  {
    "objectID": "Course_Content/Week_6/2/home.html#materials",
    "href": "Course_Content/Week_6/2/home.html#materials",
    "title": "6.2 - Machine Learning - Key Concepts & Supervised ML : Linear Regression",
    "section": "",
    "text": "Slides\nBoard I used"
  },
  {
    "objectID": "Course_Content/Week_6/2/home.html#next-class",
    "href": "Course_Content/Week_6/2/home.html#next-class",
    "title": "6.2 - Machine Learning - Key Concepts & Supervised ML : Linear Regression",
    "section": "",
    "text": "6.3 Supervised Learning - Logistic Regression & Getting to code"
  },
  {
    "objectID": "Course_Content/Week_1/home.html",
    "href": "Course_Content/Week_1/home.html",
    "title": "Week 1 : Course Introduction & Setup",
    "section": "",
    "text": "Develop proficiency in using data science tools and programming languages.\n\n\n\n\nN/A\n\n\n\n\n\n1.1 Welcome & Introduction\n1.2 Introduction to Data Science\n\n\n\n\n\n\n\nSyllabus Prompts : Easter eggs hidden in the syllabus.\nHomework0: Setup for Class\n\n\n\n\n\nGetting to know : Google Form"
  },
  {
    "objectID": "Course_Content/Week_1/home.html#objectives",
    "href": "Course_Content/Week_1/home.html#objectives",
    "title": "Week 1 : Course Introduction & Setup",
    "section": "",
    "text": "Develop proficiency in using data science tools and programming languages.\n\n\n\n\nN/A"
  },
  {
    "objectID": "Course_Content/Week_1/home.html#lectures",
    "href": "Course_Content/Week_1/home.html#lectures",
    "title": "Week 1 : Course Introduction & Setup",
    "section": "",
    "text": "1.1 Welcome & Introduction\n1.2 Introduction to Data Science"
  },
  {
    "objectID": "Course_Content/Week_1/home.html#activities",
    "href": "Course_Content/Week_1/home.html#activities",
    "title": "Week 1 : Course Introduction & Setup",
    "section": "",
    "text": "Syllabus Prompts : Easter eggs hidden in the syllabus.\nHomework0: Setup for Class\n\n\n\n\n\nGetting to know : Google Form"
  },
  {
    "objectID": "Course_Content/Week_1/1/home.html",
    "href": "Course_Content/Week_1/1/home.html",
    "title": "1.1 Introduction and Logistics",
    "section": "",
    "text": "Introduction to the course.\nLogistics.\nExpectations and Course Plan.\n\n\n\n\n\nSyllabus\n\n\n\n\n\n\n\n\n\nIntroduction to Data Science"
  },
  {
    "objectID": "Course_Content/Week_1/1/home.html#class-objectives",
    "href": "Course_Content/Week_1/1/home.html#class-objectives",
    "title": "1.1 Introduction and Logistics",
    "section": "",
    "text": "Introduction to the course.\nLogistics.\nExpectations and Course Plan."
  },
  {
    "objectID": "Course_Content/Week_1/1/home.html#materials",
    "href": "Course_Content/Week_1/1/home.html#materials",
    "title": "1.1 Introduction and Logistics",
    "section": "",
    "text": "Syllabus"
  },
  {
    "objectID": "Course_Content/Week_1/1/home.html#next-class",
    "href": "Course_Content/Week_1/1/home.html#next-class",
    "title": "1.1 Introduction and Logistics",
    "section": "",
    "text": "Introduction to Data Science"
  },
  {
    "objectID": "Course_Content/Week_1/2/home.html",
    "href": "Course_Content/Week_1/2/home.html",
    "title": "1.2 Introduction to Data Science",
    "section": "",
    "text": "Introduce the field of data science and introductory topics around them.\n\n\n\n\nN/A\n\n\n\n\n\n\n\n\nWeek 2"
  },
  {
    "objectID": "Course_Content/Week_1/2/home.html#class-objectives",
    "href": "Course_Content/Week_1/2/home.html#class-objectives",
    "title": "1.2 Introduction to Data Science",
    "section": "",
    "text": "Introduce the field of data science and introductory topics around them."
  },
  {
    "objectID": "Course_Content/Week_1/2/home.html#materials",
    "href": "Course_Content/Week_1/2/home.html#materials",
    "title": "1.2 Introduction to Data Science",
    "section": "",
    "text": "N/A"
  },
  {
    "objectID": "Course_Content/Week_1/2/home.html#next-lecture",
    "href": "Course_Content/Week_1/2/home.html#next-lecture",
    "title": "1.2 Introduction to Data Science",
    "section": "",
    "text": "Week 2"
  },
  {
    "objectID": "Course_Content/Week_7/home.html",
    "href": "Course_Content/Week_7/home.html",
    "title": "Week 7 : Supervised and Unsupervised Learning",
    "section": "",
    "text": "Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nGain familiarity with machine learning algorithms and their practical applications.\nDevelop proficiency in using data science tools and programming languages.\nEngage in critical thinking and problem-solving through project-based assignments.\nExplore the ethical considerations associated with data-driven decision-making.\nStay informed about current trends and developments in data science and artificial intelligence.\n\n\n\n\n\nExplain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.)\nConvert a raw data source into a version appropriate for downstream analysis using Python. | Week 4 & 5 .. cont to 7\nSelect the appropriate evaluation measure for the dataset and task being solved\nArticulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem | Week 6 .. cont to 7\nDemonstrate awareness of bias and ethics in data science.\n\n\n\n\n\n\n\n7.1 Data Preprocessing and ML ( Linear Regression, Naive Bayes and Kmeans)\nLecture 7.2 : Evaluation and cont Machine Learning.\n\n\n\n\n\n\nHomework 4\n\n\n\n\nWeekly Checkin for Week 6 | Due 5 pm , July 23, Sunday ( Based on Lecture 6.1,6.2,6.3 )\nWeekly Checkin for Week 7 opens"
  },
  {
    "objectID": "Course_Content/Week_7/home.html#objectives",
    "href": "Course_Content/Week_7/home.html#objectives",
    "title": "Week 7 : Supervised and Unsupervised Learning",
    "section": "",
    "text": "Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nGain familiarity with machine learning algorithms and their practical applications.\nDevelop proficiency in using data science tools and programming languages.\nEngage in critical thinking and problem-solving through project-based assignments.\nExplore the ethical considerations associated with data-driven decision-making.\nStay informed about current trends and developments in data science and artificial intelligence.\n\n\n\n\n\nExplain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.)\nConvert a raw data source into a version appropriate for downstream analysis using Python. | Week 4 & 5 .. cont to 7\nSelect the appropriate evaluation measure for the dataset and task being solved\nArticulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem | Week 6 .. cont to 7\nDemonstrate awareness of bias and ethics in data science."
  },
  {
    "objectID": "Course_Content/Week_7/home.html#lectures",
    "href": "Course_Content/Week_7/home.html#lectures",
    "title": "Week 7 : Supervised and Unsupervised Learning",
    "section": "",
    "text": "7.1 Data Preprocessing and ML ( Linear Regression, Naive Bayes and Kmeans)\nLecture 7.2 : Evaluation and cont Machine Learning."
  },
  {
    "objectID": "Course_Content/Week_7/home.html#activities",
    "href": "Course_Content/Week_7/home.html#activities",
    "title": "Week 7 : Supervised and Unsupervised Learning",
    "section": "",
    "text": "Homework 4\n\n\n\n\nWeekly Checkin for Week 6 | Due 5 pm , July 23, Sunday ( Based on Lecture 6.1,6.2,6.3 )\nWeekly Checkin for Week 7 opens"
  },
  {
    "objectID": "Course_Content/Week_7/home.html#summary-plan-versus-achievements",
    "href": "Course_Content/Week_7/home.html#summary-plan-versus-achievements",
    "title": "Week 7 : Supervised and Unsupervised Learning",
    "section": "",
    "text": "Strikethrough text is changes in plan.\nGreen is new items added after planning.\nChecked boxes are completed items.\n\n\n\n\n\n\n\n3 lectures on\nSupervised methods:\n\ncont ..on Linear Regression\nLogistic Regression\n\nUnsupervised Methods:\n\nKnn\nK means\n\nNeural networks \n\nif possible - Model Assesment - Feature Selection\n\n\n\n\nHomework 4 release\nHomework 3 due\n\n\n\n\n\nAdd Ethical question prompt to Week 6 Weekly Checkin\nCheck-in for week 7 opens ( Based on Lecture 7.1,7.2,7.3 )\n\n\n\n\n\nWeek 7 assigned reading discussion question to be released\n\n\n\n\n\nDataset Selection, exploration and viz\n\n\n\n\n\nBonus Question - Data Viz | Due by July 22nd, 2023 | 11:59 pm"
  },
  {
    "objectID": "Bonus/home.html",
    "href": "Bonus/home.html",
    "title": "Bonus Questions",
    "section": "",
    "text": "Bonus Questions\n\nActive\nDataset Suggestions - 1 point Summarise Viz Types - 2 points\n\n\nPast\nSonam’s Dilema"
  },
  {
    "objectID": "Bonus/1.html",
    "href": "Bonus/1.html",
    "title": "Sonam’s Dilemma - A Gift for Few or a Chance for All",
    "section": "",
    "text": "Sonam’s Dilemma - A Gift for Few or a Chance for All\n\nBonus Question : 1 point\n\n\n\nInstructions\n\nDue Date : July 7, Friday, 5 pm\nSubmit your answer at D2 &gt; Quiz &gt; “Sonam’s Dilemma : a gift for few or a chance for all”\n\n\n\nQuestion\n Source : Story I made up, text improved by ChatGPT, and then edited by myself.\nOnce upon a time, in the grand kingdom of Razia, there existed a mesmerizing lead dancer named Sonam. She possesed a combination of charm, extraordinary talent, and intelligence that made her stand out amongst her peers. Sonam’s passion for dance was matched only by her love for her dance group,with 2000 other talented dancers, who all shared an unbreakable bond and admired their noble Queen above all.\nOne fine day, as Sonam’s heart overflowed with gratitude for their benevolent ruler, she decided to express her deep admiration through the power of music. She crafted a beautiful song that portrayed the love and adoration her dance group held for Queen Razia. With eager anticipation, Sonam’s troupe dedicated themselves to perfecting their dance moves to match the rhythm and emotion of her heartfelt composition.\nThe auspicious occasion of Queen Razia’s birthday arrived, filling the grand palace with a sense of joy and celebration. The Queen, known for her discerning taste and appreciation for the arts, eagerly awaited the performance of Sonam’s dance group. As the music began to play and the dancers gracefully glided across the grand ballroom, a spell was cast upon all those in attendance.\nThe Queen was captivated by the talent displayed before her, and her heart swelled with pride and delight. She could not help but marvel at the synchronized movements and the sheer passion that emanated from each and every member of the dance group. As the performance reached its crescendo, Queen Razia’s decision was made in an instant—this remarkable group deserved a gift to commemorate their extraordinary dedication.\n\n\n\nHowever, there were only a limited number of gift available to give out right now. She presented Sonam with two choices, each with its own intriguing possibilities. The first option was straightforward : 660 randomly chosen dancers would receive a gift. This meant that a select few would be rewarded, leaving the remaining members without a token of appreciation.\nThe second choice, however, offered a twist. Queen Shah proposed distributing the available gifts among all dancers, but with a twist of uncertainty. Each dancer would have a 33% chance of receiving a gift. 67% of not receiving a gift.\nNow faced with this dilemma, Sonam’s sharp mind began to evaluate the situation. Which option should Sonam pick? Why? Why not the other?"
  },
  {
    "objectID": "Bonus/3.html",
    "href": "Bonus/3.html",
    "title": "Summarise Viz types",
    "section": "",
    "text": "What are the key takeaways summarizing the types of visualization discussed in the blog shared during Lecture 6.1? The blog delves into various groups of data visualization and their respective visualization types, best practices etc. Please provide a concise overview of what you learned/understood , comprising approximately 250 to 500 words.\nYou don’t have to read all the links, but go through as many as you can, and summarise.\n\n\n\n\n\n\nSelect a chart from the mentioned blog that resonated with you the most, and provide a concise overview of what you learned/understood , comprising approximately 250 to 500 words."
  },
  {
    "objectID": "Bonus/3.html#summarise",
    "href": "Bonus/3.html#summarise",
    "title": "Summarise Viz types",
    "section": "",
    "text": "What are the key takeaways summarizing the types of visualization discussed in the blog shared during Lecture 6.1? The blog delves into various groups of data visualization and their respective visualization types, best practices etc. Please provide a concise overview of what you learned/understood , comprising approximately 250 to 500 words.\nYou don’t have to read all the links, but go through as many as you can, and summarise."
  },
  {
    "objectID": "Bonus/3.html#chart-of-focus",
    "href": "Bonus/3.html#chart-of-focus",
    "title": "Summarise Viz types",
    "section": "",
    "text": "Select a chart from the mentioned blog that resonated with you the most, and provide a concise overview of what you learned/understood , comprising approximately 250 to 500 words."
  },
  {
    "objectID": "Bonus/2.html",
    "href": "Bonus/2.html",
    "title": "Dataset Suggestions",
    "section": "",
    "text": "In the D2L platform, a Google Sheets document has been shared for this task. Please note that the document will not be shared on the website, in accordance with FERPA guidelines.\n\n\n\nYour objective is to discover three datasets that are not widely known or extensively covered online. Locate your name in the first column and proceed as follows: in the second column, provide the name of the dataset, and in the third column, share the corresponding link. The additional condition is that, you need to select a dataset that has NOT already been listed by someone else in the document.\n\n\n\nIn the fourth column, suggest the name of one dataset from the sheet ( it could be from anywhere in the sheet, not restricted to your suggestions) that you may want to be considered for your final project. Remember, this suggestion does not signify a commitment but rather serves as your personal recommendation.\nExplore various sources and discover intriguing datasets. This is an excellent opportunity to dive into unexplored data and expand your knowledge of available datasets.\nNo formal submission on D2L is needed for the project. The grader will check the common sheet."
  },
  {
    "objectID": "Bonus/2.html#dataset-suggestion-graded-out-of-3",
    "href": "Bonus/2.html#dataset-suggestion-graded-out-of-3",
    "title": "Dataset Suggestions",
    "section": "",
    "text": "Your objective is to discover three datasets that are not widely known or extensively covered online. Locate your name in the first column and proceed as follows: in the second column, provide the name of the dataset, and in the third column, share the corresponding link. The additional condition is that, you need to select a dataset that has NOT already been listed by someone else in the document."
  },
  {
    "objectID": "Bonus/2.html#recommendation-for-final-project-not-graded",
    "href": "Bonus/2.html#recommendation-for-final-project-not-graded",
    "title": "Dataset Suggestions",
    "section": "",
    "text": "In the fourth column, suggest the name of one dataset from the sheet ( it could be from anywhere in the sheet, not restricted to your suggestions) that you may want to be considered for your final project. Remember, this suggestion does not signify a commitment but rather serves as your personal recommendation.\nExplore various sources and discover intriguing datasets. This is an excellent opportunity to dive into unexplored data and expand your knowledge of available datasets.\nNo formal submission on D2L is needed for the project. The grader will check the common sheet."
  },
  {
    "objectID": "Honors/home.html",
    "href": "Honors/home.html",
    "title": "Honors Contract Information",
    "section": "",
    "text": "This Honors Contract aims to provide students with an opportunity to delve deeper into the subject matter of CSC 380, encouraging critical thinking, justifying stances, and fostering the development of a comprehensive data science project.\nThe honors work consists of 4 components.\n\n\n(~1500 words )\nIn Week 2, we explored the Influence of AI on Politics and engaged in discussions on the topic. Now, you are required to further explore this subject in a well-written essay of approximately 1000 words. Your essay should encompass an overview of the current situation, highlighting the key issues, and propose your stance on how we should address these challenges. Please provide a thoughtful explanation of your opinions and beliefs on the matter. I don’t need to agree with your stance, and my stance doesn’t matter in your work and grading.\n\n\n\nYou are required to select a dataset that does not have readily available solved code repositories. Utilize this dataset to solve a machine learning problem, following the structure of the final project. This includes tasks such as data cleaning, exploration, application of various machine learning methods, and thorough analysis of results and errors.\nSubmit a report formatted in the style of a research paper, although strict adherence to research paper guidelines is not necessary. The report should include sections such as an abstract, introduction, dataset description, methodology, results, error analysis, conclusion, limitations, ethical considerations, and references.\n\n\n\nDuring Week 7, a research paper will be provided to you. Your task is to summarize the work presented in the paper in one to two paragraphs. Focus on describing the problem being addressed, the approach taken by the authors, the performance of their method, and any future directions or comments you may have.\n\n\n\nFinally, on 4th August (tentative date), you will present your work so far in a concise presentation lasting no longer than five minutes. This presentation will be followed by a question and answer session.\nAll work should be submitted 2 days before Presentation. Satisfactory completion of all components translates to a Pass for Honors Credit."
  },
  {
    "objectID": "Honors/home.html#essay-on-an-ethical-question-stance",
    "href": "Honors/home.html#essay-on-an-ethical-question-stance",
    "title": "Honors Contract Information",
    "section": "",
    "text": "(~1500 words )\nIn Week 2, we explored the Influence of AI on Politics and engaged in discussions on the topic. Now, you are required to further explore this subject in a well-written essay of approximately 1000 words. Your essay should encompass an overview of the current situation, highlighting the key issues, and propose your stance on how we should address these challenges. Please provide a thoughtful explanation of your opinions and beliefs on the matter. I don’t need to agree with your stance, and my stance doesn’t matter in your work and grading."
  },
  {
    "objectID": "Honors/home.html#project",
    "href": "Honors/home.html#project",
    "title": "Honors Contract Information",
    "section": "",
    "text": "You are required to select a dataset that does not have readily available solved code repositories. Utilize this dataset to solve a machine learning problem, following the structure of the final project. This includes tasks such as data cleaning, exploration, application of various machine learning methods, and thorough analysis of results and errors.\nSubmit a report formatted in the style of a research paper, although strict adherence to research paper guidelines is not necessary. The report should include sections such as an abstract, introduction, dataset description, methodology, results, error analysis, conclusion, limitations, ethical considerations, and references."
  },
  {
    "objectID": "Honors/home.html#summarizing-a-research-paper",
    "href": "Honors/home.html#summarizing-a-research-paper",
    "title": "Honors Contract Information",
    "section": "",
    "text": "During Week 7, a research paper will be provided to you. Your task is to summarize the work presented in the paper in one to two paragraphs. Focus on describing the problem being addressed, the approach taken by the authors, the performance of their method, and any future directions or comments you may have."
  },
  {
    "objectID": "Honors/home.html#final-presentation",
    "href": "Honors/home.html#final-presentation",
    "title": "Honors Contract Information",
    "section": "",
    "text": "Finally, on 4th August (tentative date), you will present your work so far in a concise presentation lasting no longer than five minutes. This presentation will be followed by a question and answer session.\nAll work should be submitted 2 days before Presentation. Satisfactory completion of all components translates to a Pass for Honors Credit."
  },
  {
    "objectID": "Syllabus/Key_Info.html",
    "href": "Syllabus/Key_Info.html",
    "title": "Syllabus (Key Info)",
    "section": "",
    "text": "The course introduces students to the principles of data science, which are essential for computer scientists to make effective decisions in their professional careers. In today’s data-driven world, a wide range of computer science sub-disciplines heavily rely on data collection, analysis, and interpretation. With the pervasive presence of artificial intelligence (AI) in our lives, understanding the basics of how these systems work is becoming increasingly important. Additionally, it covers the basics of artificial intelligence (AI) systems and examines practical use cases, current news, and ethical considerations through readings and discussions.\n\n\n\nCourse Objectives\nThis course aims to introduce students to the principles and techniques of data science, enabling them to make effective decisions in their computer science careers. During this course, the student will,\n\nUnderstand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nApply data analysis and visualization techniques to derive insights from diverse datasets on Week 6\nGain familiarity with machine learning algorithms and their practical applications.\nDevelop proficiency in using data science tools and programming languages.\nEngage in critical thinking and problem-solving through project-based assignments.\nExplore the ethical considerations associated with data-driven decision-making.\nStay informed about current trends and developments in data science and artificial intelligence.\n\n\n\n\nA student who successfully completes this course will be able to:\n\nExplain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.)\nConvert a raw data source into a version appropriate for downstream analysis using Python. | Week 4 & 5\nWrite appropriate visualizations for different sources and types of data | Week 6\nExplain why we seek to build machine learning models that generalize rather than memorize their input. | Week 6\nExplain the different uses for training, validation, and testing datasets | Week 6\nSelect the appropriate evaluation measure for the dataset and task being solved\nArticulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem | Week 6\nDemonstrate awareness of bias and ethics in data science."
  },
  {
    "objectID": "Syllabus/Key_Info.html#description",
    "href": "Syllabus/Key_Info.html#description",
    "title": "Syllabus (Key Info)",
    "section": "",
    "text": "The course introduces students to the principles of data science, which are essential for computer scientists to make effective decisions in their professional careers. In today’s data-driven world, a wide range of computer science sub-disciplines heavily rely on data collection, analysis, and interpretation. With the pervasive presence of artificial intelligence (AI) in our lives, understanding the basics of how these systems work is becoming increasingly important. Additionally, it covers the basics of artificial intelligence (AI) systems and examines practical use cases, current news, and ethical considerations through readings and discussions."
  },
  {
    "objectID": "Syllabus/Key_Info.html#course-objective",
    "href": "Syllabus/Key_Info.html#course-objective",
    "title": "Syllabus (Key Info)",
    "section": "",
    "text": "Course Objectives\nThis course aims to introduce students to the principles and techniques of data science, enabling them to make effective decisions in their computer science careers. During this course, the student will,\n\nUnderstand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nApply data analysis and visualization techniques to derive insights from diverse datasets on Week 6\nGain familiarity with machine learning algorithms and their practical applications.\nDevelop proficiency in using data science tools and programming languages.\nEngage in critical thinking and problem-solving through project-based assignments.\nExplore the ethical considerations associated with data-driven decision-making.\nStay informed about current trends and developments in data science and artificial intelligence."
  },
  {
    "objectID": "Syllabus/Key_Info.html#expected-learning-outcomes",
    "href": "Syllabus/Key_Info.html#expected-learning-outcomes",
    "title": "Syllabus (Key Info)",
    "section": "",
    "text": "A student who successfully completes this course will be able to:\n\nExplain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.)\nConvert a raw data source into a version appropriate for downstream analysis using Python. | Week 4 & 5\nWrite appropriate visualizations for different sources and types of data | Week 6\nExplain why we seek to build machine learning models that generalize rather than memorize their input. | Week 6\nExplain the different uses for training, validation, and testing datasets | Week 6\nSelect the appropriate evaluation measure for the dataset and task being solved\nArticulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem | Week 6\nDemonstrate awareness of bias and ethics in data science."
  },
  {
    "objectID": "Syllabus/Syllabus.html",
    "href": "Syllabus/Syllabus.html",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "Asynchronous Online\n\n\nThe course introduces students to the principles of data science, which are essential for computer scientists to make effective decisions in their professional careers. In today’s data-driven world, a wide range of computer science sub-disciplines heavily rely on data collection, analysis, and interpretation. With the pervasive presence of artificial intelligence (AI) in our lives, understanding the basics of how these systems work is becoming increasingly important. Additionally, it covers the basics of artificial intelligence (AI) systems and examines practical use cases, current news, and ethical considerations through readings and discussions.\n\n\n\nThe course introduces students to principles of data science that are necessary for computer scientists to make effective decisions in their professional careers. A number of computer science sub-disciplines now rely on data collection and analysis. For example, computer systems are now complicated enough that comparing the execution performance of two different programs becomes a statistical estimation problem rather than a deterministic computation. This course teaches students the basic principles of how to properly collect and process data sources in order to derive appropriate conclusions from them. The course has three main components: data analysis, machine learning, and a project where students apply the concepts discussed in class to a substantial open-ended problem.\n\n\n\nMajor: COSCBA or COSCBS. Completion of CSC 210 and CSC 244. If you do not meet the enrollment requirements, contact the lecturer. Instructor and Contact Information\nInstructor: Enfa George Gould-Simpson 710 enfageorge@arizona.edu\nTeaching Assistant: Bennett Brixen bennettbrixen@arizona.edu\nOffice Hours Monday : 11 am to 12 pm - Bennett, TA Tuesday : 1 pm to 2 pm - Bennett, TA Wednesday : 2 pm to 3 pm - Enfa, Instructor Thursday 4 pm to 5 pm - Bennett, TA Friday : 3 pm to 4 pm - Enfa, Instructor\nBennett’s virtual office: https://arizona.zoom.us/j/8363357057 Enfa’s virtual office: https://arizona.zoom.us/j/87083172857\n\n\n\n\nAsynchronous Online Lectures: Pre-recorded lectures will be provided for students to access at their convenience.\nDiscussions on Piazza: Piazza will be used for students to ask questions, engage in discussions, and seek clarifications from the instructor and teaching assistant.\nProject-Based Homeworks: Assignments will be designed to apply data science techniques to real-world problems.\nWeekly Check-ins: Regular individual check-ins via a D2L Quiz. This will involve questions/discussions related to the reading, a platform to address concerns, and provide live feedback on the course Obtaining Help\n\nAcademic advising: If you have questions about your academic progress this semester, or your chosen degree program, consider contacting your department’s academic advisor(s). Your academic advisor and the Advising Resource Center can guide you toward university resources to help you succeed. Computer Science major students are encouraged to visit https://www.cs.arizona.edu/undergraduate/advising for advisor contact information. CS Help Desk: The Computer Science IT team can help students with department technology issues including logging into/resetting your Lectura account, printing in the 930 lab, etc. You can submit a ticket for help by visiting the Computer Science Lab Helpdesk (note, requires UA login).\nLife challenges: If you are experiencing unexpected barriers to your success in your courses, please note the Dean of Students Office is a central support resource for all students and may be helpful. The Dean of Students Office can be reached at 520-621-2057 or DOS-deanofstudents@email.arizona.edu. Physical and mental-health challenges: If you are facing physical or mental health challenges this semester, please note that Campus Health provides quality medical and mental health care. For medical appointments, call (520-621-9202. For After Hours care, call (520) 570-7898. For the Counseling & Psych Services (CAPS) 24/7 hotline, call (520) 621-3334.\nUA Ombuds: The UA Ombuds Office (https://ombuds.arizona.edu/) helps with a wide variety of issues, concerns, questions, conflicts, and challenges. The primary mission of the Ombuds Program is to assist individuals in resolving conflict, facilitating communication, and assisting the University by surfacing issues and providing feedback on emerging or systemic concerns. Communications with the Ombuds Committee are informal and off-the-record. The Ombuds Committee is governed by the following standards: (1) Confidentiality; (2) Impartiality: (3) Informality; and (4) Independence.\n\n\n\nClass Recordings will be posted on Youtube and notifed via D2L and Piazza. Students may not modify content or re-use content for any purpose other than personal educational reasons. All recordings are subject to government and university regulations. Therefore, students using recordings in a manner inconsistent with UArizona values and educational policies (Code of Academic Integrity and the Student Code of Conduct) are also subject to civil action.\n\n\n\nThis course aims to introduce students to the principles and techniques of data science, enabling them to make effective decisions in their computer science careers. During this course, the student will, - If you are reading this, email me one of your favorite memes to count towards participation points. Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation. - Apply data analysis and visualization techniques to derive insights from diverse datasets. - Gain familiarity with machine learning algorithms and their practical applications. - Develop proficiency in using data science tools and programming languages. - Engage in critical thinking and problem-solving through project-based assignments. - Explore the ethical considerations associated with data-driven decision-making. - Stay informed about current trends and developments in data science and artificial intelligence.\n\n\n\nA student who successfully completes this course will be able to: - Explain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.) - Convert a raw data source into a version appropriate for downstream analysis using Python. - Write appropriate visualizations for different sources and types of data. - Explain why we seek to build machine learning models that generalize rather than memorize their input. - Explain the different uses for training, validation, and testing datasets - Select the appropriate evaluation measure for the dataset and task being solved - Articulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem - Demonstrate awareness of bias and ethics in data science.\n\n\n\nThe UA’s policy concerning Class Attendance, Participation, and Administrative Drops is available at https://catalog.arizona.edu/policy/class-attendance-and-participation.\nThe UA policy regarding absences for any sincerely held religious belief, observance or practice will be accommodated where reasonable: http://policy.arizona.edu/human-resources/religious-accommodation-policy.\nAbsences pre-approved by the UA Dean of Students (or dean’s designee) will be honored. See https://deanofstudents.arizona.edu/policies/attendance-policies-and-practices\nParticipating in the course, watching the lectures, and other course events, such as discussions, are vital to the learning process. If you are marked absent for multiple weeks or have missed multiple deadlines, you are encouraged to see an advisor if it is after the drop period (when a W will not appear on the transcript). Advisors will provide options and alternatives as appropriate for individual student situations.\nTo request a disability-related accommodation to this attendance policy, please contact the Disability Resource Center at (520) 621-3268 or drc-info@email.arizona.edu. If you are experiencing unexpected barriers to your success in your courses, the Dean of Students Office is a central support resource for all students and may be helpful. The Dean of Students Office is located in the Robert L. Nugent Building, room 100, or call 520-621-7057.\n\n\n\nIf you feel sick or may have been in contact with someone who is infectious, stay home. Except for seeking medical care, avoid contact with others and do not travel. Notify your instructor if you will miss up to one week of the course, meetings, weekly check-in, and/or assignment deadlines. If you must miss the equivalent of more than one week of class and have an emergency, the Dean of Students is the proper office to contact (DOS-deanofstudents@email.arizona.edu). The Dean of Students considers the following as qualified emergencies: the birth of a child, mental health hospitalization, domestic violence matter, house fire, hospitalization for physical health (concussion/emergency surgery/coma/COVID-19 complications/ICU), death of immediate family, Title IX matters, etc. Please understand that there is no guarantee of an extension when you are absent from class and/or miss a deadline.\nStatement on compliance with COVID-19 mitigation guidelines: As we enter the semester, our health and safety remain the university’s highest priority. To protect the health of everyone in this class, students are required to follow the university guidelines on COVID-19 mitigation. Please visit www.covid19.arizona.edu. Makeup Policy for Students Who Register Late\nStudents who enroll late can make up for missed assignment if any. They are required to get in touch with the instructor, complete the outstanding homework within 7 days of enrollment, and take an additional quiz/question. The grade for the missed assignment will be determined by combining the score of the homework and the quiz. There will be no credit penalty for enrolling late.\n\n\n\n\nAll course communications, including discussions and Q&A, will occur via the Piazza platform. Students are encouraged to use Piazza for asking questions and engaging in course-related discussions.\nExceptions to Piazza communication can be made via email for specific cases that require individual attention or privacy.\nImportant announcements and updates will be posted on the D2L (Desire2Learn) platform.\nAssignments will be distributed and submitted through the D2L platform. If you are reading this, email me with the title of your favorite show, this counts towards participation.\nOffice hours will be conducted remotely using Zoom. Students will have the opportunity to schedule virtual meetings with the instructor or teaching assistants to discuss course-related questions, seek clarification, or receive additional support.\n\n\n\n\nThere will be no required textbook for this course. All required readings will be made on D2L. Optional Advanced Reading: - WJ: Watkins, J., “An Introduction to the Science of Statistics: From Theory to Implementation” (https://www.math.arizona.edu/~jwatkins/statbook.pdf) - MK: Murphy, K. “Machine Learning: A Probabilistic Perspective.” MIT Press, 2012 (accessible online via UA library) - Wasserman, L. “All of Statistics: A Concise Course in Statistical Inference.” Springer, 2004 (accessible online via UA library)\nRequired or Special Materials None Required Extracurricular Activities None\n\n\n\nTo be filled later\nFinal Examination\nThere will be no final examination. The final exam grade would be from the take-home final project. Grading Scale and Policies\nHomeworks: 70% distributed unequally Final Project: 20% Participation: 5 % Weekly Check-Ins: 5%\nGrade Distribution for this Course: A: 90% and above B: 80%-89.99% C: 70%-79.99% D: 60%-69.99% E: 59% and below\nUniversity policy regarding grades and grading systems is available at http://catalog.arizona.edu/policy/grades-and-grading-system\nThe graded homework will be returned before the next homework is due. Request for regrading of homework and midterm exam can only be made within 7 academic days of the grade release. The final project grade will be within 5 days. Missed assignments result in a grade of zero. Grading delays beyond promised return-by dates will be announced as soon as possible with an explanation for the delay. Students may earn one extra credit point for actively participating in the discussions online. A very active participant is defined as someone who is active on Piazza and uses the platform to answer fellow students’ questions and/or start discussions on the content discussed in class or the reading. Incomplete (I) or Withdrawal (W):\nRequests for incomplete (I) or withdrawal (W) must be made in accordance with University policies, which are available at http://catalog.arizona.edu/policy/grades-and-grading-system#incomplete and http://catalog.arizona.edu/policy/grades-and-grading-system#Withdrawal respectively. Honors Credit\nStudents wishing to contract this course for Honors Credit should e-mail me to set up an appointment to discuss the terms of the contact and to sign the Honors Course Contract Request Form. The form is available at http://www.honors.arizona.edu/honors-contract Scheduled Topics/Activities\nThe scheduled topics below are tentative and may change.\n\n\n\nThe Department of Computer Science is committed to providing and maintaining a supportive educational environment for all. We strive to be welcoming and inclusive, respect privacy and confidentiality, behave respectfully and courteously, and practice intellectual honesty. Disruptive behaviors (such as physical or emotional harassment, dismissive attitudes, and abuse of department resources) will not be tolerated. The complete Code of Conduct is available on our department website. We expect that you will adhere to this code, as well as the UA Student Code of Conduct, while you are a member of this class. Classroom Behavior Policy\nTo foster a positive learning environment, students and instructors have a shared responsibility. We want a safe, welcoming, and inclusive environment where all of us feel comfortable with each other and where we can challenge ourselves to succeed. To that end, our focus is on the tasks at hand and not on extraneous activities.\nStudents are asked to refrain from disruptive or rude conversations with people in Piazza discussions. Students observed engaging in disruptive activity will be asked to cease this behavior. Those who continue may be reported to the Dean of Students. Threatening Behavior Policy\nThe UA Threatening Behavior by Students Policy prohibits threats of physical harm to any member of the University community, including to oneself. See http://policy.arizona.edu/education-and-student-affairs/threatening-behavior-students.\n\n\n\nIn this course, readings could potentially contain content that may be sensitive or even depressive in nature. While efforts are made to ensure the appropriateness of course materials, there is a possibility that some content may touch upon topics that could be emotionally challenging. Students are not automatically excused from interacting with such materials, but they are encouraged to speak with the instructor to voice concerns and provide feedback.\n\n\n\nAt the University of Arizona, we strive to make learning experiences as accessible as possible. If you anticipate or experience barriers based on disability or pregnancy, please contact the Disability Resource Center (520-621-3268, https://drc.arizona.edu/) to establish reasonable accommodations. Code of Academic Integrity\nStudents are encouraged to share intellectual views and discuss freely the principles and applications of course materials. However, graded work/exercises must be the product of independent effort unless otherwise instructed. Students are expected to adhere to the UA Code of Academic Integrity as described in the UA General Catalog. See https://deanofstudents.arizona.edu/student-rights-responsibilities/academic-integrity .\nUploading material from this course to a website other than D2L (or the class piazza) is strictly prohibited. It will be considered a violation of the course policy and a violation of the code of academic integrity. Obtaining material associated with this course (or previous offerings of this course) on a site other than D2L (or the class piazza), such as Chegg, Course Hero, etc. or accessing these sites during a quiz or exam, is a violation of the code of academic integrityAny student determined to have uploaded or accessed material in an unauthorized manner will be reported to the Dean of Students for a Code of Academic Integrity violation, with a recommended sanction of a failing grade in the course (faculty can replace this sanction with whatever sanction they plan to use for their course). Using generative AI models like ChatGPT to craft complete responses or homework is discouraged. This is because, then, you are less likely to meet the course objectives or apply skills earned from the course in real life.\nSelling class notes and/or other course materials to other students or to a third party for resale is not permitted without the instructor’s express written consent. Violations to this and other course rules are subject to the Code of Academic Integrity and may result in course sanctions. Additionally, students who use D2L or UA e-mail to sell or buy these copyrighted materials are subject to Code of Conduct Violations for misuse of student e-mail addresses. This conduct may also constitute copyright infringement.\n\n\n\nThe University of Arizona is committed to creating and maintaining an environment free of discrimination. In support of this commitment, the University prohibits discrimination, including harassment and retaliation, based on a protected classification, including race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, gender identity, or genetic information. For more information, including how to report a concern, please see http://policy.arizona.edu/human-resources/nondiscrimination-and-anti-harassment-policy Recommended additional language: Our classroom is a place where everyone is encouraged to express well-formed opinions and their reasons for those opinions. We also want to create a tolerant and open environment where such opinions can be expressed without resorting to bullying or discrimination of others. Additional Resources for Students\nUA Academic policies and procedures are available at http://catalog.arizona.edu/policies. Visit the UArizona COVID-19 page for regular updates.\nCampus Health\nhttp://www.health.arizona.edu/ Campus Health provides quality medical and mental health care services through virtual and in-person care. Voluntary, free, and convenient COVID-19 testing is available for students on Main Campus. COVID-19 vaccine is available for all students at Campus Health. Phone: 520-621-9202\nCounseling and Psych Services (CAPS)\nhttps://health.arizona.edu/counseling-psych-services CAPS provides mental health care, including short-term counseling services. Phone: 520-621-3334\nThe Dean of Students Office’s Student Assistance Program\nhttps://deanofstudents.arizona.edu/support/student-assistance Student Assistance helps students manage crises, life traumas, and other barriers that impede success. The staff addresses the needs of students who experience issues related to social adjustment, academic challenges, psychological health, physical health, victimization, and relationship issues, through a variety of interventions, referrals, and follow-up services. Email: DOS-deanofstudents@email.arizona.edu Phone: 520-621-7057\nSurvivor Advocacy Program\nhttps://survivoradvocacy.arizona.edu/ The Survivor Advocacy Program provides confidential support and advocacy services to student survivors of sexual and gender-based violence. The Program can also advise students about relevant non-UA resources available within the local community for support. Email: survivoradvocacy@email.arizona.edu Phone: 520-621-5767\nCampus Pantry\nAny student who has difficulty affording groceries or accessing sufficient food to eat every day, or who lacks a safe and stable place to live and believes this may affect their performance in the course, is urged to contact the Dean of Students for support. In addition, the University of Arizona Campus Pantry is open for students to receive supplemental groceries at no cost. Please see their website at: campuspantry.arizona.edu for open times. Furthermore, please notify me if you are comfortable in doing so. This will enable me to provide any resources that I may possess.\nPronouns and Preferred Names\nThis course affirms people of all gender expressions and gender identities. If you prefer to be called a different name than what is on the class roster, please let me know. Feel free to correct instructors on your pronoun. If you have any questions or concerns, please do not hesitate to contact me directly in class or via email (instructor email). If you wish to change your preferred name or pronoun in the UAccess system, please use the following guidelines: Preferred name: University of Arizona students may choose to identify themselves within the University community using a preferred first name that differs from their official/legal name. A student’s preferred name will appear instead of the person’s official/legal first name in select University-related systems and documents, provided that the name is not being used for the purpose of misrepresentation. Students are able to update their preferred names in UAccess. Pronouns: Students may designate pronouns they use to identify themselves. Instructors and staff are encouraged to use pronouns for people that they use for themselves as a sign of respect and inclusion. Students are able to update and edit their pronouns in UAccess. More information on updating your preferred name and pronouns is available on the Office of the Registrar site at https://www.registrar.arizona.edu/.\nConfidentiality of Student Records\nYour class records will be held confidential as per the following: http://www.registrar.arizona.edu/personal-information/family-educational-rights-and-privacy-act-1974-ferpa?topic=ferpa Land Acknowledgement Statement\nWe respectfully acknowledge the University of Arizona is on the land and territories of Indigenous peoples. Today, Arizona is home to 22 federally recognized tribes, with Tucson being home to the O’odham and the Yaqui. Committed to diversity and inclusion, the University strives to build sustainable relationships with sovereign Native Nations and Indigenous communities through education offerings, partnerships, and community service.\nSubject to Change Statement\nInformation contained in the course syllabus, other than the grade and absence policy, may be subject to change with advance notice, as deemed appropriate by the instructor."
  },
  {
    "objectID": "Syllabus/Syllabus.html#description-of-course",
    "href": "Syllabus/Syllabus.html#description-of-course",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "The course introduces students to the principles of data science, which are essential for computer scientists to make effective decisions in their professional careers. In today’s data-driven world, a wide range of computer science sub-disciplines heavily rely on data collection, analysis, and interpretation. With the pervasive presence of artificial intelligence (AI) in our lives, understanding the basics of how these systems work is becoming increasingly important. Additionally, it covers the basics of artificial intelligence (AI) systems and examines practical use cases, current news, and ethical considerations through readings and discussions."
  },
  {
    "objectID": "Syllabus/Syllabus.html#catalog-description",
    "href": "Syllabus/Syllabus.html#catalog-description",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "The course introduces students to principles of data science that are necessary for computer scientists to make effective decisions in their professional careers. A number of computer science sub-disciplines now rely on data collection and analysis. For example, computer systems are now complicated enough that comparing the execution performance of two different programs becomes a statistical estimation problem rather than a deterministic computation. This course teaches students the basic principles of how to properly collect and process data sources in order to derive appropriate conclusions from them. The course has three main components: data analysis, machine learning, and a project where students apply the concepts discussed in class to a substantial open-ended problem."
  },
  {
    "objectID": "Syllabus/Syllabus.html#course-prerequisites",
    "href": "Syllabus/Syllabus.html#course-prerequisites",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "Major: COSCBA or COSCBS. Completion of CSC 210 and CSC 244. If you do not meet the enrollment requirements, contact the lecturer. Instructor and Contact Information\nInstructor: Enfa George Gould-Simpson 710 enfageorge@arizona.edu\nTeaching Assistant: Bennett Brixen bennettbrixen@arizona.edu\nOffice Hours Monday : 11 am to 12 pm - Bennett, TA Tuesday : 1 pm to 2 pm - Bennett, TA Wednesday : 2 pm to 3 pm - Enfa, Instructor Thursday 4 pm to 5 pm - Bennett, TA Friday : 3 pm to 4 pm - Enfa, Instructor\nBennett’s virtual office: https://arizona.zoom.us/j/8363357057 Enfa’s virtual office: https://arizona.zoom.us/j/87083172857"
  },
  {
    "objectID": "Syllabus/Syllabus.html#course-format-and-teaching-methods",
    "href": "Syllabus/Syllabus.html#course-format-and-teaching-methods",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "Asynchronous Online Lectures: Pre-recorded lectures will be provided for students to access at their convenience.\nDiscussions on Piazza: Piazza will be used for students to ask questions, engage in discussions, and seek clarifications from the instructor and teaching assistant.\nProject-Based Homeworks: Assignments will be designed to apply data science techniques to real-world problems.\nWeekly Check-ins: Regular individual check-ins via a D2L Quiz. This will involve questions/discussions related to the reading, a platform to address concerns, and provide live feedback on the course Obtaining Help\n\nAcademic advising: If you have questions about your academic progress this semester, or your chosen degree program, consider contacting your department’s academic advisor(s). Your academic advisor and the Advising Resource Center can guide you toward university resources to help you succeed. Computer Science major students are encouraged to visit https://www.cs.arizona.edu/undergraduate/advising for advisor contact information. CS Help Desk: The Computer Science IT team can help students with department technology issues including logging into/resetting your Lectura account, printing in the 930 lab, etc. You can submit a ticket for help by visiting the Computer Science Lab Helpdesk (note, requires UA login).\nLife challenges: If you are experiencing unexpected barriers to your success in your courses, please note the Dean of Students Office is a central support resource for all students and may be helpful. The Dean of Students Office can be reached at 520-621-2057 or DOS-deanofstudents@email.arizona.edu. Physical and mental-health challenges: If you are facing physical or mental health challenges this semester, please note that Campus Health provides quality medical and mental health care. For medical appointments, call (520-621-9202. For After Hours care, call (520) 570-7898. For the Counseling & Psych Services (CAPS) 24/7 hotline, call (520) 621-3334.\nUA Ombuds: The UA Ombuds Office (https://ombuds.arizona.edu/) helps with a wide variety of issues, concerns, questions, conflicts, and challenges. The primary mission of the Ombuds Program is to assist individuals in resolving conflict, facilitating communication, and assisting the University by surfacing issues and providing feedback on emerging or systemic concerns. Communications with the Ombuds Committee are informal and off-the-record. The Ombuds Committee is governed by the following standards: (1) Confidentiality; (2) Impartiality: (3) Informality; and (4) Independence."
  },
  {
    "objectID": "Syllabus/Syllabus.html#class-recordings",
    "href": "Syllabus/Syllabus.html#class-recordings",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "Class Recordings will be posted on Youtube and notifed via D2L and Piazza. Students may not modify content or re-use content for any purpose other than personal educational reasons. All recordings are subject to government and university regulations. Therefore, students using recordings in a manner inconsistent with UArizona values and educational policies (Code of Academic Integrity and the Student Code of Conduct) are also subject to civil action."
  },
  {
    "objectID": "Syllabus/Syllabus.html#course-objectives",
    "href": "Syllabus/Syllabus.html#course-objectives",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "This course aims to introduce students to the principles and techniques of data science, enabling them to make effective decisions in their computer science careers. During this course, the student will, - If you are reading this, email me one of your favorite memes to count towards participation points. Understand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation. - Apply data analysis and visualization techniques to derive insights from diverse datasets. - Gain familiarity with machine learning algorithms and their practical applications. - Develop proficiency in using data science tools and programming languages. - Engage in critical thinking and problem-solving through project-based assignments. - Explore the ethical considerations associated with data-driven decision-making. - Stay informed about current trends and developments in data science and artificial intelligence."
  },
  {
    "objectID": "Syllabus/Syllabus.html#expected-learning-outcomes",
    "href": "Syllabus/Syllabus.html#expected-learning-outcomes",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "A student who successfully completes this course will be able to: - Explain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.) - Convert a raw data source into a version appropriate for downstream analysis using Python. - Write appropriate visualizations for different sources and types of data. - Explain why we seek to build machine learning models that generalize rather than memorize their input. - Explain the different uses for training, validation, and testing datasets - Select the appropriate evaluation measure for the dataset and task being solved - Articulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem - Demonstrate awareness of bias and ethics in data science."
  },
  {
    "objectID": "Syllabus/Syllabus.html#absence-and-class-participation-policy",
    "href": "Syllabus/Syllabus.html#absence-and-class-participation-policy",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "The UA’s policy concerning Class Attendance, Participation, and Administrative Drops is available at https://catalog.arizona.edu/policy/class-attendance-and-participation.\nThe UA policy regarding absences for any sincerely held religious belief, observance or practice will be accommodated where reasonable: http://policy.arizona.edu/human-resources/religious-accommodation-policy.\nAbsences pre-approved by the UA Dean of Students (or dean’s designee) will be honored. See https://deanofstudents.arizona.edu/policies/attendance-policies-and-practices\nParticipating in the course, watching the lectures, and other course events, such as discussions, are vital to the learning process. If you are marked absent for multiple weeks or have missed multiple deadlines, you are encouraged to see an advisor if it is after the drop period (when a W will not appear on the transcript). Advisors will provide options and alternatives as appropriate for individual student situations.\nTo request a disability-related accommodation to this attendance policy, please contact the Disability Resource Center at (520) 621-3268 or drc-info@email.arizona.edu. If you are experiencing unexpected barriers to your success in your courses, the Dean of Students Office is a central support resource for all students and may be helpful. The Dean of Students Office is located in the Robert L. Nugent Building, room 100, or call 520-621-7057."
  },
  {
    "objectID": "Syllabus/Syllabus.html#illnesses-and-emergencies",
    "href": "Syllabus/Syllabus.html#illnesses-and-emergencies",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "If you feel sick or may have been in contact with someone who is infectious, stay home. Except for seeking medical care, avoid contact with others and do not travel. Notify your instructor if you will miss up to one week of the course, meetings, weekly check-in, and/or assignment deadlines. If you must miss the equivalent of more than one week of class and have an emergency, the Dean of Students is the proper office to contact (DOS-deanofstudents@email.arizona.edu). The Dean of Students considers the following as qualified emergencies: the birth of a child, mental health hospitalization, domestic violence matter, house fire, hospitalization for physical health (concussion/emergency surgery/coma/COVID-19 complications/ICU), death of immediate family, Title IX matters, etc. Please understand that there is no guarantee of an extension when you are absent from class and/or miss a deadline.\nStatement on compliance with COVID-19 mitigation guidelines: As we enter the semester, our health and safety remain the university’s highest priority. To protect the health of everyone in this class, students are required to follow the university guidelines on COVID-19 mitigation. Please visit www.covid19.arizona.edu. Makeup Policy for Students Who Register Late\nStudents who enroll late can make up for missed assignment if any. They are required to get in touch with the instructor, complete the outstanding homework within 7 days of enrollment, and take an additional quiz/question. The grade for the missed assignment will be determined by combining the score of the homework and the quiz. There will be no credit penalty for enrolling late."
  },
  {
    "objectID": "Syllabus/Syllabus.html#course-communications",
    "href": "Syllabus/Syllabus.html#course-communications",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "All course communications, including discussions and Q&A, will occur via the Piazza platform. Students are encouraged to use Piazza for asking questions and engaging in course-related discussions.\nExceptions to Piazza communication can be made via email for specific cases that require individual attention or privacy.\nImportant announcements and updates will be posted on the D2L (Desire2Learn) platform.\nAssignments will be distributed and submitted through the D2L platform. If you are reading this, email me with the title of your favorite show, this counts towards participation.\nOffice hours will be conducted remotely using Zoom. Students will have the opportunity to schedule virtual meetings with the instructor or teaching assistants to discuss course-related questions, seek clarification, or receive additional support."
  },
  {
    "objectID": "Syllabus/Syllabus.html#required-texts-or-readings",
    "href": "Syllabus/Syllabus.html#required-texts-or-readings",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "There will be no required textbook for this course. All required readings will be made on D2L. Optional Advanced Reading: - WJ: Watkins, J., “An Introduction to the Science of Statistics: From Theory to Implementation” (https://www.math.arizona.edu/~jwatkins/statbook.pdf) - MK: Murphy, K. “Machine Learning: A Probabilistic Perspective.” MIT Press, 2012 (accessible online via UA library) - Wasserman, L. “All of Statistics: A Concise Course in Statistical Inference.” Springer, 2004 (accessible online via UA library)\nRequired or Special Materials None Required Extracurricular Activities None"
  },
  {
    "objectID": "Syllabus/Syllabus.html#assignments-and-examinations-scheduledue-dates",
    "href": "Syllabus/Syllabus.html#assignments-and-examinations-scheduledue-dates",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "To be filled later\nFinal Examination\nThere will be no final examination. The final exam grade would be from the take-home final project. Grading Scale and Policies\nHomeworks: 70% distributed unequally Final Project: 20% Participation: 5 % Weekly Check-Ins: 5%\nGrade Distribution for this Course: A: 90% and above B: 80%-89.99% C: 70%-79.99% D: 60%-69.99% E: 59% and below\nUniversity policy regarding grades and grading systems is available at http://catalog.arizona.edu/policy/grades-and-grading-system\nThe graded homework will be returned before the next homework is due. Request for regrading of homework and midterm exam can only be made within 7 academic days of the grade release. The final project grade will be within 5 days. Missed assignments result in a grade of zero. Grading delays beyond promised return-by dates will be announced as soon as possible with an explanation for the delay. Students may earn one extra credit point for actively participating in the discussions online. A very active participant is defined as someone who is active on Piazza and uses the platform to answer fellow students’ questions and/or start discussions on the content discussed in class or the reading. Incomplete (I) or Withdrawal (W):\nRequests for incomplete (I) or withdrawal (W) must be made in accordance with University policies, which are available at http://catalog.arizona.edu/policy/grades-and-grading-system#incomplete and http://catalog.arizona.edu/policy/grades-and-grading-system#Withdrawal respectively. Honors Credit\nStudents wishing to contract this course for Honors Credit should e-mail me to set up an appointment to discuss the terms of the contact and to sign the Honors Course Contract Request Form. The form is available at http://www.honors.arizona.edu/honors-contract Scheduled Topics/Activities\nThe scheduled topics below are tentative and may change."
  },
  {
    "objectID": "Syllabus/Syllabus.html#department-of-computer-science-code-of-conduct",
    "href": "Syllabus/Syllabus.html#department-of-computer-science-code-of-conduct",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "The Department of Computer Science is committed to providing and maintaining a supportive educational environment for all. We strive to be welcoming and inclusive, respect privacy and confidentiality, behave respectfully and courteously, and practice intellectual honesty. Disruptive behaviors (such as physical or emotional harassment, dismissive attitudes, and abuse of department resources) will not be tolerated. The complete Code of Conduct is available on our department website. We expect that you will adhere to this code, as well as the UA Student Code of Conduct, while you are a member of this class. Classroom Behavior Policy\nTo foster a positive learning environment, students and instructors have a shared responsibility. We want a safe, welcoming, and inclusive environment where all of us feel comfortable with each other and where we can challenge ourselves to succeed. To that end, our focus is on the tasks at hand and not on extraneous activities.\nStudents are asked to refrain from disruptive or rude conversations with people in Piazza discussions. Students observed engaging in disruptive activity will be asked to cease this behavior. Those who continue may be reported to the Dean of Students. Threatening Behavior Policy\nThe UA Threatening Behavior by Students Policy prohibits threats of physical harm to any member of the University community, including to oneself. See http://policy.arizona.edu/education-and-student-affairs/threatening-behavior-students."
  },
  {
    "objectID": "Syllabus/Syllabus.html#notification-of-objectionable-materials",
    "href": "Syllabus/Syllabus.html#notification-of-objectionable-materials",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "In this course, readings could potentially contain content that may be sensitive or even depressive in nature. While efforts are made to ensure the appropriateness of course materials, there is a possibility that some content may touch upon topics that could be emotionally challenging. Students are not automatically excused from interacting with such materials, but they are encouraged to speak with the instructor to voice concerns and provide feedback."
  },
  {
    "objectID": "Syllabus/Syllabus.html#accessibility-and-accommodations",
    "href": "Syllabus/Syllabus.html#accessibility-and-accommodations",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "At the University of Arizona, we strive to make learning experiences as accessible as possible. If you anticipate or experience barriers based on disability or pregnancy, please contact the Disability Resource Center (520-621-3268, https://drc.arizona.edu/) to establish reasonable accommodations. Code of Academic Integrity\nStudents are encouraged to share intellectual views and discuss freely the principles and applications of course materials. However, graded work/exercises must be the product of independent effort unless otherwise instructed. Students are expected to adhere to the UA Code of Academic Integrity as described in the UA General Catalog. See https://deanofstudents.arizona.edu/student-rights-responsibilities/academic-integrity .\nUploading material from this course to a website other than D2L (or the class piazza) is strictly prohibited. It will be considered a violation of the course policy and a violation of the code of academic integrity. Obtaining material associated with this course (or previous offerings of this course) on a site other than D2L (or the class piazza), such as Chegg, Course Hero, etc. or accessing these sites during a quiz or exam, is a violation of the code of academic integrityAny student determined to have uploaded or accessed material in an unauthorized manner will be reported to the Dean of Students for a Code of Academic Integrity violation, with a recommended sanction of a failing grade in the course (faculty can replace this sanction with whatever sanction they plan to use for their course). Using generative AI models like ChatGPT to craft complete responses or homework is discouraged. This is because, then, you are less likely to meet the course objectives or apply skills earned from the course in real life.\nSelling class notes and/or other course materials to other students or to a third party for resale is not permitted without the instructor’s express written consent. Violations to this and other course rules are subject to the Code of Academic Integrity and may result in course sanctions. Additionally, students who use D2L or UA e-mail to sell or buy these copyrighted materials are subject to Code of Conduct Violations for misuse of student e-mail addresses. This conduct may also constitute copyright infringement."
  },
  {
    "objectID": "Syllabus/Syllabus.html#nondiscrimination-and-anti-harassment-policy",
    "href": "Syllabus/Syllabus.html#nondiscrimination-and-anti-harassment-policy",
    "title": "CSC 380: Principles of Data Science",
    "section": "",
    "text": "The University of Arizona is committed to creating and maintaining an environment free of discrimination. In support of this commitment, the University prohibits discrimination, including harassment and retaliation, based on a protected classification, including race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, gender identity, or genetic information. For more information, including how to report a concern, please see http://policy.arizona.edu/human-resources/nondiscrimination-and-anti-harassment-policy Recommended additional language: Our classroom is a place where everyone is encouraged to express well-formed opinions and their reasons for those opinions. We also want to create a tolerant and open environment where such opinions can be expressed without resorting to bullying or discrimination of others. Additional Resources for Students\nUA Academic policies and procedures are available at http://catalog.arizona.edu/policies. Visit the UArizona COVID-19 page for regular updates.\nCampus Health\nhttp://www.health.arizona.edu/ Campus Health provides quality medical and mental health care services through virtual and in-person care. Voluntary, free, and convenient COVID-19 testing is available for students on Main Campus. COVID-19 vaccine is available for all students at Campus Health. Phone: 520-621-9202\nCounseling and Psych Services (CAPS)\nhttps://health.arizona.edu/counseling-psych-services CAPS provides mental health care, including short-term counseling services. Phone: 520-621-3334\nThe Dean of Students Office’s Student Assistance Program\nhttps://deanofstudents.arizona.edu/support/student-assistance Student Assistance helps students manage crises, life traumas, and other barriers that impede success. The staff addresses the needs of students who experience issues related to social adjustment, academic challenges, psychological health, physical health, victimization, and relationship issues, through a variety of interventions, referrals, and follow-up services. Email: DOS-deanofstudents@email.arizona.edu Phone: 520-621-7057\nSurvivor Advocacy Program\nhttps://survivoradvocacy.arizona.edu/ The Survivor Advocacy Program provides confidential support and advocacy services to student survivors of sexual and gender-based violence. The Program can also advise students about relevant non-UA resources available within the local community for support. Email: survivoradvocacy@email.arizona.edu Phone: 520-621-5767\nCampus Pantry\nAny student who has difficulty affording groceries or accessing sufficient food to eat every day, or who lacks a safe and stable place to live and believes this may affect their performance in the course, is urged to contact the Dean of Students for support. In addition, the University of Arizona Campus Pantry is open for students to receive supplemental groceries at no cost. Please see their website at: campuspantry.arizona.edu for open times. Furthermore, please notify me if you are comfortable in doing so. This will enable me to provide any resources that I may possess.\nPronouns and Preferred Names\nThis course affirms people of all gender expressions and gender identities. If you prefer to be called a different name than what is on the class roster, please let me know. Feel free to correct instructors on your pronoun. If you have any questions or concerns, please do not hesitate to contact me directly in class or via email (instructor email). If you wish to change your preferred name or pronoun in the UAccess system, please use the following guidelines: Preferred name: University of Arizona students may choose to identify themselves within the University community using a preferred first name that differs from their official/legal name. A student’s preferred name will appear instead of the person’s official/legal first name in select University-related systems and documents, provided that the name is not being used for the purpose of misrepresentation. Students are able to update their preferred names in UAccess. Pronouns: Students may designate pronouns they use to identify themselves. Instructors and staff are encouraged to use pronouns for people that they use for themselves as a sign of respect and inclusion. Students are able to update and edit their pronouns in UAccess. More information on updating your preferred name and pronouns is available on the Office of the Registrar site at https://www.registrar.arizona.edu/.\nConfidentiality of Student Records\nYour class records will be held confidential as per the following: http://www.registrar.arizona.edu/personal-information/family-educational-rights-and-privacy-act-1974-ferpa?topic=ferpa Land Acknowledgement Statement\nWe respectfully acknowledge the University of Arizona is on the land and territories of Indigenous peoples. Today, Arizona is home to 22 federally recognized tribes, with Tucson being home to the O’odham and the Yaqui. Committed to diversity and inclusion, the University strives to build sustainable relationships with sovereign Native Nations and Indigenous communities through education offerings, partnerships, and community service.\nSubject to Change Statement\nInformation contained in the course syllabus, other than the grade and absence policy, may be subject to change with advance notice, as deemed appropriate by the instructor."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "Participation_Activities/home.html",
    "href": "Participation_Activities/home.html",
    "title": "Participation Activities",
    "section": "",
    "text": "Github Setup\n\n\n\n\n\nEaster Egg in Syllabus -1 : Message to send favorite movie hidden in syllabus\naster Egg in Syllabus -1 : Message to send favorite meme hidden in syllabus\nJupyter Notebook Setup Marked as Homework 0:\n\nGithub Setup"
  },
  {
    "objectID": "Participation_Activities/home.html#active-participation-activities",
    "href": "Participation_Activities/home.html#active-participation-activities",
    "title": "Participation Activities",
    "section": "",
    "text": "Github Setup"
  },
  {
    "objectID": "Participation_Activities/home.html#past",
    "href": "Participation_Activities/home.html#past",
    "title": "Participation Activities",
    "section": "",
    "text": "Easter Egg in Syllabus -1 : Message to send favorite movie hidden in syllabus\naster Egg in Syllabus -1 : Message to send favorite meme hidden in syllabus\nJupyter Notebook Setup Marked as Homework 0:\n\nGithub Setup"
  },
  {
    "objectID": "Participation_Activities/github_setup.html",
    "href": "Participation_Activities/github_setup.html",
    "title": "Github Setup :",
    "section": "",
    "text": "Homework 3 will highly likely be a github classroom submission. This activity is to make sure we ( students, TA as well as the instructor ) won’t have any hiccups.\n\n\n\nPS : DO NOT do anything with the Feedback Pull request that was autogenerated.\n\nVisit : https://classroom.github.com/a/1ckY6YTk\nYou will be asked to accept the homework\nInstruction on screen will guide you to clone a repo ; The repo name is expected to be github-setup-username.\nClone the repo to your local ie computer.\nAdd your Name as heading to the Readme.\nCommit and Push the change\nMake sure everything is in place.\n\nExample : When I did the activity, I got the following screen.\n\nYou are done!"
  },
  {
    "objectID": "Participation_Activities/github_setup.html#objective",
    "href": "Participation_Activities/github_setup.html#objective",
    "title": "Github Setup :",
    "section": "",
    "text": "Homework 3 will highly likely be a github classroom submission. This activity is to make sure we ( students, TA as well as the instructor ) won’t have any hiccups."
  },
  {
    "objectID": "Participation_Activities/github_setup.html#instructions",
    "href": "Participation_Activities/github_setup.html#instructions",
    "title": "Github Setup :",
    "section": "",
    "text": "PS : DO NOT do anything with the Feedback Pull request that was autogenerated.\n\nVisit : https://classroom.github.com/a/1ckY6YTk\nYou will be asked to accept the homework\nInstruction on screen will guide you to clone a repo ; The repo name is expected to be github-setup-username.\nClone the repo to your local ie computer.\nAdd your Name as heading to the Readme.\nCommit and Push the change\nMake sure everything is in place.\n\nExample : When I did the activity, I got the following screen.\n\nYou are done!"
  },
  {
    "objectID": "Ethics/Week_4.html",
    "href": "Ethics/Week_4.html",
    "title": "Week 4 : AI for Mental Health Support",
    "section": "",
    "text": "Trigger Warning: Mention of Mental Health Issues and Suicide.\n\nEating Disorder Helpline Fires Staff, Transitions to Chatbot After Unionization\nEating Disorder Helpline Disables Chatbot for ‘Harmful’ Responses After Firing Human Staff\n‘He Would Still Be Here’: Man Dies by Suicide After Talking with AI Chatbot, Widow Says\n\n\n\n\n\nAs always, there is no right or wrong answers here. This is an invitation to share your opinion.\n\nShould there be legal and regulatory frameworks in place to monitor and assess the ethical implications of AI chatbot interventions in mental health support, similar to the regulations imposed on human healthcare providers? Any suggestions?\nShould we be limiting to what extent AI chatbots should be allowed to simulate human emotions and empathy, considering the potential ethical implications of deceiving users into believing they are interacting with a human?\nIf an AI “friend” could effectively simulate human emotions, generate a face, voice, and create videos,all things it can already do to an extend, would you be willing to accept and form a meaningful connection with it in a similar way to how people engage in long-distance relationships? Considering this, do you believe your perception and emotional attachment would differ between an AI companion and a friend in a long-distance relationship?\n\n[Optional] (d) Read the synopsis of the Movie Her, if you haven’t watched the movie already. Did your stance on Question c change? If yes, Why?\nQuestions prepared with aid of ChatGPT"
  },
  {
    "objectID": "Ethics/Week_4.html#reading",
    "href": "Ethics/Week_4.html#reading",
    "title": "Week 4 : AI for Mental Health Support",
    "section": "",
    "text": "Trigger Warning: Mention of Mental Health Issues and Suicide.\n\nEating Disorder Helpline Fires Staff, Transitions to Chatbot After Unionization\nEating Disorder Helpline Disables Chatbot for ‘Harmful’ Responses After Firing Human Staff\n‘He Would Still Be Here’: Man Dies by Suicide After Talking with AI Chatbot, Widow Says"
  },
  {
    "objectID": "Ethics/Week_4.html#question-for-discussion",
    "href": "Ethics/Week_4.html#question-for-discussion",
    "title": "Week 4 : AI for Mental Health Support",
    "section": "",
    "text": "As always, there is no right or wrong answers here. This is an invitation to share your opinion.\n\nShould there be legal and regulatory frameworks in place to monitor and assess the ethical implications of AI chatbot interventions in mental health support, similar to the regulations imposed on human healthcare providers? Any suggestions?\nShould we be limiting to what extent AI chatbots should be allowed to simulate human emotions and empathy, considering the potential ethical implications of deceiving users into believing they are interacting with a human?\nIf an AI “friend” could effectively simulate human emotions, generate a face, voice, and create videos,all things it can already do to an extend, would you be willing to accept and form a meaningful connection with it in a similar way to how people engage in long-distance relationships? Considering this, do you believe your perception and emotional attachment would differ between an AI companion and a friend in a long-distance relationship?\n\n[Optional] (d) Read the synopsis of the Movie Her, if you haven’t watched the movie already. Did your stance on Question c change? If yes, Why?\nQuestions prepared with aid of ChatGPT"
  },
  {
    "objectID": "Ethics/Week_7.html",
    "href": "Ethics/Week_7.html",
    "title": "Week 7 : AI for Recommendations",
    "section": "",
    "text": "How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did\n\n\n\n\n\n\nWhat ethical concerns arise when companies use consumer data, such as purchasing habits, to make sensitive inferences about individuals, like Target did with predicting a teen girl’s pregnancy?\nShould companies be allowed to gather and analyze personal data without explicit consent to infer potentially private and sensitive information about their customers?\nHow can companies balance the benefits of personalized marketing and targeted advertising with the ethical obligations to respect customer privacy and autonomy?\n\nElaborate your answers"
  },
  {
    "objectID": "Ethics/Week_7.html#reading",
    "href": "Ethics/Week_7.html#reading",
    "title": "Week 7 : AI for Recommendations",
    "section": "",
    "text": "How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did"
  },
  {
    "objectID": "Ethics/Week_7.html#activity",
    "href": "Ethics/Week_7.html#activity",
    "title": "Week 6 : AI in Autonomous Driving",
    "section": "",
    "text": "Moral Machine"
  },
  {
    "objectID": "Ethics/Week_6.html",
    "href": "Ethics/Week_6.html",
    "title": "Week 6 : AI in Autonomous Driving",
    "section": "",
    "text": "How do you teach an autonomous car when to hurt its passengers?\nThe racism of technology - and why driverless cars could be the most dangerous example yet\nThe folly of trolleys: Ethical challenges and autonomous vehicles\n\n\n\n\nMoral Machine\n\n\n\nDid you try the activity listed? If not, give it a try. After that, answer the following question.\nWhat are your thoughts on moral/ethical decisions with respect to autonomous vehicles, especially in the case of who to save when getting into an accident. Do you think your opinion on this matter would be universal? It’s unlikely. In that case, who gets to decide what decision an autonomous vehicle should make?"
  },
  {
    "objectID": "Ethics/Week_6.html#reading",
    "href": "Ethics/Week_6.html#reading",
    "title": "Week 6 : AI in Autonomous Driving",
    "section": "",
    "text": "How do you teach an autonomous car when to hurt its passengers?\nThe racism of technology - and why driverless cars could be the most dangerous example yet\nThe folly of trolleys: Ethical challenges and autonomous vehicles"
  },
  {
    "objectID": "Ethics/Week_6.html#question-for-discussion",
    "href": "Ethics/Week_6.html#question-for-discussion",
    "title": "Week 5 : AI for Recommendations",
    "section": "",
    "text": "TBD"
  },
  {
    "objectID": "Ethics/Week_2.html",
    "href": "Ethics/Week_2.html",
    "title": "Week 2 : AI in Political Content",
    "section": "",
    "text": "Deepfaking it: America’s 2024 election collides with AI boom\n\n\n\n\n\nThe following question is asking for your opinion. There is no right or wrong answers. \n\nHow might the rapid increase in numbers of deepfakes during an election impact the democratic process and the public’s ability to make informed decisions?\nDo you think social media platforms have a responsibility in combating the spread of deepfakes and ensuring the integrity of political discourse? Why/Why not?"
  },
  {
    "objectID": "Ethics/Week_2.html#reading",
    "href": "Ethics/Week_2.html#reading",
    "title": "Week 2 : AI in Political Content",
    "section": "",
    "text": "Deepfaking it: America’s 2024 election collides with AI boom"
  },
  {
    "objectID": "Ethics/Week_2.html#question-for-discussion",
    "href": "Ethics/Week_2.html#question-for-discussion",
    "title": "Week 2 : AI in Political Content",
    "section": "",
    "text": "The following question is asking for your opinion. There is no right or wrong answers. \n\nHow might the rapid increase in numbers of deepfakes during an election impact the democratic process and the public’s ability to make informed decisions?\nDo you think social media platforms have a responsibility in combating the spread of deepfakes and ensuring the integrity of political discourse? Why/Why not?"
  },
  {
    "objectID": "Ethics/Week_3.html",
    "href": "Ethics/Week_3.html",
    "title": "Week 3 : AI in Creative Work",
    "section": "",
    "text": "‘Black Mirror’ Creator Says ChatGPT Wrote a ‘S—‘ Episode When He Tested It: No ‘Real Original Thought’\nTV’s War With the Robots Is Already Here\n\n\n\n\n\nDisclaimer : There are no right or wrong answers below. Write your opinion on the topic.\nIn the article, we see the lines “Because all it’s done is look up all the synopses of ‘Black Mirror’ episodes, and sort of mush them together. Then if you dig a bit more deeply you go, ’Oh, there’s not actually any real original thought here”. We also see that “The chatbot, created by developer OpenAI, is a large language model, meaning it only mimics the large quantities of pre-existing texts it consumes from across the internet.”\n\nIf a chatbot/a large language model like ChatGPT writes a book, who do you think should get the copyright to the story ? What if the book won an award? Who wins the award?  The AI itself? The company that owns the AI? The person who gave prompts to the chatbot to create the book?  Can something like this even be copyrighted? What is your opinion?\nIf the creative content a model creates is it mimicking “the large quantities of pre-existing texts it consumes from across the internet” and often contains “no original thought”, is it plagiarism? Why/Why not?\nDo you think the writer’s worry is warranted?"
  },
  {
    "objectID": "Ethics/Week_3.html#reading",
    "href": "Ethics/Week_3.html#reading",
    "title": "Week 3 : AI in Creative Work",
    "section": "",
    "text": "‘Black Mirror’ Creator Says ChatGPT Wrote a ‘S—‘ Episode When He Tested It: No ‘Real Original Thought’\nTV’s War With the Robots Is Already Here"
  },
  {
    "objectID": "Ethics/Week_3.html#question-for-discussion",
    "href": "Ethics/Week_3.html#question-for-discussion",
    "title": "Week 3 : AI in Creative Work",
    "section": "",
    "text": "Disclaimer : There are no right or wrong answers below. Write your opinion on the topic.\nIn the article, we see the lines “Because all it’s done is look up all the synopses of ‘Black Mirror’ episodes, and sort of mush them together. Then if you dig a bit more deeply you go, ’Oh, there’s not actually any real original thought here”. We also see that “The chatbot, created by developer OpenAI, is a large language model, meaning it only mimics the large quantities of pre-existing texts it consumes from across the internet.”\n\nIf a chatbot/a large language model like ChatGPT writes a book, who do you think should get the copyright to the story ? What if the book won an award? Who wins the award?  The AI itself? The company that owns the AI? The person who gave prompts to the chatbot to create the book?  Can something like this even be copyrighted? What is your opinion?\nIf the creative content a model creates is it mimicking “the large quantities of pre-existing texts it consumes from across the internet” and often contains “no original thought”, is it plagiarism? Why/Why not?\nDo you think the writer’s worry is warranted?"
  },
  {
    "objectID": "Ethics/Week_8.html",
    "href": "Ethics/Week_8.html",
    "title": "Week 7 : AI in Housekeeping",
    "section": "",
    "text": "A Roomba recorded a woman on the toilet. How did screenshots end up on Facebook?\nHere’s What You’re Actually Agreeing To When You Accept a Privacy Policy\n\n\n\n\n\nIs it ethically acceptable for companies to present users with excessively long privacy policies, knowing that most users are unlikely to read and understand them fully?\nWhat ethical responsibilities do companies have in ensuring that their privacy policies are concise, clear, and easily understandable for users, even if it means additional effort and resources?\nShould there be legal regulations or industry standards mandating a maximum length for privacy policies to prevent companies from burdening users with overly complex and lengthy documents?\n\nElaborate."
  },
  {
    "objectID": "Ethics/Week_8.html#reading",
    "href": "Ethics/Week_8.html#reading",
    "title": "Week 7 : AI in Housekeeping",
    "section": "",
    "text": "A Roomba recorded a woman on the toilet. How did screenshots end up on Facebook?\nHere’s What You’re Actually Agreeing To When You Accept a Privacy Policy"
  },
  {
    "objectID": "Ethics/Week_9.html",
    "href": "Ethics/Week_9.html",
    "title": "Week 8 : AI for Facial Recognition",
    "section": "",
    "text": "A Case for Banning Facial Recognition\nClothing designer tricks AI-powered face recognition into thinking you’re an animal\n\n\n\n\n\nIs it ethical for governments and private entities to use facial recognition technology for surveillance without explicit consent from individuals?\nShould facial recognition technology be used in law enforcement to identify and apprehend suspects, considering the potential for misidentification and its impact on innocent individuals?"
  },
  {
    "objectID": "Ethics/Week_9.html#reading",
    "href": "Ethics/Week_9.html#reading",
    "title": "Week 8 : AI for Facial Recognition",
    "section": "",
    "text": "A Case for Banning Facial Recognition\nClothing designer tricks AI-powered face recognition into thinking you’re an animal"
  },
  {
    "objectID": "Homework/1/HW.html",
    "href": "Homework/1/HW.html",
    "title": "Homework 1",
    "section": "",
    "text": "Explain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.)\n\n\n\n\n\nQuestions\nHints\nSolutions"
  },
  {
    "objectID": "Homework/1/HW.html#tested-learning-outcome",
    "href": "Homework/1/HW.html#tested-learning-outcome",
    "title": "Homework 1",
    "section": "",
    "text": "Explain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.)"
  },
  {
    "objectID": "Homework/1/HW.html#attachments",
    "href": "Homework/1/HW.html#attachments",
    "title": "Homework 1",
    "section": "",
    "text": "Questions\nHints\nSolutions"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw07/hw07.html",
    "href": "Homework/3/Unreleased/archive/hw07/hw07.html",
    "title": "CS380 - Homework 7 : Prostate Cancer Regression (7 points)",
    "section": "",
    "text": "This homework will familiarize you with linear regression. You will be using the Prostate Cancer Dataset from a study by Stamey et al. (1989). The study aims to predict prostate-specific antigen levels from clinical measures in men about to receive a radical prostatectomy.\nThe data contain 8 features: * log cancer volume (lcavol) * log prostate weight (lweight) * age (age) * log amount of benign prostatic hyperplasia (lbph) * seminal vesicle invasion (svi) * log of capsular penetration (lcp) * Gleason score (gleason) * percent of Gleason scores 4 or 5 (pgg45)\nThe data use a fixed Train / Test split, which we will load below.\n#All finalised needed imports\nimport pandas as pd\nimport itertools\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf_train = pd.read_csv('prostate_train.csv')\ndf_train.head()\n\n\n\n\n\n\n\n\nlcavol\nlweight\nage\nlbph\nsvi\nlcp\ngleason\npgg45\nlpsa\n\n\n\n\n0\n-0.579818\n2.769459\n50\n-1.386294\n0\n-1.386294\n6\n0\n-0.430783\n\n\n1\n-0.994252\n3.319626\n58\n-1.386294\n0\n-1.386294\n6\n0\n-0.162519\n\n\n2\n-0.510826\n2.691243\n74\n-1.386294\n0\n-1.386294\n7\n20\n-0.162519\n\n\n3\n-1.203973\n3.282789\n58\n-1.386294\n0\n-1.386294\n6\n0\n-0.162519\n\n\n4\n0.751416\n3.432373\n62\n-1.386294\n0\n-1.386294\n6\n0\n0.371564"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw07/hw07.html#problem-1-your-first-regression-0.5-points",
    "href": "Homework/3/Unreleased/archive/hw07/hw07.html#problem-1-your-first-regression-0.5-points",
    "title": "CS380 - Homework 7 : Prostate Cancer Regression (7 points)",
    "section": "Problem 1: Your First Regression (0.5 points)",
    "text": "Problem 1: Your First Regression (0.5 points)\nWe will begin by fitting our first ordinary least squares regression model. But first we need to do a little data management. You will notice that the data exist in a single data frame (one for Train and one for Test). The last column of the data frame (‘lpsa’) is the quantity that we wish to predict (the Y-value).\nDo the following in the cell below, * Create X_train and Y_train by separating out the last column (‘lpsa’) and store it in Y_train * Display the DataFrame X_train\n\nfeatures = ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']\noutput = ['lpsa']\nX_train = # Insert code here\nY_train = # Insert code here\n\n# Display training inputs\n# Insert code here\n\nNow we will fit our first model using a single feature (‘lcavol’). Do the following in the cell below, * Train a linear regression model on the ‘lcavol’ feature * Compute the R-squared score of the model on the training data * Scatterplot the training data for the ‘lcavol’ feature * Plot the regression line over the scatterplot * Label the plot axis / title and report the R-squared score\nA couple of notes: * Scikit-learn gets cranky when you pass in single features. In some versions you will need to use, X_train[‘lcavol’].values.reshape(-1, 1) * To plot the regression line you can create a dense grid of points using numpy.arange, between the min() and max() of the feature values.\nDocumentation - Scikit-Learn - LinearRegression\n\n# Fit one feature\n# Insert code here\n\n# plot\n# Insert code here"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw07/hw07.html#problem-2-best-subset-feature-selection-2-points",
    "href": "Homework/3/Unreleased/archive/hw07/hw07.html#problem-2-best-subset-feature-selection-2-points",
    "title": "CS380 - Homework 7 : Prostate Cancer Regression (7 points)",
    "section": "Problem 2: Best Subset Feature Selection (2 points)",
    "text": "Problem 2: Best Subset Feature Selection (2 points)\nNow we will look at finding the best subset of features out of all possible subsets. To do this, you will implement the Best Feature Subset Selection as presented in lecture (see lecture slides). We will break this into subproblems to walk through it. To help you with this we have provided a function findsubsets(S,k). When passed a set S this function will return a set of all subsets of size k, which you can iterate through to train models.\n\ndef findsubsets(S,k):\n    return set(itertools.combinations(S, k))\n\nWe will start by getting familiar with the findsubsets() function. The variable ‘features’ was defined previously as a set of all feature names. In the cell do the following: * Use findsubsets to find all possible subsets of 3 features * Perform 5-fold cross validation to train a LinearRegression model on each set of 3 features * Find the model with the highest average \\(R^2\\) score (scoring=‘r2’) * Report the best performing set of features and the corresponding \\(R^2\\) score\nDocumentation - Scikit-Learn - cross_val_score\n\n# Insert code here\n\nNow, repeat the above process for all subsets of all sizes. For each \\(k=1,\\ldots,8\\) find all possible subsets of \\(k\\) features and evaluate a model on each set of features using 5-fold cross validation. Report your findings as follows, * Produce a scatterplot of \\(R^2\\) values for every run with subset size on the horizontal axis, and \\(R^2\\) on the vertical axis (label your plot axes/title) * Find the best performing model overall and report the \\(R^2\\) and features for that model\nHint: The plot you will produce should look similar to one presented during lecture on Best Subset Selection. See lecture slides\n\n# Insert code here\n\nExcellent You have found the best set of features by brute-force search over all possible features. Good work."
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw07/hw07.html#problem-3-ridge-regression-2-points",
    "href": "Homework/3/Unreleased/archive/hw07/hw07.html#problem-3-ridge-regression-2-points",
    "title": "CS380 - Homework 7 : Prostate Cancer Regression (7 points)",
    "section": "Problem 3 : Ridge Regression (2 points)",
    "text": "Problem 3 : Ridge Regression (2 points)\nThe problem with brute force search over features is that it doesn’t scale well. We can do it for 8 features, but we can’t do it for larger sets of features. Instead, we will look at a simpler model selection strategy by using L2 regularized linear regression (a.k.a. Ridge Regression). Do the following in the cell below, * Learn a Ridge regression model on training data with alpha=0.5 * Report the learned feature weights using the provided printFeatureWeights function\nDocumentation - Scikit-Learn - linear_model.Ridge\n\ndef printFeatureWeights(f, w):\n  for idx in range(len(f)):\n    print('%s : %f' % (f[idx], w[idx]))\n\n# Insert code here\n\nWe chose the regularization coefficient alpha=0.5 somewhat arbitrarily. We now need to perform model selection in order to learn the best value of alpha. We will do that by using cross_val_score over a range of values for alpha. When searching for regularization parameters it is generally good practice to search in log-domain, rather than linear domain. For example, we will search in the range \\([10^{-1}, 10^3]\\). Using Numpy’s “logspace” function this corresponds to the range \\([-1, 3]\\) in log-domain. In the cell below do the following, * Create a range of 50 alpha values spaced logarithmically in the range \\([10^{-1}, 10^3]\\) * Perform 5-fold cross-validation of Ridge regression model for each alpha and record \\(R^2\\) score for each run (there will be 5x50 values) * Report the best \\(R^2\\) score and the value of alpha that achieves that score * Use Matplotlib errorbar() function to plot the average \\(R^2\\) with 1 standard deviation error bars for each of the 50 alpha values\nDocumentation - Matplotlib - errorbar\nDocumentation - Numpy - logspace\n\n# Insert code here\n\nNow that we have a good model we will look at what it has learned. Train the Ridge regression model using the selected alpha from the previous cell. Report the learned feature weights using the printFeatureWeights() function previously provided."
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw07/hw07.html#problem-3-lasso-1.5-points",
    "href": "Homework/3/Unreleased/archive/hw07/hw07.html#problem-3-lasso-1.5-points",
    "title": "CS380 - Homework 7 : Prostate Cancer Regression (7 points)",
    "section": "Problem 3 : LASSO (1.5 points)",
    "text": "Problem 3 : LASSO (1.5 points)\nRidge regression performs shrinkage of the weights using the L2 norm. This will drive some weights close to zero, but not exactly zero. The LASSO method replaces the L2 penalty with an L1 penalty. Due to properties of L1 discussed in lecture, this has the effect of learning exactly zero weights on some features when it is supported by the data. In this problem we will repeat procedure of learning a Ridge regression model, but we will instead use LASSO. Let’s start by fitting a LASSO model with a fixed alpha value. In the cell below do the following, * Fit LASSO with alpha=0.1 * Use printFeatureWeights() to report the learned feature weights\nDocumentation - Scikit-Learn - linear_model.Lasso\n\n# Insert code here\n\nNow we will find a good value of alpha using cross-validation. Due to differences in how the LASSO model is optimized, there are dedicated methods for performing cross-validation on LASSO. Scikit-Learn’s LassoLarsCV class performs LASSO-specific cross-validation using an optimized Least Angle Regression (LARS) algorithm. In the cell below do the following, * Using LassoLarsCV perform 20-fold cross validation to solve all solution paths for Lasso * Plot mean +/- standard error of mean squared error versus regularization coefficient \\(\\alpha\\) * Title the plot and axes * Report the best alpha value and the corresponding average mean squared error from cross-validation\nNote: LassoLarsCV returns mean squared error, rather than \\(R^2\\). It also determines the set of \\(\\alpha\\) values automatically, which are stored in the cv_alphas_ attribute.\nDocumentation - Scikit-Learn - LassoLarsCV\n\n# Insert code here"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw07/hw07.html#problem-4-evaluate-on-test-1-point",
    "href": "Homework/3/Unreleased/archive/hw07/hw07.html#problem-4-evaluate-on-test-1-point",
    "title": "CS380 - Homework 7 : Prostate Cancer Regression (7 points)",
    "section": "Problem 4 : Evaluate on Test (1 Point)",
    "text": "Problem 4 : Evaluate on Test (1 Point)\nIn this problem we will train all of the best performing models chosen by Best Subsets, Ridge Regression, and LASSO. We will evaluate and compare these models on the test data. This dataset uses a standard train / test split so we begin by loading test data below.\n\ndf_test = pd.read_csv('prostate_test.csv')\ndf_test.head()\n\n\n\n\n\n\n\n\nlcavol\nlweight\nage\nlbph\nsvi\nlcp\ngleason\npgg45\nlpsa\n\n\n\n\n0\n0.737164\n3.473518\n64\n0.615186\n0\n-1.386294\n6\n0\n0.765468\n\n\n1\n-0.776529\n3.539509\n47\n-1.386294\n0\n-1.386294\n6\n0\n1.047319\n\n\n2\n0.223144\n3.244544\n63\n-1.386294\n0\n-1.386294\n6\n0\n1.047319\n\n\n3\n1.205971\n3.442019\n57\n-1.386294\n0\n-0.430783\n7\n5\n1.398717\n\n\n4\n2.059239\n3.501043\n60\n1.474763\n0\n1.348073\n7\n20\n1.658228\n\n\n\n\n\n\n\nRecall that all of the data are stored in a single table, with the final column being the output ‘lpsa’. Before evaluating on test you must first create X_test and Y_test input/outputs where Y_test is the final column of the DataFrame, and X_test contains all other columns.\n\n# Insert code here\n\n\nBest Subsets\nIn Problem 2 you found the best subset of features for an ordinary least squares regression model by enumerating all feature subsets. Using the best selected features train the model below and report mean squared error and \\(R^2\\) on the test set.\n\n# Insert code here\n\n\n\nRidge Regression\nIn the cell below, train a Ridge Regression model using the optimal regularization coefficient (\\(\\alpha\\)) found in Problem 2. Report mean squared error and \\(R^2\\) on the test set.\n\n# Insert code here\n\n\n\nLASSO Regression\nNow, train and evaluate your final model. Train a Lasso regression using the optimal \\(\\alpha\\) parameters from Problem 3 and report MSE and \\(R^2\\) on the test set.\n\n# Insert code here\n\n\n\nCompare feature weights for each model\nNow let’s compare the feature weight learned by each of the three models. In the cell below, report the regression weights for each feature under Best Subset, Ridge, and Lasso models evaluated above. To make the output easier to read, please use a Pandas DataFrame to display the data. To do this, create a Pandas DataFrame where each column contains regression weights for one of the previous models, and then display that DataFrame in the standard fashion. You should also provide feature names on each of the rows.\nDocumentation - Pandas - DataFrame\n\n# Insert code here"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html",
    "href": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html",
    "title": "CSC380 Homework 6 : Problem 1 : Naive Bayes Classifier (4 pts)",
    "section": "",
    "text": "The RMS Titanic was a passenger ship that infamously sunk during its maiden voyage in 1912. Most of the people on the ship unfortunately perished. In this problem we are going to train a Naive Bayes Classifier on features the passengers to see if we can accurately predict whether passengers survived the disaster. The features of each passenger include the fare they paid, their age, gender, etc. We emphasize that the model does not learn causal relationships, it only learns which features correlate with survival in the data. This is just for fun: do not draw any conclusions from this analysis. This dataset was part of a popular Kaggle Competition. If you’re curious you may enjoy a quick video about the competition.\n\n\nPlease submit the completed Jupyter notebook to D2L. Make sure it is the .ipynb file not a .html file! All cells are marked with instructions to insert your code. Please complete all cells as directed.\n\n\n\ntitanic.jpg\n\n\n\nimport warnings\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html#did-they-survive-the-sinking-of-the-titanic",
    "href": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html#did-they-survive-the-sinking-of-the-titanic",
    "title": "CSC380 Homework 6 : Problem 1 : Naive Bayes Classifier (4 pts)",
    "section": "",
    "text": "The RMS Titanic was a passenger ship that infamously sunk during its maiden voyage in 1912. Most of the people on the ship unfortunately perished. In this problem we are going to train a Naive Bayes Classifier on features the passengers to see if we can accurately predict whether passengers survived the disaster. The features of each passenger include the fare they paid, their age, gender, etc. We emphasize that the model does not learn causal relationships, it only learns which features correlate with survival in the data. This is just for fun: do not draw any conclusions from this analysis. This dataset was part of a popular Kaggle Competition. If you’re curious you may enjoy a quick video about the competition.\n\n\nPlease submit the completed Jupyter notebook to D2L. Make sure it is the .ipynb file not a .html file! All cells are marked with instructions to insert your code. Please complete all cells as directed.\n\n\n\ntitanic.jpg\n\n\n\nimport warnings\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html#data",
    "href": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html#data",
    "title": "CSC380 Homework 6 : Problem 1 : Naive Bayes Classifier (4 pts)",
    "section": "Data",
    "text": "Data\nThe first steps in any data science project involve loading and cleaning up data. In our example we will need to deal with two issues: handling missing values and converting categorical data into numerical quantities that can be handled by the Naive Bayes model. We will start by loading the data.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = #read the csv - ('data/titanic.csv')\n\n# The features we will use\nfeatures = ['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp'] \nX = data[features]\n\n# The target (predictor)\ntarget = 'Survived'\nY = data[target]\n\n# replace missing values\nX[\"Age\"].fillna(np.random.choice(X['Age'][~X['Age'].isna()]),inplace = True)\nX[\"Embarked\"].fillna(np.random.choice(X['Embarked'][~X['Embarked'].isna()]),inplace = True) \nX.head(5)\n\n\na. Encode categorical values (0.5 points)\nOur data contains two categorical features, “Sex” and “Embarked”, both containing string values. In order to train a Naive Bayes model we must first convert these categorical data into numerical values. The Scikit-Learn LabelEncoder class handles this for you. Please use the Preprocessing.LabelEncoder.fit_transform() function to fit and transform categorical values to numerical values for, both, the “Sex” and “Embarked” fields.\nDocumentation - Scikit-Learn - LabelEncoder\n\nfrom sklearn import preprocessing\nle = #instaiate the the Label Encoder here. \nX['Sex'] = #insert your code here\nX['Embarked'] = #insert your code here\nX.head(5)\n\n\n\nb. Split the dataset into test and train (0.5 points)\nUse the scikit-learn “train_test_split” function to create a Training / Test split with 75% of the data designated to training, and 25% to testing. Make sure to use the random state provided below, to ensure that everyone has the same training/test split.\nDocumentation - Scikit-Learn - Train Test Split\n\nfrom sklearn.model_selection import train_test_split\nrandom_state=42\ntest_size=0.25\n\nX_train, X_test, Y_train, Y_test = #insert your code here\n\n# look at some training data\nX_train.head(5)\n\n\n\nc. Summary statistics (0.2 points)\nIt’s good to make sure that the statistics of your data in training and test are similar. The Pandas “DataFrame.describe()” function is a very useful function for computing summary statistics of a DataFrame. Use the “describe” function on the training and test features (X_train and X_test) to look at their summary statistics.\nDocumentation - Pandas - DataFrame.describe\n\nX_train.#insert your code here\n\n\nX_test.#insert your code here"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html#checking-model-assumptions",
    "href": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html#checking-model-assumptions",
    "title": "CSC380 Homework 6 : Problem 1 : Naive Bayes Classifier (4 pts)",
    "section": "Checking Model Assumptions",
    "text": "Checking Model Assumptions\nRecall from lecture that Niave Bayes models features as conditionally independent, given the class label. In a real data this assumption doesn’t hold. It’s always good to test your assumptions to see how badly they are violated in the data. One way to test for dependence is to measure correlation. You will compute the Pearson correlation coefficient on each pair of features to give us a hint about how independent they are from the others.\n\nimport numpy as np\n#Some intialisations\n\ncolumns = features +[target]\nnColumns = len(columns)\n\nresult = pd.DataFrame(np.zeros((nColumns, nColumns)), columns=columns)\n\ntrain = X_train.copy()\ntrain[target]= Y_train\n\n\nfrom scipy.stats.stats import pearsonr\n# Apply Pearson correlation on each pair of features.\nfor col_a in range(nColumns):\n    for col_b in range(nColumns):\n        result.iloc[[col_a], [col_b]] = round(pearsonr(train.loc[:, columns[col_a]], train.loc[:,  columns[col_b]])[0],2)\n\n\nfig, ax = plt.subplots(figsize=(10,10))\nim = ax.imshow(result)\n\n#,yticklabels=columns, vmin=-1, vmax=1, annot=True, fmt='.2f', linewidths=.2)\n\n# We want to show all ticks...\nax.set_xticks(np.arange(nColumns))\nax.set_yticks(np.arange(nColumns))\n\n# ... and label them with the respective list entries\nax.set_xticklabels(columns)\nax.set_yticklabels(columns)\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\n\n# Loop over data dimensions and create text annotations.\nfor i in range(nColumns):\n    for j in range(nColumns):\n        text = ax.text(j, i, result.iloc[i, j],ha=\"center\", va=\"center\", color=\"w\")\n                   \nax.set_title('PCC - Pearson correlation coefficient')\nplt.show()\n\n\nd. Normal feature assumption (0.3 points)\nWe will model the class-conditional distributions of our continuous numerical features as Normal distributions. Let’s check that assumption as well. In the cell below, use the DataFrame.plot(kind=‘density’) function to plot the densities of each of your numerical features.\nDocumentation - Pandas - DataFrame.plot()\n\ncontinuous_numeric_features = ['Age', 'Fare', 'Parch', 'SibSp']\n\n# Insert your code here\n\nComments : ‘Fare’, ‘Parch’, and ‘SibSp’ have a distribution close to normal, but with a left side skew, “Age” have a distribution a a bit different from the other but maybe it’s close enough to Gaussian."
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html#model-training-selection-and-evaluation",
    "href": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html#model-training-selection-and-evaluation",
    "title": "CSC380 Homework 6 : Problem 1 : Naive Bayes Classifier (4 pts)",
    "section": "Model Training, Selection, and Evaluation",
    "text": "Model Training, Selection, and Evaluation\nWe will now fit our Naive Bayes model to data, compare variations on the model, and evaluate our best model.\n\ne. Initial Model Fit (0.75 points)\nWe will fit a Naive Bayes models with Normally distributed class-conditional distributions on the features. In the cell below, define a GaussianNB object and use the fit() function to fit to training data. Use the predict() function to predict labels for the test data. Compute and report the accuracy of your predictions.\nDocumentation - Scikit-Learn - naive_bayes.GaussianNB\n\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.#insert your code to fit the Test Data\nY_pred = nb.#insert your code here\n\n# Evaluate Accuracy\n# Insert your code here\n\nUsing the attributes of the GaussianNB class, display the means of the class-conditional distributions for each label category (Died, Survived). The theta_ attribute of your classifier will return a 2xM array where each row contains means for each of the M features. Print the output in the following format:\nfeature-name (label-category-name): value\nFor example:\nAge (Died): 33.732530\n\nfrom sklearn.naive_bayes import GaussianNB\n\n# Insert your code here\n\n\n\nf. Model Selection (1.25 points)\nWe will use cross-validation to choose among a number of models, and select the best to evaluate on test data. To begin, use the Scikit-Learn cross_val_score() function to perform 10-fold cross validation of the GaussianNB classifier. Print the mean and standard deviation of the cross-validation score.\nDocumentation - Scikit-Learn - model_selection.cross_val_score\n\nfrom sklearn.model_selection import cross_val_score\nscoring = 'accuracy' # use Accuracy scoring method in cross-validation\ncv=10 # 10-fold cross validation\n\n# Cross-Validation on Baseline model\n# Insert your code here\n\nHow predictive is each feature? For each feature in the data compute the cross validation score using only that feature as input. Report the mean and standard deviation of the CV score for each feature. Report which feature is most predictive.\nNote: cross_val_score() expects a 2D array as input features, so if you simply pass in X[‘feature’] it will complain. You’ll need to temporarily copy and augment the feature. I’ve included some commented code as an example.\n\n## You may need the following code to pass single features to cross_val_score\n# Xtmp = X_train[feat].values\n# Xtmp = Xtmp.reshape((len(X_train[feat]), 1))\n\n# Train / predict using each feature\n# Insert your code here\n\nprint( '&lt;insert the most predictive feature name&gt;  is most predictive.')\n\n&lt;insert the most predictive feature name&gt;  is most predictive.\n\n\nDrop highly correlated features. Recall that Naive Bayes models features as conditionally independent. However, we found that the Pearson correlation coefficient between ‘Pclass’ and ‘Fare’ indicates that the two features are highly correlated. Create a temporary copy of the Training and Test data (e.g. X_train.copy()) and drop the Fare feature but keep the ‘Pclass’ feature. You may do this using the DataFrame.drop() function.\nReport the mean/stdev of the new CV score without this feature.\nDocumentation - Pandas - DataFrame.drop\n\n# Drop Fare\n# Insert your code here\n\nNow repeat the above procedure, again copying the original training data, and drop the ‘Pclass’ feature but keep the ‘Fare’ feature. Again report the mean/stdev of the CV score of the classifier without this feature.\n\n# Drop Pclass (highly-correlated with Fare)\n# Insert your code here"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html#testing-the-model",
    "href": "Homework/3/Unreleased/archive/hw06/HW6 - Problem One.html#testing-the-model",
    "title": "CSC380 Homework 6 : Problem 1 : Naive Bayes Classifier (4 pts)",
    "section": "Testing the Model",
    "text": "Testing the Model\nIf you have done things properly you should see that discarding the ‘Pclass’ feature, but keeping ‘Fare’, leads to the best prediction accuracy. In the code below we will select this best performing model and evaluate it on the test data.\n\ng. Select the model and test (0.25 points)\nIn the cell below perform the following steps: * Permanently drop ‘Pclass’ feature from training and test data * Train a GaussianNB() model on the modified training data using the fit() function * Evaluate the model on Test data using the predict() function * Report prediction accuracy\n\n#Drop Pclass from both test and train data with axis = 1, inplace = True\n\n\nfrom sklearn.metrics import accuracy_score\n\n# Insert your code here\n\n\n\ne. Evaluation metrics (0.25 points)\nNow we will look beyond accuracy to evaluate our classifier. One useful statistic for evaluating classifiers is the confusion matrix, which enumerates categories of correct and incorrect classifications. For more information see the Wikipedia article on the confusion matrix. Compute the confusion matrix and report whether one class is confused for another class more often, or whether they are about the same (within a couple of points).\nDocumentation - Scikit-Learn - metrics.confusion_matrix\n\nfrom sklearn.metrics import confusion_matrix\n\nprint(#the confusion matrix)\nprint('Your answer here')\n\nTwo other useful statistics are precision and recall. In binary classification precision tells us the fraction of correct positives, out of the total number of predicted positives. In other words, of all the times we say “positive” what percentage were correct:\n$ = $\nRecall, on the other hand, tells us how many of the positive cases we actually guessed right:\n$ = $\nIn the cell below, use Scikit-Learn’s precision_score() and recall_score() to report the Precision and Recall of the classifier.\nDocumentation - Scikit-Learn - metrics.precision_score\nDocumentation - Scikit-Learn - metrics.recall_score\n\nfrom sklearn.metrics import recall_score, precision_score\n\n# Insert your code"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw06/HW6 - Problem Two.html#what-to-submit",
    "href": "Homework/3/Unreleased/archive/hw06/HW6 - Problem Two.html#what-to-submit",
    "title": "CSC380 Homework 6",
    "section": "What to submit",
    "text": "What to submit\nPlease submit the completed Jupyter notebook to D2L. Make sure it is the .ipynb file, not a .html file! All cells are marked with instructions to insert your code. Please complete all cells as directed.\n\nimport warnings\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw06/HW6 - Problem Two.html#data",
    "href": "Homework/3/Unreleased/archive/hw06/HW6 - Problem Two.html#data",
    "title": "CSC380 Homework 6",
    "section": "Data",
    "text": "Data\nThe first step is to look into the data : Here, since the data do not have any missing values, we deal with two other issues.\n\nCategory Encoding\nFeature Scaling\n\n\nimport pandas as pd\n\n\npurchases_df = pd.read_csv('data/train_ads.csv') \npurchases_df.head()\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15624510\nMale\n19\n19000\n0\n\n\n1\n15810944\nMale\n35\n20000\n0\n\n\n2\n15668575\nFemale\n26\n43000\n0\n\n\n3\n15603246\nFemale\n27\n57000\n0\n\n\n4\n15804002\nMale\n19\n76000\n0\n\n\n\n\n\n\n\nWe have three features that would be super useful to make the decision. Gender, Age, and Estimated Salary. Our Target is the column Purchased.\n\nX_train = purchases_df[['Gender','Age','EstimatedSalary']]\nY_train = purchases_df['Purchased']\n\n\na. Encode categorical values (0.25 points)\nOur data contains a catgorical feature (Gender) which is a string. In order to train the model we must first convert the categorical data into numerical values. The Scikit-Learn LabelEncoder class handles this for you. Please use the Preprocessing.LabelEncoder.fit_transform() function to fit and transform categorical values to numerical values for Gender. The procedure is similar to what you performed in Problem 1.\nDocumentation - Scikit-Learn - LabelEncoder\n\n# Insert your code here\n\n\n\nb. Feature Scaling (0.25 points)\nFeature Scaling is a technique to standardize the independent features present in the data. The Scikit-Learn Preprocessing StandardScaler performs Z-scoring, as discussed in lecutre. This will help all numerical data follow a standard Normal distribution \\(\\mathcal{N}(0,1)\\). Without this, features with higher variance may dominate the predictions. Please use StandardScalar to fit and transform all of your data.\nsklearn.preprocessing.StandardScaler\n\n# Insert your code here"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw06/HW6 - Problem Two.html#model-training-selection-and-evaluation",
    "href": "Homework/3/Unreleased/archive/hw06/HW6 - Problem Two.html#model-training-selection-and-evaluation",
    "title": "CSC380 Homework 6",
    "section": "Model Training, Selection, and Evaluation",
    "text": "Model Training, Selection, and Evaluation\nWe will now fit our K-Nearest Neighbor model to training data, compare variations on the model, and evaluate our best model on Test data.\n\nc. Initial Model Fit (0.5 points)\nIn the cell below create a KNeighborsClassifier object with K=2 neighbors, and evaluate cross-validation (CV) score (using cross_val_score) with 10 folds. Report the mean and standard deviation of the accuracy CV score.\nsklearn.neighbors.KNeighborsClassifier\n\n# Insert your code here\n\n\n\ne. Model Selection - 0.8 points\nRecall that a K-Nearest Neighbor model has a single hyperparameter: the number of neighbors K. We will perform model selection in order to choose the optimal value of K. To do this, evaluate the cross validation score (10-fold) for 15 models, each model having a different value K in the range 1 to 15. Produce the following output: * A plot of K versus accuracy (label axes and title your plot) * Print the value of the highest accuracy * Print the value of K that achieves the highest accuracy\nThis is a great resource Finding the optimal value of k\n\n# Insert your code here\n\n\n\nf. Evaluate the Model (0.5 points)\nNow, you will a more careful evaluation of your selected model. Unlike cross_val_score, which only allows a single scoring function, cross_validate accepts a tuple of scoring functions. In cell below please create a KNeighborsClassifier with the optimal K value as chosen above. Use the model_selection.cross_validate function to perform 10-fold cross validation and report the all of the following scores:\n\nAverage Prediction accuracy\nAverage Precision\nAverage Recall\nAverage F1\n\nPlease make the output as readible as possible for your graders.\nDocumentation - SciKit-Learn - model_selection.cross_validate\n\n# Insert your code here\n\n\n\ng. Train the Final Model (0.3 points)\nCross validation doesn’t produce a fitted model–that isn’t its purpose. Instead, cross-validation is used to estimate the generalization scores by averaging scores across multiple train/validation splits. In the cell below we will finally train our selected model. Do the following: * Create a K-Nearest Neighbors model with the optimum K value (as we chose previously) * Train the model on all training data (do not use cross validation) * Report prediction accuracy on the training set * Compute and report the confusion matrix on the training data\n\n# Insert your code here"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw06/HW6 - Problem Two.html#testing-the-model",
    "href": "Homework/3/Unreleased/archive/hw06/HW6 - Problem Two.html#testing-the-model",
    "title": "CSC380 Homework 6",
    "section": "Testing the Model",
    "text": "Testing the Model\nYou may have noticed that we did not ask you to create a Train / Test split. This dataset has a standard Training / Test split. However, the Test data represet actual user features who have not been served ads. As a result, we do not have labels in the true Test set and so cannot compute test accuracy. The following cell loads the test data, which you will then evaluate using a variety of metrics.\n\ntest_users = pd.read_csv('data/test_ads.csv')\ntest_users.head()\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\n\n\n\n\n0\n15683758\nMale\n42\n64000\n\n\n1\n15670615\nMale\n48\n33000\n\n\n2\n15715622\nFemale\n44\n139000\n\n\n3\n15707634\nMale\n49\n28000\n\n\n4\n15806901\nFemale\n57\n33000"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/hw06/HW6 - Problem Two.html#f.-applying-the-model---0.4-points",
    "href": "Homework/3/Unreleased/archive/hw06/HW6 - Problem Two.html#f.-applying-the-model---0.4-points",
    "title": "CSC380 Homework 6",
    "section": "f. Applying the Model - 0.4 points",
    "text": "f. Applying the Model - 0.4 points\nNow that we have build our model, let’s look for potensial customers of product X to send the ads too. In the data, you will find a csv test_ads.csv.\nRead the csv, perfrom the Categorical Encoding and Feature Scaling. Remember to use the same Encoder and Standard Scalar as the ones you used above and print the userIds of users who are likely to buy the product. part of the work has been done for you.\n\n# Insert your code here"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/Untitled.html",
    "href": "Homework/3/Unreleased/archive/Untitled.html",
    "title": "CSC 380",
    "section": "",
    "text": "import pandas as pd\n\n\ndata = pd.read_csv('Medicalpremium.csv')\ndata.sample(2)\n\n\n\n\n\n\n\n\nAge\nDiabetes\nBloodPressureProblems\nAnyTransplants\nAnyChronicDiseases\nHeight\nWeight\nKnownAllergies\nHistoryOfCancerInFamily\nNumberOfMajorSurgeries\nPremiumPrice\n\n\n\n\n488\n45\n0\n0\n0\n1\n175\n54\n0\n0\n0\n25000\n\n\n406\n35\n1\n0\n0\n0\n162\n59\n0\n0\n0\n23000"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/linear-regression-tutorial.html",
    "href": "Homework/3/Unreleased/archive/linear-regression-tutorial.html",
    "title": "Import Library and Dataset",
    "section": "",
    "text": "Now we will import couple of python library required for our analysis and import dataset\n# Import library\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization\nplt.rcParams['figure.figsize'] = [8,5]\nplt.rcParams['font.size'] =14\nplt.rcParams['font.weight']= 'bold'\n# Import dataset\ndf = pd.read_csv('Medicalpremium.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()\n\n\nNumber of rows and columns in the data set:  (986, 11)\n\n\n\n\n\n\n\n\n\n\nAge\nDiabetes\nBloodPressureProblems\nAnyTransplants\nAnyChronicDiseases\nHeight\nWeight\nKnownAllergies\nHistoryOfCancerInFamily\nNumberOfMajorSurgeries\nPremiumPrice\n\n\n\n\n0\n45\n0\n0\n0\n0\n155\n57\n0\n0\n0\n25000\n\n\n1\n60\n1\n0\n0\n0\n180\n73\n0\n0\n0\n29000\n\n\n2\n36\n1\n1\n0\n0\n158\n59\n0\n0\n1\n23000\n\n\n3\n52\n1\n1\n0\n1\n183\n93\n0\n0\n2\n28000\n\n\n4\n38\n0\n0\n0\n1\n166\n88\n0\n0\n1\n23000\ndf.describe()\n\n\n\n\n\n\n\n\nAge\nDiabetes\nBloodPressureProblems\nAnyTransplants\nAnyChronicDiseases\nHeight\nWeight\nKnownAllergies\nHistoryOfCancerInFamily\nNumberOfMajorSurgeries\nPremiumPrice\n\n\n\n\ncount\n986.000000\n986.000000\n986.000000\n986.000000\n986.000000\n986.000000\n986.000000\n986.000000\n986.000000\n986.000000\n986.000000\n\n\nmean\n41.745436\n0.419878\n0.468560\n0.055781\n0.180527\n168.182556\n76.950304\n0.215010\n0.117647\n0.667343\n24336.713996\n\n\nstd\n13.963371\n0.493789\n0.499264\n0.229615\n0.384821\n10.098155\n14.265096\n0.411038\n0.322353\n0.749205\n6248.184382\n\n\nmin\n18.000000\n0.000000\n0.000000\n0.000000\n0.000000\n145.000000\n51.000000\n0.000000\n0.000000\n0.000000\n15000.000000\n\n\n25%\n30.000000\n0.000000\n0.000000\n0.000000\n0.000000\n161.000000\n67.000000\n0.000000\n0.000000\n0.000000\n21000.000000\n\n\n50%\n42.000000\n0.000000\n0.000000\n0.000000\n0.000000\n168.000000\n75.000000\n0.000000\n0.000000\n1.000000\n23000.000000\n\n\n75%\n53.000000\n1.000000\n1.000000\n0.000000\n0.000000\n176.000000\n87.000000\n0.000000\n0.000000\n1.000000\n28000.000000\n\n\nmax\n66.000000\n1.000000\n1.000000\n1.000000\n1.000000\n188.000000\n132.000000\n1.000000\n1.000000\n3.000000\n40000.000000"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/linear-regression-tutorial.html#data-preprocessing",
    "href": "Homework/3/Unreleased/archive/linear-regression-tutorial.html#data-preprocessing",
    "title": "Import Library and Dataset",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nEncoding\nMachine learning algorithms cannot work with categorical data directly, categorical data must be converted to number. 1. Label Encoding 2. One hot encoding 3. Dummy variable trap\nLabel encoding refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them.\nA One hot encoding is a representation of categorical variable as binary vectors.It allows the representation of categorical data to be more expresive. This first requires that the categorical values be mapped to integer values, that is label encoding. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\nThe Dummy variable trap is a scenario in which the independent variable are multicollinear, a scenario in which two or more variables are highly correlated in simple term one variable can be predicted from the others.\nBy using pandas get_dummies function we can do all above three step in line of code. We will this fuction to get dummy variable for sex, children,smoker,region features. By setting drop_first =True function will remove dummy variable trap by droping one variable and original variable.The pandas makes our life easy.\n\n# Dummy variable\ncategorical_columns = ['sex','children', 'smoker', 'region']\ndf_encode = pd.get_dummies(data = df, prefix = 'OHE', prefix_sep='_',\n               columns = categorical_columns,\n               drop_first =True,\n              dtype='int8')\n\n\n# Lets verify the dummay variable process\nprint('Columns in original data frame:\\n',df.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df.shape)\nprint('\\nColumns in data frame after encoding dummy variable:\\n',df_encode.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df_encode.shape)\n\nColumns in original data frame:\n ['age' 'sex' 'bmi' 'children' 'smoker' 'region' 'charges']\n\nNumber of rows and columns in the dataset: (1338, 7)\n\nColumns in data frame after encoding dummy variable:\n ['age' 'bmi' 'charges' 'OHE_male' 'OHE_1' 'OHE_2' 'OHE_3' 'OHE_4' 'OHE_5'\n 'OHE_yes' 'OHE_northwest' 'OHE_southeast' 'OHE_southwest']\n\nNumber of rows and columns in the dataset: (1338, 13)\n\n\n\n\nBox -Cox transformation\nA Box Cox transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn’t normal, applying a Box-Cox means that you are able to run a broader number of tests. All that we need to perform this transformation is to find lambda value and apply the rule shown below to your variable.\n\\[\\mathbf{ \\begin {cases}\\frac {y^\\lambda - 1}{\\lambda},& y_i\\neg=0 \\\\\nlog(y_i) & \\lambda = 0 \\end{cases}}\\] The trick of Box-Cox transformation is to find lambda value, however in practice this is quite affordable. The following function returns the transformed variable, lambda value,confidence interval\n\nfrom scipy.stats import boxcox\ny_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\n\n#df['charges'] = y_bc  \n# it did not perform better for this model, so log transform is used\nci,lam\n\n((-0.01140290617294196, 0.0988096859767545), 0.043649053770664956)\n\n\n\n## Log transform\ndf_encode['charges'] = np.log(df_encode['charges'])\n\nThe original categorical variable are remove and also one of the one hot encode varible column for perticular categorical variable is droped from the column. So we completed all three encoding step by using get dummies function."
  },
  {
    "objectID": "Homework/3/Unreleased/archive/linear-regression-tutorial.html#train-test-split",
    "href": "Homework/3/Unreleased/archive/linear-regression-tutorial.html#train-test-split",
    "title": "Import Library and Dataset",
    "section": "Train Test split",
    "text": "Train Test split\n\nfrom sklearn.model_selection import train_test_split\nX = df_encode.drop('charges',axis=1) # Independet variable\ny = df_encode['charges'] # dependent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)"
  },
  {
    "objectID": "Homework/3/Unreleased/archive/linear-regression-tutorial.html#model-building",
    "href": "Homework/3/Unreleased/archive/linear-regression-tutorial.html#model-building",
    "title": "Import Library and Dataset",
    "section": "Model building",
    "text": "Model building\nIn this step build model using our linear regression equation \\(\\mathbf{\\theta = (X^T X)^{-1} X^Ty}\\). In first step we need to add a feature \\(\\mathbf{x_0 =1}\\) to our original data set.\n\n# Step 1: add x0 =1 to dataset\nX_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]\nX_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]\n\n# Step2: build model\ntheta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train)) \n\n\n# The parameters for linear regression model\nparameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]\ncolumns = ['intersect:x_0=1'] + list(X.columns.values)\nparameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})\n\n\n# Scikit Learn module\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train) # Note: x_0 =1 is no need to add, sklearn will take care of it.\n\n#Parameter\nsk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)\nparameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))\nparameter_df\n\n\n\n\n\n\n\n\nParameter\nColumns\ntheta\nSklearn_theta\n\n\n\n\n0\ntheta_0\nintersect:x_0=1\n7.059171\n7.059171\n\n\n1\ntheta_1\nage\n0.033134\n0.033134\n\n\n2\ntheta_2\nbmi\n0.013517\n0.013517\n\n\n3\ntheta_3\nOHE_male\n-0.067767\n-0.067767\n\n\n4\ntheta_4\nOHE_1\n0.149457\n0.149457\n\n\n5\ntheta_5\nOHE_2\n0.272919\n0.272919\n\n\n6\ntheta_6\nOHE_3\n0.244095\n0.244095\n\n\n7\ntheta_7\nOHE_4\n0.523339\n0.523339\n\n\n8\ntheta_8\nOHE_5\n0.466030\n0.466030\n\n\n9\ntheta_9\nOHE_yes\n1.550481\n1.550481\n\n\n10\ntheta_10\nOHE_northwest\n-0.055845\n-0.055845\n\n\n11\ntheta_11\nOHE_southeast\n-0.146578\n-0.146578\n\n\n12\ntheta_12\nOHE_southwest\n-0.133508\n-0.133508\n\n\n\n\n\n\n\nThe parameter obtained from both the model are same.So we succefull build our model using normal equation and verified using sklearn linear regression module. Let’s move ahead, next step is prediction and model evaluvation."
  },
  {
    "objectID": "Homework/3/Unreleased/archive/linear-regression-tutorial.html#model-evaluation",
    "href": "Homework/3/Unreleased/archive/linear-regression-tutorial.html#model-evaluation",
    "title": "Import Library and Dataset",
    "section": "Model evaluation",
    "text": "Model evaluation\nWe will predict value for target variable by using our model parameter for test data set. Then compare the predicted value with actual valu in test set. We compute Mean Square Error using formula \\[\\mathbf{ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2}\\]\n\\(\\mathbf{R^2}\\) is statistical measure of how close data are to the fitted regression line. \\(\\mathbf{R^2}\\) is always between 0 to 100%. 0% indicated that model explains none of the variability of the response data around it’s mean. 100% indicated that model explains all the variablity of the response data around the mean.\n\\[\\mathbf{R^2 = 1 - \\frac{SSE}{SST}}\\] SSE = Sum of Square Error\nSST = Sum of Square Total\n\\[\\mathbf{SSE = \\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2}\\] \\[\\mathbf{SST = \\sum_{i=1}^{m}(y_i - \\bar{y}_i)^2}\\] Here \\(\\mathbf{\\hat{y}}\\) is predicted value and \\(\\mathbf{\\bar{y}}\\) is mean value of \\(\\mathbf{y}\\).\n\n# Normal equation\ny_pred_norm =  np.matmul(X_test_0,theta)\n\n#Evaluvation: MSE\nJ_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]\n\n# R_square \nsse = np.sum((y_pred_norm - y_test)**2)\nsst = np.sum((y_test - y_test.mean())**2)\nR_square = 1 - (sse/sst)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse)\nprint('R square obtain for normal equation method is :',R_square)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.18729622322982067\nR square obtain for normal equation method is : 0.7795687545055299\n\n\n\n# sklearn regression module\ny_pred_sk = lin_reg.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk = mean_squared_error(y_pred_sk, y_test)\n\n# R_square\nR_square_sk = lin_reg.score(X_test,y_test)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)\nprint('R square obtain for scikit learn library is :',R_square_sk)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.1872962232298189\nR square obtain for scikit learn library is : 0.7795687545055318\n\n\nThe model returns \\(R^2\\) value of 77.95%, so it fit our data test very well, but still we can imporve the the performance of by diffirent technique. Please make a note that we have transformer out variable by applying natural log. When we put model into production antilog is applied to the equation."
  },
  {
    "objectID": "Homework/3/Unreleased/archive/linear-regression-tutorial.html#model-validation",
    "href": "Homework/3/Unreleased/archive/linear-regression-tutorial.html#model-validation",
    "title": "Import Library and Dataset",
    "section": "Model Validation",
    "text": "Model Validation\nIn order to validated model we need to check few assumption of linear regression model. The common assumption for Linear Regression model are following 1. Linear Relationship: In linear regression the relationship between the dependent and independent variable to be linear. This can be checked by scatter ploting Actual value Vs Predicted value 2. The residual error plot should be normally distributed. 3. The mean of residual error should be 0 or close to 0 as much as possible 4. The linear regression require all variables to be multivariate normal. This assumption can best checked with Q-Q plot. 5. Linear regession assumes that there is little or no Multicollinearity in the data. Multicollinearity occurs when the independent variables are too highly correlated with each other. The variance inflation factor VIF* identifies correlation between independent variables and strength of that correlation. \\(\\mathbf{VIF = \\frac {1}{1-R^2}}\\), If VIF &gt;1 & VIF &lt;5 moderate correlation, VIF &lt; 5 critical level of multicollinearity. 6. Homoscedasticity: The data are homoscedastic meaning the residuals are equal across the regression line. We can look at residual Vs fitted value scatter plot. If heteroscedastic plot would exhibit a funnel shape pattern.\n\n# Check for Linearity\nf = plt.figure(figsize=(14,5))\nax = f.add_subplot(121)\nsns.scatterplot(y_test,y_pred_sk,ax=ax,color='r')\nax.set_title('Check for Linearity:\\n Actual Vs Predicted value')\n\n# Check for Residual normality & mean\nax = f.add_subplot(122)\nsns.distplot((y_test - y_pred_sk),ax=ax,color='b')\nax.axvline((y_test - y_pred_sk).mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror');\n\n/opt/conda/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n\n\n\n\n\n\n# Check for Multivariate Normality\n# Quantile-Quantile plot \nf,ax = plt.subplots(1,2,figsize=(14,6))\nimport scipy as sp\n_,(_,_,r)= sp.stats.probplot((y_test - y_pred_sk),fit=True,plot=ax[0])\nax[0].set_title('Check for Multivariate Normality: \\nQ-Q Plot')\n\n#Check for Homoscedasticity\nsns.scatterplot(y = (y_test - y_pred_sk), x= y_pred_sk, ax = ax[1],color='r') \nax[1].set_title('Check for Homoscedasticity: \\nResidual Vs Predicted');\n\n\n\n\n\n# Check for Multicollinearity\n#Variance Inflation Factor\nVIF = 1/(1- R_square_sk)\nVIF\n\n4.536561945911135\n\n\nThe model assumption linear regression as follows 1. In our model the actual vs predicted plot is curve so linear assumption fails 2. The residual mean is zero and residual error plot right skewed 3. Q-Q plot shows as value log value greater than 1.5 trends to increase 4. The plot is exhibit heteroscedastic, error will insease after certian point. 5. Variance inflation factor value is less than 5, so no multicollearity."
  },
  {
    "objectID": "Homework/3/Unreleased/Homework 3 | Solutions.html",
    "href": "Homework/3/Unreleased/Homework 3 | Solutions.html",
    "title": "1. Data Processing",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('../dataset.csv')\ndf.head()\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\n\n\n\n\n0\n01-01-2016 21:11\n01-01-2016 21:17\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n\n\n1\n01-02-2016 01:25\n01-02-2016 01:37\nBusiness\nFort Pierce\nFort Pierce\n5.0\nNaN\n\n\n2\n01-02-2016 20:25\n01-02-2016 20:38\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n\n\n3\n01-05-2016 17:31\n01-05-2016 17:45\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n\n\n4\n01-06-2016 14:42\n01-06-2016 15:49\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1156 entries, 0 to 1155\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   START_DATE  1156 non-null   object \n 1   END_DATE    1155 non-null   object \n 2   CATEGORY    1155 non-null   object \n 3   START       1155 non-null   object \n 4   STOP        1155 non-null   object \n 5   MILES       1156 non-null   float64\n 6   PURPOSE     653 non-null    object \ndtypes: float64(1), object(6)\nmemory usage: 63.3+ KB\ndf.describe()\n\n\n\n\n\n\n\n\nMILES\n\n\n\n\ncount\n1156.000000\n\n\nmean\n21.115398\n\n\nstd\n359.299007\n\n\nmin\n0.500000\n\n\n25%\n2.900000\n\n\n50%\n6.000000\n\n\n75%\n10.400000\n\n\nmax\n12204.700000\nSTART_DATE [ 2 points ]\ndf['START_DATE'].isna().any()\n\nFalse\ndf['start time'] = df['START_DATE'].apply(lambda x : x.split(\" \")[-1])\ndf['START_DATE'] = pd.to_datetime(df['START_DATE'], errors= \"ignore\" )\ndf.sample()\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\n\n\n\n\n1043\n12-12-2016 13:22\n12-12-2016 13:32\nBusiness\nCary\nCary\n3.1\nErrand/Supplies\n13:22\nEND_DATE [ 2 points ]\ndf['END_DATE'].isna().any()\n\nTrue\ndf['END_DATE'].isna().sum()\n\n1\ndf[df['END_DATE'].isna()]\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\n\n\n\n\n1155\nTotals\nNaN\nNaN\nNaN\nNaN\n12204.7\nNaN\nTotals\ndf.drop(index=1155,inplace = True)\ndf['END_DATE'].isna().sum()\n\n0\ndf['end time'] = df['END_DATE'].apply(lambda x : x.split(\" \")[-1])\ndf.sample()\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n121\n2/18/2016 8:19\n2/18/2016 8:27\nBusiness\nUnknown Location\nUnknown Location\n23.5\nTemporary Site\n8:19\n8:27\ndf['END_DATE'] = pd.to_datetime(df['END_DATE'])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1155 entries, 0 to 1154\nData columns (total 9 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   START_DATE  1155 non-null   object        \n 1   END_DATE    1155 non-null   datetime64[ns]\n 2   CATEGORY    1155 non-null   object        \n 3   START       1155 non-null   object        \n 4   STOP        1155 non-null   object        \n 5   MILES       1155 non-null   float64       \n 6   PURPOSE     653 non-null    object        \n 7   start time  1155 non-null   object        \n 8   end time    1155 non-null   object        \ndtypes: datetime64[ns](1), float64(1), object(7)\nmemory usage: 81.3+ KB\nCategory\ndf['CATEGORY'].isna().sum()\n\n0\ndf['CATEGORY'].unique()\n\narray(['Business', 'Personal'], dtype=object)\ndf['CATEGORY'].value_counts()\n\nBusiness    1078\nPersonal      77\nName: CATEGORY, dtype: int64\nSTART\ndf['START'].isna().sum()\n\n0\ndf['START'].unique()\n\narray(['Fort Pierce', 'West Palm Beach', 'Cary', 'Jamaica', 'New York',\n       'Elmhurst', 'Midtown', 'East Harlem', 'Flatiron District',\n       'Midtown East', 'Hudson Square', 'Lower Manhattan',\n       \"Hell's Kitchen\", 'Downtown', 'Gulfton', 'Houston', 'Eagan Park',\n       'Morrisville', 'Durham', 'Farmington Woods', 'Whitebridge',\n       'Lake Wellingborough', 'Fayetteville Street', 'Raleigh',\n       'Hazelwood', 'Fairmont', 'Meredith Townes', 'Apex', 'Chapel Hill',\n       'Northwoods', 'Edgehill Farms', 'Tanglewood', 'Preston',\n       'Eastgate', 'East Elmhurst', 'Jackson Heights', 'Long Island City',\n       'Katunayaka', 'Unknown Location', 'Colombo', 'Nugegoda',\n       'Islamabad', 'R?walpindi', 'Noorpur Shahan', 'Heritage Pines',\n       'Westpark Place', 'Waverly Place', 'Wayne Ridge', 'Weston',\n       'East Austin', 'West University', 'South Congress', 'The Drag',\n       'Congress Ave District', 'Red River District', 'Georgian Acres',\n       'North Austin', 'Coxville', 'Convention Center District', 'Austin',\n       'Katy', 'Sharpstown', 'Sugar Land', 'Galveston', 'Port Bolivar',\n       'Washington Avenue', 'Briar Meadow', 'Latta', 'Jacksonville',\n       'Couples Glen', 'Kissimmee', 'Lake Reams', 'Orlando',\n       'Sand Lake Commons', 'Sky Lake', 'Daytona Beach', 'Ridgeland',\n       'Florence', 'Meredith', 'Holly Springs', 'Chessington', 'Burtrose',\n       'Parkway', 'Mcvan', 'Capitol One', 'University District',\n       'Seattle', 'Redmond', 'Bellevue', 'San Francisco', 'Palo Alto',\n       'Sunnyvale', 'Newark', 'Menlo Park', 'Old City', 'Savon Height',\n       'Kilarney Woods', 'Townes at Everett Crossing', 'Huntington Woods',\n       'Seaport', 'Medical Centre', 'Rose Hill', 'Soho', 'Tribeca',\n       'Financial District', 'Oakland', 'Emeryville', 'Berkeley',\n       'Kenner', 'CBD', 'Lower Garden District', 'Lakeview', 'Storyville',\n       'New Orleans', 'Metairie', 'Chalmette', 'Arabi',\n       'Pontchartrain Shores', 'Marigny', 'Covington', 'Mandeville',\n       'Jamestown Court', 'Summerwinds', 'Parkwood',\n       'Pontchartrain Beach', 'St Thomas', 'Banner Elk', 'Elk Park',\n       'Newland', 'Boone', 'Stonewater', 'Lexington Park at Amberly',\n       'Arlington Park at Amberly', 'Arlington', 'Kalorama Triangle',\n       'K Street', 'West End', 'Connecticut Avenue', 'Columbia Heights',\n       'Washington', 'Wake Forest', 'Lahore', 'Karachi', 'SOMISSPO',\n       'West Berkeley', 'North Berkeley Hills', 'San Jose', 'Eagle Rock',\n       'Winston Salem', 'Asheville', 'Topton', 'Hayesville',\n       'Bryson City', 'Almond', 'Mebane', 'Agnew', 'Cory', 'Renaissance',\n       'Santa Clara', 'NOMA', 'Sunnyside', 'Ingleside', 'Central',\n       'Tenderloin', 'College Avenue', 'South', 'Southside',\n       'South Berkeley', 'Mountain View', 'El Cerrito', 'Krendle Woods',\n       'Wake Co.', 'Fuquay-Varina', 'Rawalpindi', 'Kar?chi', 'Katunayake',\n       'Gampaha'], dtype=object)\ndf['START'].value_counts()\n\nCary                201\nUnknown Location    148\nMorrisville          85\nWhitebridge          68\nIslamabad            57\n                   ... \nFlorence              1\nRidgeland             1\nDaytona Beach         1\nSky Lake              1\nGampaha               1\nName: START, Length: 177, dtype: int64\ndf[df['START']=='Unknown Location']\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n109\n2/16/2016 8:29\n2016-02-16 09:34:00\nBusiness\nUnknown Location\nColombo\n14.1\nNaN\n8:29\n9:34\n\n\n117\n2/17/2016 13:18\n2016-02-17 14:04:00\nBusiness\nUnknown Location\nColombo\n14.7\nTemporary Site\n13:18\n14:04\n\n\n121\n2/18/2016 8:19\n2016-02-18 08:27:00\nBusiness\nUnknown Location\nUnknown Location\n23.5\nTemporary Site\n8:19\n8:27\n\n\n122\n2/18/2016 14:03\n2016-02-18 14:45:00\nBusiness\nUnknown Location\nIslamabad\n12.7\nTemporary Site\n14:03\n14:45\n\n\n124\n2/18/2016 18:44\n2016-02-18 18:58:00\nBusiness\nUnknown Location\nIslamabad\n5.2\nCustomer Visit\n18:44\n18:58\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1129\n12/28/2016 17:02\n2016-12-28 17:16:00\nBusiness\nUnknown Location\nKar?chi\n4.4\nErrand/Supplies\n17:02\n17:16\n\n\n1134\n12/29/2016 11:28\n2016-12-29 12:00:00\nBusiness\nUnknown Location\nKar?chi\n11.9\nMeal/Entertain\n11:28\n12:00\n\n\n1141\n12/29/2016 19:50\n2016-12-29 20:10:00\nBusiness\nUnknown Location\nKar?chi\n4.1\nCustomer Visit\n19:50\n20:10\n\n\n1144\n12/29/2016 23:14\n2016-12-29 23:47:00\nBusiness\nUnknown Location\nKar?chi\n12.9\nMeeting\n23:14\n23:47\n\n\n1152\n12/31/2016 15:03\n2016-12-31 15:38:00\nBusiness\nUnknown Location\nUnknown Location\n16.2\nMeeting\n15:03\n15:38\n\n\n\n\n148 rows × 9 columns\ndf = df[df['START']!='Unknown Location']\ndf.sample()\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n982\n11/20/2016 17:45\n2016-11-20 18:37:00\nBusiness\nCary\nCary\n18.5\nErrand/Supplies\n17:45\n18:37\nSTOP\ndf[df['STOP']=='Unknown Location']\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n108\n2/16/2016 3:21\n2016-02-16 04:13:00\nBusiness\nKatunayaka\nUnknown Location\n43.7\nCustomer Visit\n3:21\n4:13\n\n\n116\n2/16/2016 17:40\n2016-02-16 17:44:00\nBusiness\nNugegoda\nUnknown Location\n3.6\nErrand/Supplies\n17:40\n17:44\n\n\n123\n2/18/2016 15:16\n2016-02-18 15:31:00\nBusiness\nIslamabad\nUnknown Location\n6.0\nTemporary Site\n15:16\n15:31\n\n\n125\n2/18/2016 19:27\n2016-02-18 20:08:00\nBusiness\nIslamabad\nUnknown Location\n10.0\nMeeting\n19:27\n20:08\n\n\n131\n2/19/2016 12:09\n2016-02-19 12:27:00\nBusiness\nIslamabad\nUnknown Location\n7.3\nTemporary Site\n12:09\n12:27\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1128\n12/28/2016 15:04\n2016-12-28 15:39:00\nBusiness\nKar?chi\nUnknown Location\n8.5\nMeal/Entertain\n15:04\n15:39\n\n\n1133\n12/29/2016 9:44\n2016-12-29 10:07:00\nBusiness\nKar?chi\nUnknown Location\n11.6\nMeal/Entertain\n9:44\n10:07\n\n\n1140\n12/29/2016 18:59\n2016-12-29 19:14:00\nBusiness\nKar?chi\nUnknown Location\n3.0\nMeal/Entertain\n18:59\n19:14\n\n\n1143\n12/29/2016 20:53\n2016-12-29 21:42:00\nBusiness\nKar?chi\nUnknown Location\n6.4\nNaN\n20:53\n21:42\n\n\n1151\n12/31/2016 13:24\n2016-12-31 13:42:00\nBusiness\nKar?chi\nUnknown Location\n3.9\nTemporary Site\n13:24\n13:42\n\n\n\n\n63 rows × 9 columns\ndf = df[df['STOP']!='Unknown Location']\ndf\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n0\n01-01-2016 21:11\n2016-01-01 21:17:00\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n21:11\n21:17\n\n\n1\n01-02-2016 01:25\n2016-01-02 01:37:00\nBusiness\nFort Pierce\nFort Pierce\n5.0\nNaN\n01:25\n01:37\n\n\n2\n01-02-2016 20:25\n2016-01-02 20:38:00\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n20:25\n20:38\n\n\n3\n01-05-2016 17:31\n2016-01-05 17:45:00\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n17:31\n17:45\n\n\n4\n01-06-2016 14:42\n2016-01-06 15:49:00\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n14:42\n15:49\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1148\n12/30/2016 16:45\n2016-12-30 17:08:00\nBusiness\nKar?chi\nKar?chi\n4.6\nMeeting\n16:45\n17:08\n\n\n1149\n12/30/2016 23:06\n2016-12-30 23:10:00\nBusiness\nKar?chi\nKar?chi\n0.8\nCustomer Visit\n23:06\n23:10\n\n\n1150\n12/31/2016 1:07\n2016-12-31 01:14:00\nBusiness\nKar?chi\nKar?chi\n0.7\nMeeting\n1:07\n1:14\n\n\n1153\n12/31/2016 21:32\n2016-12-31 21:50:00\nBusiness\nKatunayake\nGampaha\n6.4\nTemporary Site\n21:32\n21:50\n\n\n1154\n12/31/2016 22:08\n2016-12-31 23:51:00\nBusiness\nGampaha\nIlukwatta\n48.2\nTemporary Site\n22:08\n23:51\n\n\n\n\n944 rows × 9 columns\ndf.shape\n\n(944, 9)\ndf\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n0\n01-01-2016 21:11\n2016-01-01 21:17:00\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n21:11\n21:17\n\n\n1\n01-02-2016 01:25\n2016-01-02 01:37:00\nBusiness\nFort Pierce\nFort Pierce\n5.0\nNaN\n01:25\n01:37\n\n\n2\n01-02-2016 20:25\n2016-01-02 20:38:00\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n20:25\n20:38\n\n\n3\n01-05-2016 17:31\n2016-01-05 17:45:00\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n17:31\n17:45\n\n\n4\n01-06-2016 14:42\n2016-01-06 15:49:00\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n14:42\n15:49\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1148\n12/30/2016 16:45\n2016-12-30 17:08:00\nBusiness\nKar?chi\nKar?chi\n4.6\nMeeting\n16:45\n17:08\n\n\n1149\n12/30/2016 23:06\n2016-12-30 23:10:00\nBusiness\nKar?chi\nKar?chi\n0.8\nCustomer Visit\n23:06\n23:10\n\n\n1150\n12/31/2016 1:07\n2016-12-31 01:14:00\nBusiness\nKar?chi\nKar?chi\n0.7\nMeeting\n1:07\n1:14\n\n\n1153\n12/31/2016 21:32\n2016-12-31 21:50:00\nBusiness\nKatunayake\nGampaha\n6.4\nTemporary Site\n21:32\n21:50\n\n\n1154\n12/31/2016 22:08\n2016-12-31 23:51:00\nBusiness\nGampaha\nIlukwatta\n48.2\nTemporary Site\n22:08\n23:51\n\n\n\n\n944 rows × 9 columns\nMILES\ndf['MILES'].isna().sum()\n\n0\nPURPOSE\ndf['PURPOSE'].isna().sum()\n\n372\ndf['PURPOSE'].unique()\n\narray(['Meal/Entertain', nan, 'Errand/Supplies', 'Meeting',\n       'Customer Visit', 'Temporary Site', 'Between Offices',\n       'Charity ($)', 'Commute', 'Moving', 'Airport/Travel'], dtype=object)\ndf['PURPOSE'] = df['PURPOSE'].fillna('Unknown')\ndf['PURPOSE'].value_counts()\n\nUnknown            372\nMeeting            164\nMeal/Entertain     148\nErrand/Supplies    111\nCustomer Visit      92\nTemporary Site      32\nBetween Offices     18\nMoving               4\nCharity ($)          1\nCommute              1\nAirport/Travel       1\nName: PURPOSE, dtype: int64\ndf.sample()\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n972\n11/18/2016 21:56\n2016-11-18 22:21:00\nBusiness\nKrendle Woods\nWhitebridge\n6.1\nMeeting\n21:56\n22:21"
  },
  {
    "objectID": "Homework/3/Unreleased/Homework 3 | Solutions.html#data-exploration",
    "href": "Homework/3/Unreleased/Homework 3 | Solutions.html#data-exploration",
    "title": "1. Data Processing",
    "section": "Data Exploration",
    "text": "Data Exploration\n\ndf['CATEGORY'].value_counts().plot.pie()\n\n&lt;AxesSubplot: ylabel='CATEGORY'&gt;\n\n\n\n\n\n\ndf['START'].value_counts().to_frame()\n\n\n\n\n\n\n\n\nSTART\n\n\n\n\nCary\n200\n\n\nMorrisville\n85\n\n\nWhitebridge\n68\n\n\nDurham\n37\n\n\nIslamabad\n29\n\n\n...\n...\n\n\nCoxville\n1\n\n\nLakeview\n1\n\n\nLower Garden District\n1\n\n\nConvention Center District\n1\n\n\nGampaha\n1\n\n\n\n\n175 rows × 1 columns\n\n\n\n\ndf['STOP'].value_counts().to_frame()\n\n\n\n\n\n\n\n\nSTOP\n\n\n\n\nCary\n203\n\n\nMorrisville\n83\n\n\nWhitebridge\n65\n\n\nDurham\n36\n\n\nIslamabad\n30\n\n\n...\n...\n\n\nDaytona Beach\n1\n\n\nSand Lake Commons\n1\n\n\nSky Lake\n1\n\n\nVista East\n1\n\n\nIlukwatta\n1\n\n\n\n\n187 rows × 1 columns\n\n\n\n\ndf\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n0\n01-01-2016 21:11\n2016-01-01 21:17:00\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n21:11\n21:17\n\n\n1\n01-02-2016 01:25\n2016-01-02 01:37:00\nBusiness\nFort Pierce\nFort Pierce\n5.0\nUnknown\n01:25\n01:37\n\n\n2\n01-02-2016 20:25\n2016-01-02 20:38:00\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n20:25\n20:38\n\n\n3\n01-05-2016 17:31\n2016-01-05 17:45:00\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n17:31\n17:45\n\n\n4\n01-06-2016 14:42\n2016-01-06 15:49:00\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n14:42\n15:49\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1148\n12/30/2016 16:45\n2016-12-30 17:08:00\nBusiness\nKar?chi\nKar?chi\n4.6\nMeeting\n16:45\n17:08\n\n\n1149\n12/30/2016 23:06\n2016-12-30 23:10:00\nBusiness\nKar?chi\nKar?chi\n0.8\nCustomer Visit\n23:06\n23:10\n\n\n1150\n12/31/2016 1:07\n2016-12-31 01:14:00\nBusiness\nKar?chi\nKar?chi\n0.7\nMeeting\n1:07\n1:14\n\n\n1153\n12/31/2016 21:32\n2016-12-31 21:50:00\nBusiness\nKatunayake\nGampaha\n6.4\nTemporary Site\n21:32\n21:50\n\n\n1154\n12/31/2016 22:08\n2016-12-31 23:51:00\nBusiness\nGampaha\nIlukwatta\n48.2\nTemporary Site\n22:08\n23:51\n\n\n\n\n944 rows × 9 columns\n\n\n\n\nplt.figure(figsize=(8, 6))\nplt.hist(df['MILES'], bins=10, edgecolor='black')\nplt.xlabel('Miles')\nplt.ylabel('Frequency')\nplt.title('Distribution of Miles Driven')\nplt.show()\n\n\n\n\n\npurpose_counts = df[df['PURPOSE']!='Unknown']['PURPOSE'].value_counts()\nplt.figure(figsize=(8, 6))\nplt.pie(purpose_counts.values, labels=purpose_counts.index, autopct='%1.1f%%')\nplt.title('Purpose of Trips')\nplt.show()\n\n\n\n\n\ndf = df[df['START_DATE'] != \"Totals\"]\n\ndf['START_DATE'] = pd.to_datetime(df['START_DATE'])\n\ndf.set_index('START_DATE', inplace=True)\n\ndaily_miles = df.resample('D')['MILES'].sum()\n\nplt.figure(figsize=(12, 6))\nplt.plot(daily_miles.index, daily_miles.values)\nplt.xlabel('Date')\nplt.ylabel('Miles Driven')\nplt.title('Miles Driven Over Time')\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nX_train\n\n\n\n\n\n\n\n\nSTART_Agnew\nSTART_Almond\nSTART_Apex\nSTART_Arabi\nSTART_Arlington\nSTART_Arlington Park at Amberly\nSTART_Asheville\nSTART_Austin\nSTART_Banner Elk\nSTART_Bellevue\n...\nPURPOSE_Between Offices\nPURPOSE_Charity ($)\nPURPOSE_Commute\nPURPOSE_Customer Visit\nPURPOSE_Errand/Supplies\nPURPOSE_Meal/Entertain\nPURPOSE_Meeting\nPURPOSE_Moving\nPURPOSE_Temporary Site\nPURPOSE_Unknown\n\n\nSTART_DATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-03-26 16:26:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2016-02-07 20:22:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-05-01 17:33:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2016-05-22 15:39:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2016-06-29 11:49:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2016-02-14 16:35:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-04-03 02:00:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-12-02 20:41:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2016-06-24 20:44:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2016-02-13 23:45:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n755 rows × 188 columns\n\n\n\n\nfeatures = ['START', 'CATEGORY', 'PURPOSE']\n\n\nX = df[features]\nX = pd.get_dummies(X) \ny = df['MILES']\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nX_val,X_test,y_val,y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\n\nprint(len(X_train),len(X_val),len(X_test))\n\n755 94 95\n\n\n\nX_train\n\n\n\n\n\n\n\n\nSTART_Agnew\nSTART_Almond\nSTART_Apex\nSTART_Arabi\nSTART_Arlington\nSTART_Arlington Park at Amberly\nSTART_Asheville\nSTART_Austin\nSTART_Banner Elk\nSTART_Bellevue\n...\nPURPOSE_Between Offices\nPURPOSE_Charity ($)\nPURPOSE_Commute\nPURPOSE_Customer Visit\nPURPOSE_Errand/Supplies\nPURPOSE_Meal/Entertain\nPURPOSE_Meeting\nPURPOSE_Moving\nPURPOSE_Temporary Site\nPURPOSE_Unknown\n\n\nSTART_DATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-03-26 16:26:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2016-02-07 20:22:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-05-01 17:33:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2016-05-22 15:39:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2016-06-29 11:49:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2016-02-14 16:35:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-04-03 02:00:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-12-02 20:41:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2016-06-24 20:44:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2016-02-13 23:45:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n755 rows × 188 columns\n\n\n\n\nmodel = LinearRegression(fit_intercept=True)\n\n\nmodel.fit(X_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_pred = model.predict(X_val)\nlen(y_pred)\n\n94\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(np.arange(len(y_val)), y_val, label='Actual Trend')\nplt.plot(np.arange(len(y_pred)), y_pred, label='Predicted Trend')\nplt.xlabel('Data Index')\nplt.ylabel('Trend')\nplt.title('Actual Trend vs. Predicted Trend')\nplt.legend()\nplt.show()\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R-squared Score: {r2}\")\n\nMean Squared Error: 2.6619837257798516e+27\nR-squared Score: -4.087381199893086e+24\n\n\n\nmodel.intercept_\n\n0.0\n\n\n\nfrom sklearn.linear_model import Ridge\n\n\nridge_model = Ridge()\n\n\nridge_model.fit(X_train,y_train)\n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\ny_pred = ridge_model.predict(X_val)\n\n\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R-squared Score: {r2}\")\n\nMean Squared Error: 657.8014876232421\nR-squared Score: -0.010030755535618496\n\n\n\nridge_model.alpha\n\n1.0\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(np.arange(len(y_test)), y_test, label='Actual Trend')\nplt.plot(np.arange(len(y_pred)), y_pred, label='Predicted Trend')\nplt.xlabel('Data Index')\nplt.ylabel('Trend')\nplt.title('Actual Trend vs. Predicted Trend')\nplt.legend()\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\n\n\nlasso_model = Lasso()\n\n\nlasso_model.fit(X_train,y_train)\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso()\n\n\n\ny_pred = lasso_model.predict(X_val)\n\n\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R-squared Score: {r2}\")\n\nMean Squared Error: 647.395188616323\nR-squared Score: 0.0059477459516085185"
  },
  {
    "objectID": "Homework/3/HW.html",
    "href": "Homework/3/HW.html",
    "title": "Homework 3",
    "section": "",
    "text": "Convert a raw data source into a version appropriate for downstream analysis using Python. | Week 4 & 5\nWrite appropriate visualizations for different sources and types of data | Week 6\nExplain why we seek to build machine learning models that generalize rather than memorize their input. | Week 6\nExplain the different uses for training, validation, and testing datasets | Week 6\nArticulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem | Week 6\n\n\n\n\n\nQuestions\nDataset . Source of the dataset and credit will be added once the submission window is closed."
  },
  {
    "objectID": "Homework/3/HW.html#tested-learning-outcome",
    "href": "Homework/3/HW.html#tested-learning-outcome",
    "title": "Homework 3",
    "section": "",
    "text": "Convert a raw data source into a version appropriate for downstream analysis using Python. | Week 4 & 5\nWrite appropriate visualizations for different sources and types of data | Week 6\nExplain why we seek to build machine learning models that generalize rather than memorize their input. | Week 6\nExplain the different uses for training, validation, and testing datasets | Week 6\nArticulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem | Week 6"
  },
  {
    "objectID": "Homework/3/HW.html#attachments",
    "href": "Homework/3/HW.html#attachments",
    "title": "Homework 3",
    "section": "",
    "text": "Questions\nDataset . Source of the dataset and credit will be added once the submission window is closed."
  },
  {
    "objectID": "Homework/2/Unreleased/Solutions_HW2_Enfa_George.html",
    "href": "Homework/2/Unreleased/Solutions_HW2_Enfa_George.html",
    "title": "Homework 2 : Data Collection",
    "section": "",
    "text": "# For web and html \nimport requests\nfrom time import sleep\nfrom bs4 import BeautifulSoup\n\n#For working with raw files\nimport zipfile\n\n#For working with data\nimport pandas as pd\nfrom datetime import datetime"
  },
  {
    "objectID": "Homework/2/Unreleased/Solutions_HW2_Enfa_George.html#problem-1-whats-the-secret-code",
    "href": "Homework/2/Unreleased/Solutions_HW2_Enfa_George.html#problem-1-whats-the-secret-code",
    "title": "Homework 2 : Data Collection",
    "section": "Problem 1 : What’s the secret code?",
    "text": "Problem 1 : What’s the secret code?\n\nQ 1. Use the library we learned in class to get the page’s HTML.\n\nBONUS_URL = \"https://csc380.beingenfa.com/Bonus/1.html\"\n\n\nbonus_req_obj = requests.get(BONUS_URL)\nbonus_req_obj\n\n&lt;Response [200]&gt;\n\n\n\nbonus_page_html = bonus_req_obj.text\nbonus_page_html\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\\n\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"generator\" content=\"quarto-1.3.361\"&gt;\\n\\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\\n\\n\\n&lt;title&gt;CSC 380 – quarto-input9d7f016f&lt;/title&gt;\\n&lt;style&gt;\\ncode{white-space: pre-wrap;}\\nspan.smallcaps{font-variant: small-caps;}\\ndiv.columns{display: flex; gap: min(4vw, 1.5em);}\\ndiv.column{flex: auto; overflow-x: auto;}\\ndiv.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}\\nul.task-list{list-style: none;}\\nul.task-list li input[type=\"checkbox\"] {\\n  width: 0.8em;\\n  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ \\n  vertical-align: middle;\\n}\\n&lt;/style&gt;\\n\\n\\n&lt;script src=\"../site_libs/quarto-nav/quarto-nav.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-nav/headroom.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/clipboard/clipboard.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/autocomplete.umd.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/fuse.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/quarto-search.js\"&gt;&lt;/script&gt;\\n&lt;meta name=\"quarto:offset\" content=\"../\"&gt;\\n&lt;script src=\"../site_libs/quarto-html/quarto.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/popper.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/tippy.umd.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/anchor.min.js\"&gt;&lt;/script&gt;\\n&lt;link href=\"../site_libs/quarto-html/tippy.css\" rel=\"stylesheet\"&gt;\\n&lt;link href=\"../site_libs/quarto-html/quarto-syntax-highlighting.css\" rel=\"stylesheet\" id=\"quarto-text-highlighting-styles\"&gt;\\n&lt;script src=\"../site_libs/bootstrap/bootstrap.min.js\"&gt;&lt;/script&gt;\\n&lt;link href=\"../site_libs/bootstrap/bootstrap-icons.css\" rel=\"stylesheet\"&gt;\\n&lt;link href=\"../site_libs/bootstrap/bootstrap.min.css\" rel=\"stylesheet\" id=\"quarto-bootstrap\" data-mode=\"light\"&gt;\\n&lt;script id=\"quarto-search-options\" type=\"application/json\"&gt;{\\n  \"location\": \"sidebar\",\\n  \"copy-button\": false,\\n  \"collapse-after\": 3,\\n  \"panel-placement\": \"start\",\\n  \"type\": \"textbox\",\\n  \"limit\": 20,\\n  \"language\": {\\n    \"search-no-results-text\": \"No results\",\\n    \"search-matching-documents-text\": \"matching documents\",\\n    \"search-copy-link-title\": \"Copy link to search\",\\n    \"search-hide-matches-text\": \"Hide additional matches\",\\n    \"search-more-match-text\": \"more match in this document\",\\n    \"search-more-matches-text\": \"more matches in this document\",\\n    \"search-clear-button-title\": \"Clear\",\\n    \"search-detached-cancel-button-title\": \"Cancel\",\\n    \"search-submit-button-title\": \"Submit\",\\n    \"search-label\": \"Search\"\\n  }\\n}&lt;/script&gt;\\n\\n\\n&lt;link rel=\"stylesheet\" href=\"../styles.css\"&gt;\\n&lt;/head&gt;\\n\\n&lt;body class=\"nav-sidebar docked\"&gt;\\n\\n&lt;div id=\"quarto-search-results\"&gt;&lt;/div&gt;\\n  &lt;header id=\"quarto-header\" class=\"headroom fixed-top\"&gt;\\n  &lt;nav class=\"quarto-secondary-nav\"&gt;\\n    &lt;div class=\"container-fluid d-flex\"&gt;\\n      &lt;button type=\"button\" class=\"quarto-btn-toggle btn\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;\\n        &lt;i class=\"bi bi-layout-text-sidebar-reverse\"&gt;&lt;/i&gt;\\n      &lt;/button&gt;\\n      &lt;nav class=\"quarto-page-breadcrumbs\" aria-label=\"breadcrumb\"&gt;&lt;ol class=\"breadcrumb\"&gt;&lt;li class=\"breadcrumb-item\"&gt;&lt;a href=\"../Bonus/1.html\"&gt;Bonus Questions&lt;/a&gt;&lt;/li&gt;&lt;li class=\"breadcrumb-item\"&gt;&lt;a href=\"../Bonus/1.html\"&gt;Sonam’s Dilemma: A Gift for Few or a Chance for All&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/nav&gt;\\n      &lt;a class=\"flex-grow-1\" role=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;      \\n      &lt;/a&gt;\\n      &lt;button type=\"button\" class=\"btn quarto-search-button\" aria-label=\"\" onclick=\"window.quartoOpenSearch();\"&gt;\\n        &lt;i class=\"bi bi-search\"&gt;&lt;/i&gt;\\n      &lt;/button&gt;\\n    &lt;/div&gt;\\n  &lt;/nav&gt;\\n&lt;/header&gt;\\n&lt;!-- content --&gt;\\n&lt;div id=\"quarto-content\" class=\"quarto-container page-columns page-rows-contents page-layout-article\"&gt;\\n&lt;!-- sidebar --&gt;\\n  &lt;nav id=\"quarto-sidebar\" class=\"sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto\"&gt;\\n    &lt;div class=\"pt-lg-2 mt-2 text-left sidebar-header\"&gt;\\n    &lt;div class=\"sidebar-title mb-0 py-0\"&gt;\\n      &lt;a href=\"../\"&gt;CSC 380&lt;/a&gt; \\n    &lt;/div&gt;\\n      &lt;/div&gt;\\n        &lt;div class=\"mt-2 flex-shrink-0 align-items-center\"&gt;\\n        &lt;div class=\"sidebar-search\"&gt;\\n        &lt;div id=\"quarto-search\" class=\"\" title=\"Search\"&gt;&lt;/div&gt;\\n        &lt;/div&gt;\\n        &lt;/div&gt;\\n    &lt;div class=\"sidebar-menu-container\"&gt; \\n    &lt;ul class=\"list-unstyled mt-1\"&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-1\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Course Content&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-1\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-1\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_1/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 1&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_2/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 2&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_3/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 3&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_4/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 4&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-2\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Homework&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-2\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-2\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Homework/1/HW.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;HW1: Probability&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Homework/2/HW.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;HW2: Data Collection&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-3\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Ethics Discussions&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-3\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-3\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_2.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W2: Political Content&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_3.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W3: Creative Work&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_4.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W4: Mental Health Support&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_5.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W5: Recommendations&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_6.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W6: Autonomous Driving&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_7.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W7: Housekeeping&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_8.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W8: Facial Recognition&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_9.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W9:ChatGPT and friends&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-4\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Bonus Questions&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-4\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-4\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Bonus/1.html\" class=\"sidebar-item-text sidebar-link active\"&gt;\\n &lt;span class=\"menu-text\"&gt;Sonam’s Dilemma: A Gift for Few or a Chance for All&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-5\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Syllabus&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-5\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-5\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Syllabus/Key_Info.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Key Info&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Syllabus/Syllabus.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Official Syllabus&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n    &lt;/ul&gt;\\n    &lt;/div&gt;\\n&lt;/nav&gt;\\n&lt;div id=\"quarto-sidebar-glass\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\"&gt;&lt;/div&gt;\\n&lt;!-- margin-sidebar --&gt;\\n    &lt;div id=\"quarto-margin-sidebar\" class=\"sidebar margin-sidebar\"&gt;\\n        &lt;nav id=\"TOC\" role=\"doc-toc\" class=\"toc-active\"&gt;\\n    &lt;h2 id=\"toc-title\"&gt;On this page&lt;/h2&gt;\\n   \\n  &lt;ul&gt;\\n  &lt;li&gt;&lt;a href=\"#sonams-dilemma---a-gift-for-few-or-a-chance-for-all\" id=\"toc-sonams-dilemma---a-gift-for-few-or-a-chance-for-all\" class=\"nav-link active\" data-scroll-target=\"#sonams-dilemma---a-gift-for-few-or-a-chance-for-all\"&gt;Sonam’s Dilemma - A Gift for Few or a Chance for All&lt;/a&gt;\\n  &lt;ul class=\"collapse\"&gt;\\n  &lt;li&gt;&lt;a href=\"#instructions\" id=\"toc-instructions\" class=\"nav-link\" data-scroll-target=\"#instructions\"&gt;Instructions&lt;/a&gt;&lt;/li&gt;\\n  &lt;li&gt;&lt;a href=\"#question\" id=\"toc-question\" class=\"nav-link\" data-scroll-target=\"#question\"&gt;Question&lt;/a&gt;&lt;/li&gt;\\n  &lt;/ul&gt;&lt;/li&gt;\\n  &lt;/ul&gt;\\n&lt;/nav&gt;\\n    &lt;/div&gt;\\n&lt;!-- main --&gt;\\n&lt;main class=\"content\" id=\"quarto-document-content\"&gt;\\n\\n\\n\\n&lt;section id=\"sonams-dilemma---a-gift-for-few-or-a-chance-for-all\" class=\"level1\"&gt;\\n&lt;h1&gt;Sonam’s Dilemma - A Gift for Few or a Chance for All&lt;/h1&gt;\\n&lt;section id=\"bonus-question-1-point\" class=\"level6\"&gt;\\n&lt;h6 class=\"anchored\" data-anchor-id=\"bonus-question-1-point\"&gt;Bonus Question : 1 point&lt;/h6&gt;\\n&lt;p&gt;&lt;br&gt;&lt;/p&gt;\\n&lt;/section&gt;\\n&lt;section id=\"instructions\" class=\"level3\"&gt;\\n&lt;h3 class=\"anchored\" data-anchor-id=\"instructions\"&gt;Instructions&lt;/h3&gt;\\n&lt;ul&gt;\\n&lt;li&gt;Due Date : July 7, Friday, 5 pm&lt;/li&gt;\\n&lt;li&gt;Submit your answer at D2 &gt; Quiz &gt; “Sonam’s Dilemma : a gift for few or a chance for all”&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/section&gt;\\n&lt;section id=\"question\" class=\"level3\"&gt;\\n&lt;h3 class=\"anchored\" data-anchor-id=\"question\"&gt;Question&lt;/h3&gt;\\n&lt;p&gt;&lt;br&gt; &lt;em&gt;Source : Story I made up, text improved by ChatGPT, and then edited by myself.&lt;/em&gt;&lt;/p&gt;\\n&lt;p&gt;Once upon a time, in the grand kingdom of Razia, there existed a mesmerizing lead dancer named Sonam. She possesed a combination of charm, extraordinary talent, and intelligence that made her stand out amongst her peers. Sonam’s passion for dance was matched only by her love for her dance group,with 2000 other talented dancers, who all shared an unbreakable bond and admired their noble Queen above all.&lt;/p&gt;\\n&lt;p&gt;One fine day, as Sonam’s heart overflowed with gratitude for their benevolent ruler, she decided to express her deep admiration through the power of music. She crafted a beautiful song that portrayed the love and adoration her dance group held for Queen Razia. With eager anticipation, Sonam’s troupe dedicated themselves to perfecting their dance moves to match the rhythm and emotion of her heartfelt composition.&lt;/p&gt;\\n&lt;p&gt;The auspicious occasion of Queen Razia’s birthday arrived, filling the grand palace with a sense of joy and celebration. The Queen, known for her discerning taste and appreciation for the arts, eagerly awaited the performance of Sonam’s dance group. As the music began to play and the dancers gracefully glided across the grand ballroom, a spell was cast upon all those in attendance.&lt;/p&gt;\\n&lt;p&gt;The Queen was captivated by the talent displayed before her, and her heart swelled with pride and delight. She could not help but marvel at the synchronized movements and the sheer passion that emanated from each and every member of the dance group. As the performance reached its crescendo, Queen Razia’s decision was made in an instant—this remarkable group deserved a gift to commemorate their extraordinary dedication.&lt;/p&gt;\\n&lt;div class=\"Secret\"&gt;\\n&lt;dr_cynthia_breazeal&gt;&lt;/dr_cynthia_breazeal&gt;\\n&lt;/div&gt;\\n&lt;p&gt;However, there were only a limited number of gift available to give out right now. She presented Sonam with two choices, each with its own intriguing possibilities. The first option was straightforward : 660 randomly chosen dancers would receive a gift. This meant that a select few would be rewarded, leaving the remaining members without a token of appreciation.&lt;/p&gt;\\n&lt;p&gt;The second choice, however, offered a twist. Queen Shah proposed distributing the available gifts among all dancers, but with a twist of uncertainty. Each dancer would have a 33% chance of receiving a gift and a 66% chance of going without.&lt;/p&gt;\\n&lt;p&gt;Now faced with this dilemma, Sonam’s sharp mind began to evaluate the situation. Which option should Sonam pick? Why? Why not the other?&lt;/p&gt;\\n\\n\\n&lt;/section&gt;\\n&lt;/section&gt;\\n\\n&lt;/main&gt; &lt;!-- /main --&gt;\\n&lt;script id=\"quarto-html-after-body\" type=\"application/javascript\"&gt;\\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\\n    const bodyEl = window.document.querySelector(\"body\");\\n    if (mode === \"dark\") {\\n      bodyEl.classList.add(\"quarto-dark\");\\n      bodyEl.classList.remove(\"quarto-light\");\\n    } else {\\n      bodyEl.classList.add(\"quarto-light\");\\n      bodyEl.classList.remove(\"quarto-dark\");\\n    }\\n  }\\n  const toggleBodyColorPrimary = () =&gt; {\\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\\n    if (bsSheetEl) {\\n      toggleBodyColorMode(bsSheetEl);\\n    }\\n  }\\n  toggleBodyColorPrimary();  \\n  const icon = \"\\ue9cb\";\\n  const anchorJS = new window.AnchorJS();\\n  anchorJS.options = {\\n    placement: \\'right\\',\\n    icon: icon\\n  };\\n  anchorJS.add(\\'.anchored\\');\\n  const isCodeAnnotation = (el) =&gt; {\\n    for (const clz of el.classList) {\\n      if (clz.startsWith(\\'code-annotation-\\')) {                     \\n        return true;\\n      }\\n    }\\n    return false;\\n  }\\n  const clipboard = new window.ClipboardJS(\\'.code-copy-button\\', {\\n    text: function(trigger) {\\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\\n      for (const childEl of codeEl.children) {\\n        if (isCodeAnnotation(childEl)) {\\n          childEl.remove();\\n        }\\n      }\\n      return codeEl.innerText;\\n    }\\n  });\\n  clipboard.on(\\'success\\', function(e) {\\n    // button target\\n    const button = e.trigger;\\n    // don\\'t keep focus\\n    button.blur();\\n    // flash \"checked\"\\n    button.classList.add(\\'code-copy-button-checked\\');\\n    var currentTitle = button.getAttribute(\"title\");\\n    button.setAttribute(\"title\", \"Copied!\");\\n    let tooltip;\\n    if (window.bootstrap) {\\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\\n      button.setAttribute(\"data-bs-placement\", \"left\");\\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\\n      tooltip = new bootstrap.Tooltip(button, \\n        { trigger: \"manual\", \\n          customClass: \"code-copy-button-tooltip\",\\n          offset: [0, -8]});\\n      tooltip.show();    \\n    }\\n    setTimeout(function() {\\n      if (tooltip) {\\n        tooltip.hide();\\n        button.removeAttribute(\"data-bs-title\");\\n        button.removeAttribute(\"data-bs-toggle\");\\n        button.removeAttribute(\"data-bs-placement\");\\n      }\\n      button.setAttribute(\"title\", currentTitle);\\n      button.classList.remove(\\'code-copy-button-checked\\');\\n    }, 1000);\\n    // clear code selection\\n    e.clearSelection();\\n  });\\n  function tippyHover(el, contentFn) {\\n    const config = {\\n      allowHTML: true,\\n      content: contentFn,\\n      maxWidth: 500,\\n      delay: 100,\\n      arrow: false,\\n      appendTo: function(el) {\\n          return el.parentElement;\\n      },\\n      interactive: true,\\n      interactiveBorder: 10,\\n      theme: \\'quarto\\',\\n      placement: \\'bottom-start\\'\\n    };\\n    window.tippy(el, config); \\n  }\\n  const noterefs = window.document.querySelectorAll(\\'a[role=\"doc-noteref\"]\\');\\n  for (var i=0; i&lt;noterefs.length; i++) {\\n    const ref = noterefs[i];\\n    tippyHover(ref, function() {\\n      // use id or data attribute instead here\\n      let href = ref.getAttribute(\\'data-footnote-href\\') || ref.getAttribute(\\'href\\');\\n      try { href = new URL(href).hash; } catch {}\\n      const id = href.replace(/^#\\\\/?/, \"\");\\n      const note = window.document.getElementById(id);\\n      return note.innerHTML;\\n    });\\n  }\\n      let selectedAnnoteEl;\\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\\n        let cellAttr = \\'data-code-cell=\"\\' + cell + \\'\"\\';\\n        let lineAttr = \\'data-code-annotation=\"\\' +  annotation + \\'\"\\';\\n        const selector = \\'span[\\' + cellAttr + \\'][\\' + lineAttr + \\']\\';\\n        return selector;\\n      }\\n      const selectCodeLines = (annoteEl) =&gt; {\\n        const doc = window.document;\\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\\n        const lineIds = lines.map((line) =&gt; {\\n          return targetCell + \"-\" + line;\\n        })\\n        let top = null;\\n        let height = null;\\n        let parent = null;\\n        if (lineIds.length &gt; 0) {\\n            //compute the position of the single el (top and bottom and make a div)\\n            const el = window.document.getElementById(lineIds[0]);\\n            top = el.offsetTop;\\n            height = el.offsetHeight;\\n            parent = el.parentElement.parentElement;\\n          if (lineIds.length &gt; 1) {\\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\\n            height = bottom - top;\\n          }\\n          if (top !== null && height !== null && parent !== null) {\\n            // cook up a div (if necessary) and position it \\n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\\n            if (div === null) {\\n              div = window.document.createElement(\"div\");\\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\\n              div.style.position = \\'absolute\\';\\n              parent.appendChild(div);\\n            }\\n            div.style.top = top - 2 + \"px\";\\n            div.style.height = height + 4 + \"px\";\\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\\n            if (gutterDiv === null) {\\n              gutterDiv = window.document.createElement(\"div\");\\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\\n              gutterDiv.style.position = \\'absolute\\';\\n              const codeCell = window.document.getElementById(targetCell);\\n              const gutter = codeCell.querySelector(\\'.code-annotation-gutter\\');\\n              gutter.appendChild(gutterDiv);\\n            }\\n            gutterDiv.style.top = top - 2 + \"px\";\\n            gutterDiv.style.height = height + 4 + \"px\";\\n          }\\n          selectedAnnoteEl = annoteEl;\\n        }\\n      };\\n      const unselectCodeLines = () =&gt; {\\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\\n        elementsIds.forEach((elId) =&gt; {\\n          const div = window.document.getElementById(elId);\\n          if (div) {\\n            div.remove();\\n          }\\n        });\\n        selectedAnnoteEl = undefined;\\n      };\\n      // Attach click handler to the DT\\n      const annoteDls = window.document.querySelectorAll(\\'dt[data-target-cell]\\');\\n      for (const annoteDlNode of annoteDls) {\\n        annoteDlNode.addEventListener(\\'click\\', (event) =&gt; {\\n          const clickedEl = event.target;\\n          if (clickedEl !== selectedAnnoteEl) {\\n            unselectCodeLines();\\n            const activeEl = window.document.querySelector(\\'dt[data-target-cell].code-annotation-active\\');\\n            if (activeEl) {\\n              activeEl.classList.remove(\\'code-annotation-active\\');\\n            }\\n            selectCodeLines(clickedEl);\\n            clickedEl.classList.add(\\'code-annotation-active\\');\\n          } else {\\n            // Unselect the line\\n            unselectCodeLines();\\n            clickedEl.classList.remove(\\'code-annotation-active\\');\\n          }\\n        });\\n      }\\n  const findCites = (el) =&gt; {\\n    const parentEl = el.parentElement;\\n    if (parentEl) {\\n      const cites = parentEl.dataset.cites;\\n      if (cites) {\\n        return {\\n          el,\\n          cites: cites.split(\\' \\')\\n        };\\n      } else {\\n        return findCites(el.parentElement)\\n      }\\n    } else {\\n      return undefined;\\n    }\\n  };\\n  var bibliorefs = window.document.querySelectorAll(\\'a[role=\"doc-biblioref\"]\\');\\n  for (var i=0; i&lt;bibliorefs.length; i++) {\\n    const ref = bibliorefs[i];\\n    const citeInfo = findCites(ref);\\n    if (citeInfo) {\\n      tippyHover(citeInfo.el, function() {\\n        var popup = window.document.createElement(\\'div\\');\\n        citeInfo.cites.forEach(function(cite) {\\n          var citeDiv = window.document.createElement(\\'div\\');\\n          citeDiv.classList.add(\\'hanging-indent\\');\\n          citeDiv.classList.add(\\'csl-entry\\');\\n          var biblioDiv = window.document.getElementById(\\'ref-\\' + cite);\\n          if (biblioDiv) {\\n            citeDiv.innerHTML = biblioDiv.innerHTML;\\n          }\\n          popup.appendChild(citeDiv);\\n        });\\n        return popup.innerHTML;\\n      });\\n    }\\n  }\\n});\\n&lt;/script&gt;\\n&lt;/div&gt; &lt;!-- /content --&gt;\\n\\n\\n\\n&lt;/body&gt;&lt;/html&gt;'\n\n\n\n\nQ 2. Find the section with the secret code by using the Beautiful Soup’s find function\n\nbonus_bs4_obj = BeautifulSoup(bonus_page_html,\"html.parser\")\nbonus_bs4_obj.prettify\n\n&lt;bound method Tag.prettify of &lt;!DOCTYPE html&gt;\n\n&lt;html lang=\"en\" xml:lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;head&gt;\n&lt;meta charset=\"utf-8\"/&gt;\n&lt;meta content=\"quarto-1.3.361\" name=\"generator\"/&gt;\n&lt;meta content=\"width=device-width, initial-scale=1.0, user-scalable=yes\" name=\"viewport\"/&gt;\n&lt;title&gt;CSC 380 – quarto-input9d7f016f&lt;/title&gt;\n&lt;style&gt;\ncode{white-space: pre-wrap;}\nspan.smallcaps{font-variant: small-caps;}\ndiv.columns{display: flex; gap: min(4vw, 1.5em);}\ndiv.column{flex: auto; overflow-x: auto;}\ndiv.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}\nul.task-list{list-style: none;}\nul.task-list li input[type=\"checkbox\"] {\n  width: 0.8em;\n  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ \n  vertical-align: middle;\n}\n&lt;/style&gt;\n&lt;script src=\"../site_libs/quarto-nav/quarto-nav.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-nav/headroom.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/clipboard/clipboard.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-search/autocomplete.umd.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-search/fuse.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-search/quarto-search.js\"&gt;&lt;/script&gt;\n&lt;meta content=\"../\" name=\"quarto:offset\"/&gt;\n&lt;script src=\"../site_libs/quarto-html/quarto.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-html/popper.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-html/tippy.umd.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-html/anchor.min.js\"&gt;&lt;/script&gt;\n&lt;link href=\"../site_libs/quarto-html/tippy.css\" rel=\"stylesheet\"/&gt;\n&lt;link href=\"../site_libs/quarto-html/quarto-syntax-highlighting.css\" id=\"quarto-text-highlighting-styles\" rel=\"stylesheet\"/&gt;\n&lt;script src=\"../site_libs/bootstrap/bootstrap.min.js\"&gt;&lt;/script&gt;\n&lt;link href=\"../site_libs/bootstrap/bootstrap-icons.css\" rel=\"stylesheet\"/&gt;\n&lt;link data-mode=\"light\" href=\"../site_libs/bootstrap/bootstrap.min.css\" id=\"quarto-bootstrap\" rel=\"stylesheet\"/&gt;\n&lt;script id=\"quarto-search-options\" type=\"application/json\"&gt;{\n  \"location\": \"sidebar\",\n  \"copy-button\": false,\n  \"collapse-after\": 3,\n  \"panel-placement\": \"start\",\n  \"type\": \"textbox\",\n  \"limit\": 20,\n  \"language\": {\n    \"search-no-results-text\": \"No results\",\n    \"search-matching-documents-text\": \"matching documents\",\n    \"search-copy-link-title\": \"Copy link to search\",\n    \"search-hide-matches-text\": \"Hide additional matches\",\n    \"search-more-match-text\": \"more match in this document\",\n    \"search-more-matches-text\": \"more matches in this document\",\n    \"search-clear-button-title\": \"Clear\",\n    \"search-detached-cancel-button-title\": \"Cancel\",\n    \"search-submit-button-title\": \"Submit\",\n    \"search-label\": \"Search\"\n  }\n}&lt;/script&gt;\n&lt;link href=\"../styles.css\" rel=\"stylesheet\"/&gt;\n&lt;/head&gt;\n&lt;body class=\"nav-sidebar docked\"&gt;\n&lt;div id=\"quarto-search-results\"&gt;&lt;/div&gt;\n&lt;header class=\"headroom fixed-top\" id=\"quarto-header\"&gt;\n&lt;nav class=\"quarto-secondary-nav\"&gt;\n&lt;div class=\"container-fluid d-flex\"&gt;\n&lt;button aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" class=\"quarto-btn-toggle btn\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" data-bs-toggle=\"collapse\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\" type=\"button\"&gt;\n&lt;i class=\"bi bi-layout-text-sidebar-reverse\"&gt;&lt;/i&gt;\n&lt;/button&gt;\n&lt;nav aria-label=\"breadcrumb\" class=\"quarto-page-breadcrumbs\"&gt;&lt;ol class=\"breadcrumb\"&gt;&lt;li class=\"breadcrumb-item\"&gt;&lt;a href=\"../Bonus/1.html\"&gt;Bonus Questions&lt;/a&gt;&lt;/li&gt;&lt;li class=\"breadcrumb-item\"&gt;&lt;a href=\"../Bonus/1.html\"&gt;Sonam’s Dilemma: A Gift for Few or a Chance for All&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/nav&gt;\n&lt;a aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" class=\"flex-grow-1\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" data-bs-toggle=\"collapse\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\" role=\"button\"&gt;\n&lt;/a&gt;\n&lt;button aria-label=\"\" class=\"btn quarto-search-button\" onclick=\"window.quartoOpenSearch();\" type=\"button\"&gt;\n&lt;i class=\"bi bi-search\"&gt;&lt;/i&gt;\n&lt;/button&gt;\n&lt;/div&gt;\n&lt;/nav&gt;\n&lt;/header&gt;\n&lt;!-- content --&gt;\n&lt;div class=\"quarto-container page-columns page-rows-contents page-layout-article\" id=\"quarto-content\"&gt;\n&lt;!-- sidebar --&gt;\n&lt;nav class=\"sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto\" id=\"quarto-sidebar\"&gt;\n&lt;div class=\"pt-lg-2 mt-2 text-left sidebar-header\"&gt;\n&lt;div class=\"sidebar-title mb-0 py-0\"&gt;\n&lt;a href=\"../\"&gt;CSC 380&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"mt-2 flex-shrink-0 align-items-center\"&gt;\n&lt;div class=\"sidebar-search\"&gt;\n&lt;div class=\"\" id=\"quarto-search\" title=\"Search\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"sidebar-menu-container\"&gt;\n&lt;ul class=\"list-unstyled mt-1\"&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-1\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Course Content&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-1\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-1\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Course_Content/Week_1/home.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Week 1&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Course_Content/Week_2/home.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Week 2&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Course_Content/Week_3/home.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Week 3&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Course_Content/Week_4/home.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Week 4&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-2\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Homework&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-2\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-2\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Homework/1/HW.html\"&gt;\n&lt;span class=\"menu-text\"&gt;HW1: Probability&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Homework/2/HW.html\"&gt;\n&lt;span class=\"menu-text\"&gt;HW2: Data Collection&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-3\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Ethics Discussions&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-3\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-3\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_2.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W2: Political Content&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_3.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W3: Creative Work&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_4.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W4: Mental Health Support&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_5.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W5: Recommendations&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_6.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W6: Autonomous Driving&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_7.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W7: Housekeeping&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_8.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W8: Facial Recognition&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_9.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W9:ChatGPT and friends&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-4\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Bonus Questions&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-4\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-4\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link active\" href=\"../Bonus/1.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Sonam’s Dilemma: A Gift for Few or a Chance for All&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-5\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Syllabus&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-5\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-5\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Syllabus/Key_Info.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Key Info&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Syllabus/Syllabus.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Official Syllabus&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/nav&gt;\n&lt;div data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" data-bs-toggle=\"collapse\" id=\"quarto-sidebar-glass\"&gt;&lt;/div&gt;\n&lt;!-- margin-sidebar --&gt;\n&lt;div class=\"sidebar margin-sidebar\" id=\"quarto-margin-sidebar\"&gt;\n&lt;nav class=\"toc-active\" id=\"TOC\" role=\"doc-toc\"&gt;\n&lt;h2 id=\"toc-title\"&gt;On this page&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a class=\"nav-link active\" data-scroll-target=\"#sonams-dilemma---a-gift-for-few-or-a-chance-for-all\" href=\"#sonams-dilemma---a-gift-for-few-or-a-chance-for-all\" id=\"toc-sonams-dilemma---a-gift-for-few-or-a-chance-for-all\"&gt;Sonam’s Dilemma - A Gift for Few or a Chance for All&lt;/a&gt;\n&lt;ul class=\"collapse\"&gt;\n&lt;li&gt;&lt;a class=\"nav-link\" data-scroll-target=\"#instructions\" href=\"#instructions\" id=\"toc-instructions\"&gt;Instructions&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a class=\"nav-link\" data-scroll-target=\"#question\" href=\"#question\" id=\"toc-question\"&gt;Question&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/nav&gt;\n&lt;/div&gt;\n&lt;!-- main --&gt;\n&lt;main class=\"content\" id=\"quarto-document-content\"&gt;\n&lt;section class=\"level1\" id=\"sonams-dilemma---a-gift-for-few-or-a-chance-for-all\"&gt;\n&lt;h1&gt;Sonam’s Dilemma - A Gift for Few or a Chance for All&lt;/h1&gt;\n&lt;section class=\"level6\" id=\"bonus-question-1-point\"&gt;\n&lt;h6 class=\"anchored\" data-anchor-id=\"bonus-question-1-point\"&gt;Bonus Question : 1 point&lt;/h6&gt;\n&lt;p&gt;&lt;br/&gt;&lt;/p&gt;\n&lt;/section&gt;\n&lt;section class=\"level3\" id=\"instructions\"&gt;\n&lt;h3 class=\"anchored\" data-anchor-id=\"instructions\"&gt;Instructions&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;Due Date : July 7, Friday, 5 pm&lt;/li&gt;\n&lt;li&gt;Submit your answer at D2 &gt; Quiz &gt; “Sonam’s Dilemma : a gift for few or a chance for all”&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section class=\"level3\" id=\"question\"&gt;\n&lt;h3 class=\"anchored\" data-anchor-id=\"question\"&gt;Question&lt;/h3&gt;\n&lt;p&gt;&lt;br/&gt; &lt;em&gt;Source : Story I made up, text improved by ChatGPT, and then edited by myself.&lt;/em&gt;&lt;/p&gt;\n&lt;p&gt;Once upon a time, in the grand kingdom of Razia, there existed a mesmerizing lead dancer named Sonam. She possesed a combination of charm, extraordinary talent, and intelligence that made her stand out amongst her peers. Sonam’s passion for dance was matched only by her love for her dance group,with 2000 other talented dancers, who all shared an unbreakable bond and admired their noble Queen above all.&lt;/p&gt;\n&lt;p&gt;One fine day, as Sonam’s heart overflowed with gratitude for their benevolent ruler, she decided to express her deep admiration through the power of music. She crafted a beautiful song that portrayed the love and adoration her dance group held for Queen Razia. With eager anticipation, Sonam’s troupe dedicated themselves to perfecting their dance moves to match the rhythm and emotion of her heartfelt composition.&lt;/p&gt;\n&lt;p&gt;The auspicious occasion of Queen Razia’s birthday arrived, filling the grand palace with a sense of joy and celebration. The Queen, known for her discerning taste and appreciation for the arts, eagerly awaited the performance of Sonam’s dance group. As the music began to play and the dancers gracefully glided across the grand ballroom, a spell was cast upon all those in attendance.&lt;/p&gt;\n&lt;p&gt;The Queen was captivated by the talent displayed before her, and her heart swelled with pride and delight. She could not help but marvel at the synchronized movements and the sheer passion that emanated from each and every member of the dance group. As the performance reached its crescendo, Queen Razia’s decision was made in an instant—this remarkable group deserved a gift to commemorate their extraordinary dedication.&lt;/p&gt;\n&lt;div class=\"Secret\"&gt;\n&lt;dr_cynthia_breazeal&gt;&lt;/dr_cynthia_breazeal&gt;\n&lt;/div&gt;\n&lt;p&gt;However, there were only a limited number of gift available to give out right now. She presented Sonam with two choices, each with its own intriguing possibilities. The first option was straightforward : 660 randomly chosen dancers would receive a gift. This meant that a select few would be rewarded, leaving the remaining members without a token of appreciation.&lt;/p&gt;\n&lt;p&gt;The second choice, however, offered a twist. Queen Shah proposed distributing the available gifts among all dancers, but with a twist of uncertainty. Each dancer would have a 33% chance of receiving a gift and a 66% chance of going without.&lt;/p&gt;\n&lt;p&gt;Now faced with this dilemma, Sonam’s sharp mind began to evaluate the situation. Which option should Sonam pick? Why? Why not the other?&lt;/p&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id=\"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n&lt;/body&gt;&lt;/html&gt;&gt;\n\n\n\nsecret_section = bonus_bs4_obj.find(\"div\",\"Secret\")\nsecret_section\n\n&lt;div class=\"Secret\"&gt;\n&lt;dr_cynthia_breazeal&gt;&lt;/dr_cynthia_breazeal&gt;\n&lt;/div&gt;\n\n\n\n\nQ 3. Clean up the secret code and print it as “The Secret Code is: CSC380”\n\nlist(secret_section.children)\n\n['\\n', &lt;dr_cynthia_breazeal&gt;&lt;/dr_cynthia_breazeal&gt;, '\\n']\n\n\n\nsecret_code_tag = list(secret_section.children)[1]\nsecret_code_tag\n\n&lt;dr_cynthia_breazeal&gt;&lt;/dr_cynthia_breazeal&gt;\n\n\n\nsecret_code = secret_code_tag.name\nsecret_code\n\n'dr_cynthia_breazeal'\n\n\n\nprint(\"The Secret Code is: \",secret_code)\n\nThe Secret Code is:  dr_cynthia_breazeal"
  },
  {
    "objectID": "Homework/2/Unreleased/Solutions_HW2_Enfa_George.html#problem-2.-random-facts-api",
    "href": "Homework/2/Unreleased/Solutions_HW2_Enfa_George.html#problem-2.-random-facts-api",
    "title": "Homework 2 : Data Collection",
    "section": "Problem 2. Random Facts API",
    "text": "Problem 2. Random Facts API\n\nRANDOM_FACT_WEBSITE_URL = \"https://uselessfacts.jsph.pl\"\nRANDOM_FACTS_ENDPOINT = \"/api/v2/facts/random\"\nTODAY_RANDOM_FACT_ENDPOINT = \"/api/v2/facts/today\"\n\n\nQ1. Find the URL for random facts API.\n\nrandom_facts_api = RANDOM_FACT_WEBSITE_URL+RANDOM_FACTS_ENDPOINT\nrandom_facts_api\n\n'https://uselessfacts.jsph.pl/api/v2/facts/random'\n\n\n\n\nQs. 2,3,4 . Collect 10 Random Facts\n\nrandom_facts_list_of_json = []\nnos_of_facts = 10\n\nfor fact_no in range(nos_of_facts):\n    \n    random_fact_req_obj = requests.get(random_facts_api)\n    \n    if random_fact_req_obj.status_code == 200:\n        \n        random_facts_list_of_json.append(random_fact_req_obj.json())\n        \n    sleep(2)\n\n\nlen(random_facts_list_of_json)\n\n10\n\n\n\nrandom_facts_list_of_json\n\n[{'id': '160bb76999759540891142babca2eb81',\n  'text': 'A duck`s quack doesn`t echo, and no one knows why.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/160bb76999759540891142babca2eb81'},\n {'id': '2dcf6a99cfdeedc526fa31128043c547',\n  'text': 'Intelligent people have more zinc and copper in their hair.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/2dcf6a99cfdeedc526fa31128043c547'},\n {'id': '76d9063d8bc889cc22659c46a05ac562',\n  'text': 'PEZ candy even comes in a Coffee flavor.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/76d9063d8bc889cc22659c46a05ac562'},\n {'id': '748c08a0310dd6f69b410add8fb0886f',\n  'text': 'Only 6 people in the whole world have died from moshing.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/748c08a0310dd6f69b410add8fb0886f'},\n {'id': 'e02d2e9a0bc1c19edd341bd53fd7aebc',\n  'text': 'The average North American will eat 35,000 cookies during their life span.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/e02d2e9a0bc1c19edd341bd53fd7aebc'},\n {'id': '06490161dca519171f50e98a22861b2e',\n  'text': 'During a severe windstorm or rainstorm the Empire State Building sways several feet to either side.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/06490161dca519171f50e98a22861b2e'},\n {'id': 'a313d6861427c089d43b14ea32472e1f',\n  'text': 'Donald Duck comics were banned from Finland because he doesn`t wear pants!',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/a313d6861427c089d43b14ea32472e1f'},\n {'id': 'a79df1c3d4ee87fa1c6ffb798cb6c3d1',\n  'text': 'Approximately every seven minutes of every day, someone in an aerobics class pulls their hamstring.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/a79df1c3d4ee87fa1c6ffb798cb6c3d1'},\n {'id': '4dba7ea68b06030dfd95fc40e3b73fa1',\n  'text': 'James Bond`s car had three different license plates in Goldfinger',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/4dba7ea68b06030dfd95fc40e3b73fa1'},\n {'id': '5db4693ed3350b11ddd4366783cc8eec',\n  'text': 'There are more than 10 million bricks in the Empire State Building!',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/5db4693ed3350b11ddd4366783cc8eec'}]\n\n\n\n\nQ.5 Creating the Dataframe\n\nrandom_facts_df = pd.DataFrame(random_facts_list_of_json)\nrandom_facts_df.sample(2)\n\n\n\n\n\n\n\n\nid\ntext\nsource\nsource_url\nlanguage\npermalink\n\n\n\n\n9\n5db4693ed3350b11ddd4366783cc8eec\nThere are more than 10 million bricks in the E...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/5db4...\n\n\n1\n2dcf6a99cfdeedc526fa31128043c547\nIntelligent people have more zinc and copper i...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/2dcf...\n\n\n\n\n\n\n\n\nrandom_facts_df\n\n\n\n\n\n\n\n\nid\ntext\nsource\nsource_url\nlanguage\npermalink\n\n\n\n\n0\n160bb76999759540891142babca2eb81\nA duck`s quack doesn`t echo, and no one knows ...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/160b...\n\n\n1\n2dcf6a99cfdeedc526fa31128043c547\nIntelligent people have more zinc and copper i...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/2dcf...\n\n\n2\n76d9063d8bc889cc22659c46a05ac562\nPEZ candy even comes in a Coffee flavor.\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/76d9...\n\n\n3\n748c08a0310dd6f69b410add8fb0886f\nOnly 6 people in the whole world have died fro...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/748c...\n\n\n4\ne02d2e9a0bc1c19edd341bd53fd7aebc\nThe average North American will eat 35,000 coo...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/e02d...\n\n\n5\n06490161dca519171f50e98a22861b2e\nDuring a severe windstorm or rainstorm the Emp...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/0649...\n\n\n6\na313d6861427c089d43b14ea32472e1f\nDonald Duck comics were banned from Finland be...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/a313...\n\n\n7\na79df1c3d4ee87fa1c6ffb798cb6c3d1\nApproximately every seven minutes of every day...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/a79d...\n\n\n8\n4dba7ea68b06030dfd95fc40e3b73fa1\nJames Bond`s car had three different license p...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/4dba...\n\n\n9\n5db4693ed3350b11ddd4366783cc8eec\nThere are more than 10 million bricks in the E...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/5db4...\n\n\n\n\n\n\n\n\n\nQ 6. Display Full Facts\n\nrandom_facts_series = random_facts_df['text'] # Part a\nrandom_facts_series\n\n0    A duck`s quack doesn`t echo, and no one knows ...\n1    Intelligent people have more zinc and copper i...\n2             PEZ candy even comes in a Coffee flavor.\n3    Only 6 people in the whole world have died fro...\n4    The average North American will eat 35,000 coo...\n5    During a severe windstorm or rainstorm the Emp...\n6    Donald Duck comics were banned from Finland be...\n7    Approximately every seven minutes of every day...\n8    James Bond`s car had three different license p...\n9    There are more than 10 million bricks in the E...\nName: text, dtype: object\n\n\n\nrandom_facts_list = random_facts_series.to_list() # Part b\nrandom_facts_list\n\n['A duck`s quack doesn`t echo, and no one knows why.',\n 'Intelligent people have more zinc and copper in their hair.',\n 'PEZ candy even comes in a Coffee flavor.',\n 'Only 6 people in the whole world have died from moshing.',\n 'The average North American will eat 35,000 cookies during their life span.',\n 'During a severe windstorm or rainstorm the Empire State Building sways several feet to either side.',\n 'Donald Duck comics were banned from Finland because he doesn`t wear pants!',\n 'Approximately every seven minutes of every day, someone in an aerobics class pulls their hamstring.',\n 'James Bond`s car had three different license plates in Goldfinger',\n 'There are more than 10 million bricks in the Empire State Building!']\n\n\n\n\nQ 7. Show 3 random facts from the data frame\n\nrandom_facts_df.sample(3)\n\n\n\n\n\n\n\n\nid\ntext\nsource\nsource_url\nlanguage\npermalink\n\n\n\n\n3\n748c08a0310dd6f69b410add8fb0886f\nOnly 6 people in the whole world have died fro...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/748c...\n\n\n2\n76d9063d8bc889cc22659c46a05ac562\nPEZ candy even comes in a Coffee flavor.\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/76d9...\n\n\n8\n4dba7ea68b06030dfd95fc40e3b73fa1\nJames Bond`s car had three different license p...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/4dba...\n\n\n\n\n\n\n\n\n\nQ8. What is today’s random fact?\n\nPart a\n\ntodays_random_fact_api = RANDOM_FACT_WEBSITE_URL+ TODAY_RANDOM_FACT_ENDPOINT\ntodays_random_fact_api\n\n'https://uselessfacts.jsph.pl/api/v2/facts/today'\n\n\n\ntoday_fact_req_obj = requests.get(todays_random_fact_api)\ntoday_fact_req_obj.status_code\n\n200\n\n\n\nrandom_fact_of_the_day = today_fact_req_obj.json()['text'] #Part a\nrandom_fact_of_the_day\n\n'Napoleon`s penis was sold to an American Urologist for $40,000.'\n\n\n\n\nPart b\n\ntime_rn = datetime.today()\nprint('Time right now is :',time_rn.strftime(\"%Y-%m-%d %I:%M:%S %p\"))\n\nTime right now is : 2023-07-06 06:36:36 PM\n\n\n\n\nPart c\n\nprint(\"At\", time_rn.strftime(\"%Y-%m-%d %I:%M:%S %p\"), \" the random fact of the day is \",random_fact_of_the_day)\n\nAt 2023-07-06 06:36:36 PM  the random fact of the day is  Napoleon`s penis was sold to an American Urologist for $40,000."
  },
  {
    "objectID": "Homework/2/Unreleased/Solutions_HW2_Enfa_George.html#part-3.-movies-and-shows",
    "href": "Homework/2/Unreleased/Solutions_HW2_Enfa_George.html#part-3.-movies-and-shows",
    "title": "Homework 2 : Data Collection",
    "section": "Part 3. Movies and Shows",
    "text": "Part 3. Movies and Shows\n\nQ1. Download the following dataset\n\ndataset_names = ['hulu','disney','prime','netflix']\n\n\nfor dataset_name in dataset_names:\n    with zipfile.ZipFile(dataset_name+\".zip\",\"r\") as zip_ref:\n        zip_ref.extractall(dataset_name)\n\n\n\nQ2. Create one large dataframe\n\nPart a\n\nhulu_df = pd.read_csv('hulu/hulu_titles.csv')\nhulu_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n930\ns931\nMovie\nJust Married\nNaN\nNaN\nNaN\nMarch 1, 2021\n2003\nPG-13\n95 min\nComedy, Drama, Romance\nAshton Kutcher and Brittany Murphy star as two...\n\n\n\n\n\n\n\n\nnetflix_df = pd.read_csv('netflix/netflix_titles.csv')\nnetflix_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n1482\ns1483\nTV Show\nTransformers: War for Cybertron: Earthrise\nNaN\nJake Foushee, Jason Marnocha, Linsay Rousseau,...\nNaN\nDecember 30, 2020\n2020\nTV-Y7\n1 Season\nAnime Series\nWhile Megatron takes drastic measures to save ...\n\n\n\n\n\n\n\n\nprime_df = pd.read_csv('prime/amazon_prime_titles.csv')\nprime_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n2395\ns2396\nMovie\nGallagher: That's Stupid\nGallagher\nGallagher\nNaN\nNaN\n1982\n16+\n63 min\nComedy, Special Interest\nIn 1982 Gallagher had a lot on his mind. Forev...\n\n\n\n\n\n\n\n\ndisney_df = pd.read_csv('disney/disney_plus_titles.csv')\ndisney_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n366\ns367\nMovie\nHome Alone 3\nRaja Gosnell\nAlex D. Linz, Haviland Morris, Olek Krupa\nUnited States\nNovember 13, 2020\n1997\nPG\n103 min\nComedy, Family\nA gang of criminals come up against 8 year-old...\n\n\n\n\n\n\n\n\n\nPart b\n\nhulu_df['Platform'] = \"Hulu\"\nnetflix_df['Platform'] = \"Netflix\"\ndisney_df['Platform'] = \"Disney\"\nprime_df['Platform'] = \"Prime\"\n\n\nhulu_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n2042\ns2043\nTV Show\nWe Never Learn: BOKUBEN\nNaN\nNaN\nJapan\nOctober 5, 2019\n2019\nTV-14\n2 Seasons\nAnime, Comedy, Romance\nNariyuki Yuiga is in his last and most painful...\nHulu\n\n\n\n\n\n\n\n\nnetflix_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n7069\ns7070\nTV Show\nIndia: Nature's Wonderland\nNaN\nLiz Bonnin, Jon Gupta, Freida Pinto\nUnited Kingdom\nMarch 1, 2017\n2015\nTV-PG\n1 Season\nBritish TV Shows, Docuseries, Science & Nature TV\nWildlife biologist Liz Bonnin explores the nat...\nNetflix\n\n\n\n\n\n\n\n\ndisney_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n274\ns275\nTV Show\nThe Mighty Ducks: Game Changers\nNaN\nNaN\nNaN\nMarch 26, 2021\n2021\nTV-PG\n1 Season\nComedy, Drama, Sports\nA new group of misfits rediscovers the joys of...\nDisney\n\n\n\n\n\n\n\n\nprime_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n7726\ns7727\nMovie\nKarma Cartel\nVinod Bharathan\nVinay Forrt, Sabumon Abdusamad, Jinu Joseph, T...\nIndia, Denmark\nNaN\n2014\n13+\n101 min\nDrama, Suspense\nA multi-award-winning, avant-garde film set In...\nPrime\n\n\n\n\n\n\n\n\n\nPart c\n\nall_platforms_df = pd.concat([prime_df,netflix_df,disney_df,hulu_df])\nall_platforms_df.sample(5)\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n8448\ns8449\nMovie\nThe Peacemaker\nMimi Leder\nGeorge Clooney, Nicole Kidman, Marcel Iures, A...\nUnited States\nJanuary 1, 2020\n1997\nR\n124 min\nAction & Adventure\nAfter terrorists trigger a nuclear blast in Ru...\nNetflix\n\n\n7166\ns7167\nTV Show\nLove And War\nNaN\nJay Thomas, Annie Potts, Joanna Gleaon\nNaN\nAugust 6, 2021\n1995\nTV-14\n1 Season\nTV Shows\nJack and Dana may have finally gotten together...\nPrime\n\n\n672\ns673\nTV Show\nThe Face UK\nNaN\nNaomi Campbell, Erin O'Connor, Caroline Winberg\nNaN\nNaN\n2013\n13+\n1 Season\nTV Shows\nThe Face will see three fashion icons compete ...\nPrime\n\n\n1179\ns1180\nMovie\nRamen Heads\nNaN\nNaN\nJapan\nNovember 15, 2020\n2017\nPG\n93 min\nDocumentaries\nJapan's king of ramen opens his kitchen and fi...\nHulu\n\n\n1114\ns1115\nMovie\n3 Idiots\nRajkumar Hirani\nAamir Khan, Kareena Kapoor, Madhavan, Sharman ...\nIndia\nApril 1, 2021\n2009\nPG-13\n164 min\nComedies, Dramas, International Movies\nWhile attending one of India's premier college...\nNetflix\n\n\n\n\n\n\n\n\n\n\nQ3. Longest show and movie\n\nPart a\n\nshows_df = all_platforms_df[all_platforms_df['type']=='TV Show']\nshows_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n1192\ns1193\nTV Show\nRed Rock\nNaN\nAndrea Irvine, Patrick Ryan, Cathy Belton, Ste...\nNaN\nNaN\n2018\n18+\n3 Seasons\nTV Shows\nSharon goes to extreme measures to keep Robbie...\nPrime\n\n\n\n\n\n\n\n\nshows_count_df = shows_df.value_counts(\"duration\").to_frame()\nshows_count_df.sample(3)\n\n\n\n\n\n\n\n\n0\n\n\nduration\n\n\n\n\n\n1 Season\n4183\n\n\n7 Seasons\n89\n\n\n32 Seasons\n1\n\n\n\n\n\n\n\n\nshows_count_df.reset_index(inplace=True)\nshows_count_df.sample(3)\n\n\n\n\n\n\n\n\nduration\n0\n\n\n\n\n9\n10 Seasons\n24\n\n\n14\n15 Seasons\n5\n\n\n7\n8 Seasons\n49\n\n\n\n\n\n\n\n\nshows_count_df = shows_count_df.rename({\n    'duration' : 'Number of seasons', \n    0: 'No of shows'\n    }, axis = 1)\n\nshows_count_df.sample(3)\n\n\n\n\n\n\n\n\nNumber of seasons\nNo of shows\n\n\n\n\n11\n12 Seasons\n13\n\n\n4\n5 Seasons\n195\n\n\n5\n6 Seasons\n115\n\n\n\n\n\n\n\n\nprint(\"Before preprocessing , Longest running season appears to be :\",shows_count_df['Number of seasons'].max())\n\nBefore preprocessing , Longest running season appears to be : 9 Seasons\n\n\nBonus\n\nshows_count_df['Number of seasons'] = shows_count_df['Number of seasons'].apply(lambda x : int(x.replace('Seasons','').replace('Season','')))\nshows_count_df.sample()\n\n\n\n\n\n\n\n\nNumber of seasons\nNo of shows\n\n\n\n\n10\n11\n18\n\n\n\n\n\n\n\n\nprint(\"After preprocessing , Longest running season appears to be :\",shows_count_df['Number of seasons'].max(), \"Seasons\")\n\nAfter preprocessing , Longest running season appears to be : 34 Seasons\n\n\n\n\nPart b\n\nmovies_df = all_platforms_df[all_platforms_df['type']=='Movie']\nmovies_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n2191\ns2192\nMovie\nHoneymoon Limited\nArthur Lubin\nNeil Hamilton, George \"Gabby\" Hayes, June Film...\nNaN\nNaN\n1935\n13+\n72 min\nComedy, Drama\nDiscouraged and unsuccessful writer Dick Gordo...\nPrime\n\n\n\n\n\n\n\n\nmovies_count_df = movies_df.value_counts(\"duration\").to_frame()\nmovies_count_df.sample(3)\n\n\n\n\n\n\n\n\n0\n\n\nduration\n\n\n\n\n\n21 min\n28\n\n\n98 min\n286\n\n\n53 min\n91\n\n\n\n\n\n\n\n\nmovies_count_df.reset_index(inplace=True)\nmovies_count_df.sample(3)\n\n\n\n\n\n\n\n\nduration\n0\n\n\n\n\n177\n173 min\n8\n\n\n179\n182 min\n7\n\n\n63\n73 min\n84\n\n\n\n\n\n\n\n\nmovies_count_df = movies_count_df.rename({\n    'duration' : 'Duration in minutes', \n    0: 'No of movies'\n    }, axis = 1)\n\nmovies_count_df.sample(3)\n\n\n\n\n\n\n\n\nDuration in minutes\nNo of movies\n\n\n\n\n25\n110 min\n187\n\n\n188\n190 min\n3\n\n\n217\n214 min\n1\n\n\n\n\n\n\n\nPart i\n\nmovies_count_df_i = movies_count_df.sort_values(by=\"Duration in minutes\", ascending=False)\nmovies_count_df_i\n\n\n\n\n\n\n\n\nDuration in minutes\nNo of movies\n\n\n\n\n12\n99 min\n288\n\n\n13\n98 min\n286\n\n\n8\n97 min\n324\n\n\n10\n96 min\n303\n\n\n7\n95 min\n340\n\n\n...\n...\n...\n\n\n16\n101 min\n253\n\n\n14\n100 min\n260\n\n\n126\n10 min\n25\n\n\n165\n1 min\n11\n\n\n166\n0 min\n10\n\n\n\n\n225 rows × 2 columns\n\n\n\n\nlongest_movie_duration = movies_count_df_i.head(1)['Duration in minutes'].to_list()[0]\nprint(\"Longest Movie Duration is : \",longest_movie_duration)\n\nLongest Movie Duration is :  99 min\n\n\n\nlongest_movies_df = movies_df[movies_df[\"duration\"] == longest_movie_duration ]\nlongest_movies_df.sample(2)\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n1321\ns1322\nMovie\nThe Princess and the Frog\nJohn Musker, Ron Clements\nAnika Rose, Bruno Campos, Keith David, Michael...\nUnited States\nNovember 12, 2019\n2009\nG\n99 min\nAnimation, Comedy, Family\nA fateful kiss with a frog sends Tiana on a my...\nDisney\n\n\n7648\ns7649\nMovie\nBlack and White\nJames Toback\nRobert Downey Jr., Jared Leto, Michael B. Jord...\nNaN\nNaN\n2000\nR\n99 min\nDrama, Music Videos and Concerts\nA group of white high school teens becomes inv...\nPrime\n\n\n\n\n\n\n\n\nprint(\"Longest Movie Duration is : \",longest_movie_duration)\nprint(\"The Movies are : \",\"\\n\".join(longest_movies_df['title'].to_list()))\n\nLongest Movie Duration is :  99 min\nThe Movies are :  Woman Of Desire\nWhy We Fight\nValley Uprising\nThe Zookeeper\nThe Woman in Black 2: Angel of Death\nThe Ultimate Legacy\nThe Italian Job (1969)\nThe Hungry\nThe Donkey King\nSuper Size Me\nSunshine Hotel\nStreet Dance\nStorm Boy\nShottas\nOur Town\nMy Foolish Heart\nLine of Duty\nHonest Thief\nHiding Out\nHellbound: Hellraiser 2\nHearts in Bondage\nHappythankyoumoreplease\nGrace of God\nGo Fast. Go North.\nDoe\nDigging to China\nBetter Luck Tomorrow\nBelle and Sebastian\nAtlas Shrugged: Part III\nAnna\n23 Blast\nThe Adventurer: The Curse of the Midas Box\nHallowed Be Thy Name\nMadness in the Method\nHick\nAssault on Wall Street\nThe Baytown Outlaws\nRolling Stone: Life And Death Of Brian Jones\nDark Was the Night\nCaught Up\nStinger\nGoodbye, Butterfly\nLittle Kingdom\nPERSECUTION\nBlame it On Fidel\nReversion\nHappy Home\nSuper Fast\nIntersection\nTusks\nThe Shootist\nDeepStar Six\nFirefly\nRock the Casbah\nNightingale: A Melody of Life\nToys Storage 2\nHoop Soldiers\nThe Naked Truth\nAlejandro\nP!nk: All I Know So Far\nVivarium\nThaen\nCBI Vs Lovers\nNight Hunter\nDIVOS!\nThe Map of Tiny Perfect Things\nThe Booksellers\nSeven Days In Utopia\nTales of an American Hoodrat\nRoad to Damascus\nPerfect Strangers\nBoonie Bears: The Big Top Secret\nPunarjagran\nSomeone to Carry Me\nKansas City Confidential\nThe Silent Mountain\nPlayback\nTobruk\nMay Morning\nThe Grotto\nTransmigrate (The Troubled One)\nB. A. Pass\nSideshow\nAngel And The Badman\nPartners\nM.O.M. Mothers of Monsters\nBottom Feeders\nRun For The Sun\nAll About Steve\nJapanese Story\nAzhaggiye Thee\nAcross The Line\nNot That Funny\nThe Woman In The Window\nIlakku\nA Social Cure\nBlack and White\nCruel Train\nDelicatessen\nAbsence of The Good\nFair Play\nThe Boss\nPhysical Evidence\nBad Impulse\nMara\nChinese Box\nBlue Juice\nRoute 24\nRebirth\nThe Descent\nPaper Dragons\nAlienated\nThe Bloody Vampire\nPasser By\nDeath Machine\nThe Omega Code\nGolden Job\nMadison\nLittle Man Tate\nTotò Eva e il pennello proibito\nThe Lady Doctor\nThe Intruder\nOutlaws: The Legend of O.B. Taggart\nGli Onorevoli\nNo Loss, No Gain\nThe Sacrament\nRed Sun Rising\nDriven to Dance\nThe Trench\nOng Bak 3\nDead Serious\nWhat Happens in Vegas\nThe Rocky Horror Picture Show\nInuYasha the Movie 2: The Castle Beyond the Looking Glass\nInuYasha the Movie 3: Swords of an Honorable Ruler\nHere and There\nGood Luck Chuck\nMy Girl 2\nCousins\nRogue Warfare: Death of a Nation\nCopenhagen\nBlack Holes | The Edge of All We Know\nNinja Assassin\nSwordfish\n678\nCinema Bandi\nDance of the Forty One\nMonster\nDead Again in Tombstone\nThe Whole Nine Yards\nSniper: Ghost Shooter\nThe Block Island Sound\nThe Misadventures of Hedi and Cokeman\nIs Love Enough? Sir\nTony Parker: The Final Shot\nU-Turn\nDolly Parton’s Christmas on the Square\nZozo\nA Babysitter's Guide to Monster Hunting\n#Alive\nWhat Keeps You Alive\nRide Like a Girl\nFreej Al Taibeen\nCurtiz\nMindGamers\nA Trip to Jamaica\nIn My Country\nUp North\nThe Bling Lagosians\nRim of the World\nNappily Ever After\nStudio 54\nThe Last Laugh\nAll's Well, End's Well (2009)\nErrementari: The Blacksmith and the Devil\nHarishchandrachi Factory\nThi Mai\nI Am not an Easy Man\nLayla M.\nBad Day for the Cut\nThe Worthy\n#realityhigh\nIn the Shadow of Iris\nSlam\nLife 2.0\nOperações Especiais\nTini: The New Life of Violetta\nJourney to Greenland\nMercenary\nWelcome Mr. President\nAudrie & Daisy\nWinter on Fire: Ukraine's Fight for Freedom\nÇok Filim Hareketler Bunlar\n50 First Dates\nA.X.L.\nAdrishya\nAmerican Hangman\nB.A. Pass\nBarely Lethal\nBecause We're Heading Out\nBeing AP\nBerlin Kaplani\nBobbi Jene\nBobby Robson: More Than a Manager\nBolt\nCandyman\nChasing Trane\nChristian Mingle\nChristmas with the Kranks\nDemocrats\nDetention Letter\nDon't Be Afraid of the Dark\nDriven to Dance\nEyyvah Eyyvah\nFamiliye\nFireflies\nFroning: The Fittest Man in History\nGood Luck\nHangman\nHaunting on Fraternity Row\nIndiscretion\nInuyasha the Movie - La spada del dominatore del mondo\nInuYasha: The Movie 2: The Castle Beyond the Looking Glass\nKaleidoscope\nKnock Knock\nKung Fu Hustle\nLike Arrows\nMad Money\nMara\nMay We Chat\nMy Dog is My Guide\nMy Scientology Movie\nMy Week with Marilyn\nMy Wife and My Wifey\nNowhere Boy\nRace to Witch Mountain\nReaction\nSalem: His Sister's Father\nSay When\nThe Aerial\nThe Croods\nThe Damned Rain\nThe Drowning\nThe Heroes of Evil\nThe Intent\nThe Invention of Lying\nThe Nutcracker and the Four Realms\nThe Sapphires\nThe Tuxedo\nTremors 5:  Bloodline\nVS.\nMaleficent\nDan in Real Life\nBaby's Day Out\nRio\nCheaper By the Dozen\nSecret Society of Second-Born Royals\nMorning Light\nThe Princess Bride\nCool Runnings\nGeek Charming\nIce Princess\nThe Princess and the Frog\nWALL-E\nMaggie's Plan\nClass\nIntersection\nLight It Up\nThe Rocky Horror Picture Show\nTransporter 3\nGirls! Girls! Girls!\nChasing Trane: The John Coltrane Documentary\nHer Name Is Chef\nThe Donut King\nPriceless\nSonic the Hedgehog\nEmperor\nBernie the Dolphin 2\nLucky Day\n'71\nThe Wolf Hour\nVault\nBrian Banks\nHellbound: Hellraiser II\nUntouchable\nAsk Dr. Ruth\nDriverX\nDust 2 Glory\n\n\nPart ii\n\nmovies_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n1107\ns1108\nMovie\nOlaf's Frozen Adventure\nStevie Wermers-Skelton, Kevin Deters\nJosh Gad, Idina Menzel, Kristen Bell, Jonathan...\nUnited States\nNovember 12, 2019\n2017\nG\n25 min\nAnimation, Comedy, Family\nOlaf and Sven set out to save Christmas for An...\nDisney\n\n\n\n\n\n\n\n\nmovies_count_df_ii = movies_df[\"duration\"].value_counts().to_frame()\nmovies_count_df_ii.sample()\n\n\n\n\n\n\n\n\nduration\n\n\n\n\n132 min\n71\n\n\n\n\n\n\n\n\nlongest_duration_for_movies = movies_count_df_ii.sort_index().tail(1).index[0]\nlongest_duration_for_movies\n\n'99 min'\n\n\n\nlongest_movies_df = movies_df[movies_df[\"duration\"] == longest_movie_duration ]\nlongest_movies_df.sample(2)\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n3198\ns3199\nMovie\nAtlas Shrugged: Part III\nJames Manera\nKristoffer Polaha, Laura Regan, Peter Mackenzi...\nNaN\nNaN\n2014\nPG-13\n99 min\nDrama, Science Fiction, Suspense\nAs crime and fear take over the countryside, t...\nPrime\n\n\n6269\ns6270\nMovie\nBecause We're Heading Out\nKhaled El Halafawy\nHassan El Raddad, Emy Samir Ghanim, Bayyumi Fu...\nEgypt\nApril 18, 2019\n2016\nTV-14\n99 min\nComedies, International Movies\nTrouble and high jinks ensue when a man and wo...\nNetflix\n\n\n\n\n\n\n\n\nprint(\"Longest Movie Duration is : \",longest_movie_duration)\nprint(\"The Movies are : \",\"\\n\".join(longest_movies_df['title'].to_list()))\n\nLongest Movie Duration is :  99 min\nThe Movies are :  Woman Of Desire\nWhy We Fight\nValley Uprising\nThe Zookeeper\nThe Woman in Black 2: Angel of Death\nThe Ultimate Legacy\nThe Italian Job (1969)\nThe Hungry\nThe Donkey King\nSuper Size Me\nSunshine Hotel\nStreet Dance\nStorm Boy\nShottas\nOur Town\nMy Foolish Heart\nLine of Duty\nHonest Thief\nHiding Out\nHellbound: Hellraiser 2\nHearts in Bondage\nHappythankyoumoreplease\nGrace of God\nGo Fast. Go North.\nDoe\nDigging to China\nBetter Luck Tomorrow\nBelle and Sebastian\nAtlas Shrugged: Part III\nAnna\n23 Blast\nThe Adventurer: The Curse of the Midas Box\nHallowed Be Thy Name\nMadness in the Method\nHick\nAssault on Wall Street\nThe Baytown Outlaws\nRolling Stone: Life And Death Of Brian Jones\nDark Was the Night\nCaught Up\nStinger\nGoodbye, Butterfly\nLittle Kingdom\nPERSECUTION\nBlame it On Fidel\nReversion\nHappy Home\nSuper Fast\nIntersection\nTusks\nThe Shootist\nDeepStar Six\nFirefly\nRock the Casbah\nNightingale: A Melody of Life\nToys Storage 2\nHoop Soldiers\nThe Naked Truth\nAlejandro\nP!nk: All I Know So Far\nVivarium\nThaen\nCBI Vs Lovers\nNight Hunter\nDIVOS!\nThe Map of Tiny Perfect Things\nThe Booksellers\nSeven Days In Utopia\nTales of an American Hoodrat\nRoad to Damascus\nPerfect Strangers\nBoonie Bears: The Big Top Secret\nPunarjagran\nSomeone to Carry Me\nKansas City Confidential\nThe Silent Mountain\nPlayback\nTobruk\nMay Morning\nThe Grotto\nTransmigrate (The Troubled One)\nB. A. Pass\nSideshow\nAngel And The Badman\nPartners\nM.O.M. Mothers of Monsters\nBottom Feeders\nRun For The Sun\nAll About Steve\nJapanese Story\nAzhaggiye Thee\nAcross The Line\nNot That Funny\nThe Woman In The Window\nIlakku\nA Social Cure\nBlack and White\nCruel Train\nDelicatessen\nAbsence of The Good\nFair Play\nThe Boss\nPhysical Evidence\nBad Impulse\nMara\nChinese Box\nBlue Juice\nRoute 24\nRebirth\nThe Descent\nPaper Dragons\nAlienated\nThe Bloody Vampire\nPasser By\nDeath Machine\nThe Omega Code\nGolden Job\nMadison\nLittle Man Tate\nTotò Eva e il pennello proibito\nThe Lady Doctor\nThe Intruder\nOutlaws: The Legend of O.B. Taggart\nGli Onorevoli\nNo Loss, No Gain\nThe Sacrament\nRed Sun Rising\nDriven to Dance\nThe Trench\nOng Bak 3\nDead Serious\nWhat Happens in Vegas\nThe Rocky Horror Picture Show\nInuYasha the Movie 2: The Castle Beyond the Looking Glass\nInuYasha the Movie 3: Swords of an Honorable Ruler\nHere and There\nGood Luck Chuck\nMy Girl 2\nCousins\nRogue Warfare: Death of a Nation\nCopenhagen\nBlack Holes | The Edge of All We Know\nNinja Assassin\nSwordfish\n678\nCinema Bandi\nDance of the Forty One\nMonster\nDead Again in Tombstone\nThe Whole Nine Yards\nSniper: Ghost Shooter\nThe Block Island Sound\nThe Misadventures of Hedi and Cokeman\nIs Love Enough? Sir\nTony Parker: The Final Shot\nU-Turn\nDolly Parton’s Christmas on the Square\nZozo\nA Babysitter's Guide to Monster Hunting\n#Alive\nWhat Keeps You Alive\nRide Like a Girl\nFreej Al Taibeen\nCurtiz\nMindGamers\nA Trip to Jamaica\nIn My Country\nUp North\nThe Bling Lagosians\nRim of the World\nNappily Ever After\nStudio 54\nThe Last Laugh\nAll's Well, End's Well (2009)\nErrementari: The Blacksmith and the Devil\nHarishchandrachi Factory\nThi Mai\nI Am not an Easy Man\nLayla M.\nBad Day for the Cut\nThe Worthy\n#realityhigh\nIn the Shadow of Iris\nSlam\nLife 2.0\nOperações Especiais\nTini: The New Life of Violetta\nJourney to Greenland\nMercenary\nWelcome Mr. President\nAudrie & Daisy\nWinter on Fire: Ukraine's Fight for Freedom\nÇok Filim Hareketler Bunlar\n50 First Dates\nA.X.L.\nAdrishya\nAmerican Hangman\nB.A. Pass\nBarely Lethal\nBecause We're Heading Out\nBeing AP\nBerlin Kaplani\nBobbi Jene\nBobby Robson: More Than a Manager\nBolt\nCandyman\nChasing Trane\nChristian Mingle\nChristmas with the Kranks\nDemocrats\nDetention Letter\nDon't Be Afraid of the Dark\nDriven to Dance\nEyyvah Eyyvah\nFamiliye\nFireflies\nFroning: The Fittest Man in History\nGood Luck\nHangman\nHaunting on Fraternity Row\nIndiscretion\nInuyasha the Movie - La spada del dominatore del mondo\nInuYasha: The Movie 2: The Castle Beyond the Looking Glass\nKaleidoscope\nKnock Knock\nKung Fu Hustle\nLike Arrows\nMad Money\nMara\nMay We Chat\nMy Dog is My Guide\nMy Scientology Movie\nMy Week with Marilyn\nMy Wife and My Wifey\nNowhere Boy\nRace to Witch Mountain\nReaction\nSalem: His Sister's Father\nSay When\nThe Aerial\nThe Croods\nThe Damned Rain\nThe Drowning\nThe Heroes of Evil\nThe Intent\nThe Invention of Lying\nThe Nutcracker and the Four Realms\nThe Sapphires\nThe Tuxedo\nTremors 5:  Bloodline\nVS.\nMaleficent\nDan in Real Life\nBaby's Day Out\nRio\nCheaper By the Dozen\nSecret Society of Second-Born Royals\nMorning Light\nThe Princess Bride\nCool Runnings\nGeek Charming\nIce Princess\nThe Princess and the Frog\nWALL-E\nMaggie's Plan\nClass\nIntersection\nLight It Up\nThe Rocky Horror Picture Show\nTransporter 3\nGirls! Girls! Girls!\nChasing Trane: The John Coltrane Documentary\nHer Name Is Chef\nThe Donut King\nPriceless\nSonic the Hedgehog\nEmperor\nBernie the Dolphin 2\nLucky Day\n'71\nThe Wolf Hour\nVault\nBrian Banks\nHellbound: Hellraiser II\nUntouchable\nAsk Dr. Ruth\nDriverX\nDust 2 Glory\n\n\nPart iii\n\nmovies_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n7153\ns7154\nMovie\nJustice, My Foot!\nJohnnie To\nStephen Chow, Anita Mui, Carrie Ng, Wai Ai, Be...\nHong Kong\nAugust 31, 2018\n1992\nTV-14\n95 min\nAction & Adventure, Comedies, International Mo...\nSlick lawyer Sung fears he's been cursed with ...\nNetflix\n\n\n\n\n\n\n\n\nmovies_df['duration'].max()\n\nTypeError: '&gt;=' not supported between instances of 'str' and 'float'\n\n\n\nmovies_df.idxmax(axis=\"columns\")\n\nTypeError: reduction operation 'argmax' not allowed for this dtype\n\n\nMax function tried to compare values in the column. But the column had two datatypes in it - string and float( for null values ). Comparison was not a permitted operation between string and float. Hence the error\n\n\n\nQ 4.Shows streaming on multiple platforms\n\nall_platforms_df.sample()\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n1144\ns1145\nMovie\nYes Man\nPeyton Reed\nJim Carrey, Zooey Deschanel, Bradley Cooper, J...\nUnited States, United Kingdom\nApril 1, 2021\n2008\nPG-13\n104 min\nComedies, Romantic Movies\nAfter a bitter divorce, a loan officer falls u...\nNetflix\n\n\n\n\n\n\n\nPart a\n\nprint(\"The number of rows is :\",all_platforms_df.shape[0])\n\nThe number of rows is : 22998\n\n\nPart b\n\nunique_titles = all_platforms_df['title'].unique()\nunique_titles\n\narray(['The Grand Seduction', 'Take Care Good Night',\n       'Secrets of Deception', ..., 'Star Trek: The Original Series',\n       'The Twilight Zone', 'Tokyo Magnitude 8.0'], dtype=object)\n\n\n\nprint('Number of unique titles :',len(unique_titles))\n\nNumber of unique titles : 22115\n\n\nPart c\n\ntitles_count_df = all_platforms_df['title'].value_counts().to_frame()\ntitles_count_df.sample(2)\n\n\n\n\n\n\n\n\ntitle\n\n\n\n\nBatla House\n1\n\n\nAsmaa\n1\n\n\n\n\n\n\n\nPart d\n\ntitles_count_df.reset_index(inplace=True)\n\n\ntitles_count_df.rename({\n    \"index\":\"name\"\n}, axis = 1, inplace = True)\n\n\ntitles_count_df.sample(5)\n\n\n\n\n\n\n\n\nname\ntitle\n\n\n\n\n21333\nTorture Chamber\n1\n\n\n9479\nMonty Don's Italian Gardens\n1\n\n\n8892\nSchool Daze\n1\n\n\n18403\nAny Crybabies Around?\n1\n\n\n1230\nCoco y Raulito: Carrusel de ternura\n1\n\n\n\n\n\n\n\nPart e\n\ntitles_count_df.rename({\n    'name' : \"Movie or Show Name\",\n    \"title\" : \"No of Platforms\"\n}, axis = 1, inplace = True)\n\n\ntitles_count_df.sample(5)\n\n\n\n\n\n\n\n\nMovie or Show Name\nNo of Platforms\n\n\n\n\n7743\nShuffle\n1\n\n\n5470\nFadily Camara : La plus drôle de tes copines\n1\n\n\n11135\nThe New Neighbor\n1\n\n\n974\nMaya Memsaab\n1\n\n\n6309\nMake This Tonight\n1\n\n\n\n\n\n\n\nPart f\n\nmax_platform_shows = shows_df['title'].value_counts().to_frame()['title'].max()\nmax_platform_shows\n\n3\n\n\n\nprint(\"The Maximum number of platforms a show is on is \",max_platform_shows)\n\nThe Maximum number of platforms a show is on is  3\n\n\n\n\nQ5. Favorite show or movie\nPart a\n\nduplicated_shows = shows_df[shows_df.duplicated(subset=['title'], keep = False)]\nduplicated_shows.sample(2)\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n2264\ns2265\nTV Show\nHangar 1: The UFO Files\nNaN\nNik Petcov, Cornell Womack\nNaN\nNaN\n2015\nTV-PG\n2 Seasons\nScience Fiction, Suspense, Unscripted\nGo behind the restricted doors as Hangar 1: Th...\nPrime\n\n\n5803\ns5804\nTV Show\nForensic Files\nNaN\nPeter Thomas\nUnited States\nSeptember 1, 2016\n2011\nTV-MA\n9 Seasons\nCrime TV Shows, Docuseries, Science & Nature TV\nDetectives and crime lab technicians use the l...\nNetflix\n\n\n\n\n\n\n\nPart b\n\nfavorite_movie = 'Everything Everywhere All at Once'\n\n\nall_platforms_df[all_platforms_df['title'].apply(lambda x : True if x.lower()==favorite_movie.lower() else False )]\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n\n\n\n\n\n\nrandom_row = all_platforms_df.sample(1)\nrandom_title = random_row['title'].values[0]\nprint(random_title)\nrandom_row\n\nBlack Dawn\n\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n7450\ns7451\nMovie\nBlack Dawn\nAlexander Gruszynski\nSteven Seagal, Tamara Davies, Eddie Velez\nNaN\nNaN\n2005\nR\n96 min\nAction\nTrying to determine who seeks to sell a nuclea...\nPrime\n\n\n\n\n\n\n\n\nall_platforms_df[all_platforms_df['title'] == random_title]\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n7450\ns7451\nMovie\nBlack Dawn\nAlexander Gruszynski\nSteven Seagal, Tamara Davies, Eddie Velez\nNaN\nNaN\n2005\nR\n96 min\nAction\nTrying to determine who seeks to sell a nuclea...\nPrime\n\n\n\n\n\n\n\n\n\nQ 6. Save Data\n\ntarget_file_name = \"streaming_titles.csv\"\nall_platforms_df.to_csv(target_file_name,index=False)\n\n\n\nQ 7 . Name starts with\nPart a\n\nstreaming_titles_df = pd.read_csv(target_file_name)\nstreaming_titles_df.sample(2)\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n22897\ns2973\nTV Show\nCoppelion\nNaN\nNaN\nJapan\nOctober 2, 2013\n2013\nTV-14\n1 Season\nAction, Adventure, Anime\nFrom the team that brought you K comes the new...\nHulu\n\n\n2543\ns2544\nMovie\nErnest Cole\nJurgen Schadeberg\nNaN\nNaN\nNaN\n2006\n13+\n54 min\nDocumentary\nJurgen Schadeberg brings light on fellow photo...\nPrime\n\n\n\n\n\n\n\nPart b\n\nfirst_name = \"Enfa\"\nfirst_letter_match_movies = movies_df[movies_df['title'].str.startswith(first_name[0])]\nfirst_letter_match_movies.sample(2)\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n2540\ns2541\nMovie\nEscape from the Bronx\nEnzo G. Castellari\nMark Gregory, Henry Silva, Valeria D'Obici, Ti...\nNaN\nNaN\n1983\nR\n90 min\nAction, Science Fiction\nThe year is 2000 and the Bronx has been reduce...\nPrime\n\n\n268\ns269\nMovie\nEl Dorado\nNaN\nNaN\nNaN\nSeptember 1, 2021\n1967\nTV-PG\n126 min\nAction, Adventure\nA gunslinger and his shotgun-wielding sidekick...\nHulu\n\n\n\n\n\n\n\nPart c\n\nfirst_letter_match_shows = shows_df[shows_df['title'].str.startswith(first_name[0])]\nfirst_letter_match_shows.sample(2)\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n418\ns419\nTV Show\nEarth to Ned\nNaN\nNaN\nUnited States\nSeptember 4, 2020\n2020\nTV-PG\n1 Season\nComedy, Parody, Science Fiction\nFrom the Jim Henson Company, alien commander N...\nDisney\n\n\n706\ns707\nTV Show\nElite Short Stories: Omar Ander Alexis\nNaN\nArón Piper, Omar Ayuso, Claudia Salas, Jorge C...\nNaN\nJune 16, 2021\n2021\nTV-MA\n1 Season\nInternational TV Shows, Spanish-Language TV Sh...\nNow in remission, Ander is set on spending his...\nNetflix\n\n\n\n\n\n\n\n\nprint(\"Number of unique shows that start with my first name first letter are : \",len(first_letter_match_shows['title'].unique()))\n\nNumber of unique shows that start with my first name first letter are :  140\n\n\nPart d\n\nTA_first_letter = 'B'\n\n\nfirst_letter_match_movies = movies_df[movies_df['title'].str.startswith(TA_first_letter)]\nfirst_letter_match_movies.sample(2)\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n6254\ns6255\nMovie\nBawarchi\nHrishikesh Mukherjee\nRajesh Khanna, A.K. Hangal, Durga Khote, Jaya ...\nIndia\nDecember 31, 2019\n1972\nTV-PG\n125 min\nClassic Movies, Comedies, Dramas\nA dysfunctional middle-class family is transfo...\nNetflix\n\n\n6367\ns6368\nMovie\nBoy Bye\nChris Stokes\nWendy Raquel Robinson, Ross Fleming, Shondrell...\nUnited States\nApril 2, 2019\n2016\nTV-MA\n86 min\nComedies, Romantic Movies\nSingle entrepreneur Veronica finally starts to...\nNetflix\n\n\n\n\n\n\n\nPart e\n\nfirst_letter_match_shows = shows_df[shows_df['title'].str.startswith(TA_first_letter)]\nfirst_letter_match_shows.sample(2)\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n7759\ns7760\nTV Show\nBluestone Four Two\nNaN\nOliver Chris, Gary Carr, Tony Gardner, Kelly A...\nNaN\nNaN\n2015\n16+\n3 Seasons\nComedy\nThe men and women of bomb disposal unit Bluest...\nPrime\n\n\n3063\ns3064\nTV Show\nBilly the Exterminator\nNaN\nBilly Bretherton\nNaN\nNaN\n2012\nTV-PG\n6 Seasons\nDocumentary, Drama, Unscripted\nJoin us as we follow Billy Bretherton and his ...\nPrime\n\n\n\n\n\n\n\n\nfirst_name_TA_uniq = len(first_letter_match_shows['title'].unique())\nprint(\"Number of unique shows that start with our TA's first name first letter are : \",first_name_TA_uniq)\n\nNumber of unique shows that start with our TA's first name first letter are :  428\n\n\nPart f\n\nfirst_letter_match_shows = shows_df[shows_df['title'].str.startswith(TA_first_letter)]\nfirst_letter_match_shows.sample(2)\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n1285\ns1286\nTV Show\nBurger Scholar Sessions\nNaN\nNaN\nNaN\nOctober 26, 2020\n2020\nNaN\n1 Season\nCooking & Food, Lifestyle & Culture\nIn First We Feast's new series \"Burger Scholar...\nHulu\n\n\n2224\ns2225\nTV Show\nBarefoot Contessa\nNaN\nNaN\nUnited States\nApril 15, 2019\n2002\nTV-G\n1 Season\nCooking & Food\nIna Garten knows how to entertain with simplic...\nHulu\n\n\n\n\n\n\n\n\nlast_name_TA_uniq = len(first_letter_match_shows['title'].unique())\nprint(\"Number of unique shows that start with our TA's Last name first letter are : \",last_name_TA_uniq)\n\nNumber of unique shows that start with our TA's Last name first letter are :  428\n\n\nPart g\n\nprint(\"Diff between f and g is\", last_name_TA_uniq-first_name_TA_uniq)\n\nDiff between f and g is 0"
  },
  {
    "objectID": "Homework/2/Solutions.html",
    "href": "Homework/2/Solutions.html",
    "title": "Homework 2 : Data Collection",
    "section": "",
    "text": "## Notebook Presentation [6 points ]\n# For web and html\nimport requests\nfrom time import sleep\nfrom bs4 import BeautifulSoup\n\n#For working with raw files\nimport zipfile\n\n#For working with data\nimport pandas as pd\nfrom datetime import datetime"
  },
  {
    "objectID": "Homework/2/Solutions.html#problem-1-whats-the-secret-code-4-points",
    "href": "Homework/2/Solutions.html#problem-1-whats-the-secret-code-4-points",
    "title": "Homework 2 : Data Collection",
    "section": "Problem 1 : What’s the secret code? [ 4 points ]",
    "text": "Problem 1 : What’s the secret code? [ 4 points ]\n\nQ 1. Use the library we learned in class to get the page’s HTML. [ 1 point ]. See 2:36 in the Solutions Video\n\nBONUS_URL = \"https://csc380.beingenfa.com/Bonus/1.html\"\n\n\nbonus_req_obj = requests.get(BONUS_URL)\nbonus_req_obj\n\n&lt;Response [200]&gt;\n\n\n\nbonus_page_html = bonus_req_obj.text\nbonus_page_html\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\\n\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"generator\" content=\"quarto-1.3.361\"&gt;\\n\\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\\n\\n\\n&lt;title&gt;CSC 380 – quarto-inputa5805b45&lt;/title&gt;\\n&lt;style&gt;\\ncode{white-space: pre-wrap;}\\nspan.smallcaps{font-variant: small-caps;}\\ndiv.columns{display: flex; gap: min(4vw, 1.5em);}\\ndiv.column{flex: auto; overflow-x: auto;}\\ndiv.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}\\nul.task-list{list-style: none;}\\nul.task-list li input[type=\"checkbox\"] {\\n  width: 0.8em;\\n  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ \\n  vertical-align: middle;\\n}\\n&lt;/style&gt;\\n\\n\\n&lt;script src=\"../site_libs/quarto-nav/quarto-nav.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-nav/headroom.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/clipboard/clipboard.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/autocomplete.umd.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/fuse.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-search/quarto-search.js\"&gt;&lt;/script&gt;\\n&lt;meta name=\"quarto:offset\" content=\"../\"&gt;\\n&lt;script src=\"../site_libs/quarto-html/quarto.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/popper.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/tippy.umd.min.js\"&gt;&lt;/script&gt;\\n&lt;script src=\"../site_libs/quarto-html/anchor.min.js\"&gt;&lt;/script&gt;\\n&lt;link href=\"../site_libs/quarto-html/tippy.css\" rel=\"stylesheet\"&gt;\\n&lt;link href=\"../site_libs/quarto-html/quarto-syntax-highlighting.css\" rel=\"stylesheet\" id=\"quarto-text-highlighting-styles\"&gt;\\n&lt;script src=\"../site_libs/bootstrap/bootstrap.min.js\"&gt;&lt;/script&gt;\\n&lt;link href=\"../site_libs/bootstrap/bootstrap-icons.css\" rel=\"stylesheet\"&gt;\\n&lt;link href=\"../site_libs/bootstrap/bootstrap.min.css\" rel=\"stylesheet\" id=\"quarto-bootstrap\" data-mode=\"light\"&gt;\\n&lt;script id=\"quarto-search-options\" type=\"application/json\"&gt;{\\n  \"location\": \"sidebar\",\\n  \"copy-button\": false,\\n  \"collapse-after\": 3,\\n  \"panel-placement\": \"start\",\\n  \"type\": \"textbox\",\\n  \"limit\": 20,\\n  \"language\": {\\n    \"search-no-results-text\": \"No results\",\\n    \"search-matching-documents-text\": \"matching documents\",\\n    \"search-copy-link-title\": \"Copy link to search\",\\n    \"search-hide-matches-text\": \"Hide additional matches\",\\n    \"search-more-match-text\": \"more match in this document\",\\n    \"search-more-matches-text\": \"more matches in this document\",\\n    \"search-clear-button-title\": \"Clear\",\\n    \"search-detached-cancel-button-title\": \"Cancel\",\\n    \"search-submit-button-title\": \"Submit\",\\n    \"search-label\": \"Search\"\\n  }\\n}&lt;/script&gt;\\n\\n\\n&lt;link rel=\"stylesheet\" href=\"../styles.css\"&gt;\\n&lt;/head&gt;\\n\\n&lt;body class=\"nav-sidebar docked\"&gt;\\n\\n&lt;div id=\"quarto-search-results\"&gt;&lt;/div&gt;\\n  &lt;header id=\"quarto-header\" class=\"headroom fixed-top\"&gt;\\n  &lt;nav class=\"quarto-secondary-nav\"&gt;\\n    &lt;div class=\"container-fluid d-flex\"&gt;\\n      &lt;button type=\"button\" class=\"quarto-btn-toggle btn\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;\\n        &lt;i class=\"bi bi-layout-text-sidebar-reverse\"&gt;&lt;/i&gt;\\n      &lt;/button&gt;\\n      &lt;nav class=\"quarto-page-breadcrumbs\" aria-label=\"breadcrumb\"&gt;&lt;ol class=\"breadcrumb\"&gt;&lt;li class=\"breadcrumb-item\"&gt;\\n      CSC 380\\n      &lt;/li&gt;&lt;/ol&gt;&lt;/nav&gt;\\n      &lt;a class=\"flex-grow-1\" role=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\"&gt;      \\n      &lt;/a&gt;\\n      &lt;button type=\"button\" class=\"btn quarto-search-button\" aria-label=\"\" onclick=\"window.quartoOpenSearch();\"&gt;\\n        &lt;i class=\"bi bi-search\"&gt;&lt;/i&gt;\\n      &lt;/button&gt;\\n    &lt;/div&gt;\\n  &lt;/nav&gt;\\n&lt;/header&gt;\\n&lt;!-- content --&gt;\\n&lt;div id=\"quarto-content\" class=\"quarto-container page-columns page-rows-contents page-layout-article\"&gt;\\n&lt;!-- sidebar --&gt;\\n  &lt;nav id=\"quarto-sidebar\" class=\"sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto\"&gt;\\n    &lt;div class=\"pt-lg-2 mt-2 text-left sidebar-header\"&gt;\\n    &lt;div class=\"sidebar-title mb-0 py-0\"&gt;\\n      &lt;a href=\"../\"&gt;CSC 380&lt;/a&gt; \\n    &lt;/div&gt;\\n      &lt;/div&gt;\\n        &lt;div class=\"mt-2 flex-shrink-0 align-items-center\"&gt;\\n        &lt;div class=\"sidebar-search\"&gt;\\n        &lt;div id=\"quarto-search\" class=\"\" title=\"Search\"&gt;&lt;/div&gt;\\n        &lt;/div&gt;\\n        &lt;/div&gt;\\n    &lt;div class=\"sidebar-menu-container\"&gt; \\n    &lt;ul class=\"list-unstyled mt-1\"&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-1\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Course Content&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-1\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-1\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_1/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 1&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_2/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 2&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_3/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 3&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Course_Content/Week_4&amp;5/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Week 4 &amp; 5&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-2\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Homework&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-2\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-2\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Homework/1/HW.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;HW1: Probability&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Homework/2/HW.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;HW2: Data Collection&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-3\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Ethics Discussions&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-3\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-3\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_2.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W2: Political Content&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_3.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W3: Creative Work&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_4.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W4: Mental Health Support&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_6.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W6: Recommendations&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_7.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W7: Autonomous Driving&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_8.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W8: Housekeeping&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Ethics/Week_9.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;W9: Facial Recognition&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-4\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Bonus Questions&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-4\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-4\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Bonus/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Home&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-5\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Participation&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-5\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-5\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Participation_Activities/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Home&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-6\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Honors&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-6\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-6\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Honors/home.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Assignments&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n        &lt;li class=\"sidebar-item sidebar-item-section\"&gt;\\n      &lt;div class=\"sidebar-item-container\"&gt; \\n            &lt;a class=\"sidebar-item-text sidebar-link text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-7\" aria-expanded=\"true\"&gt;\\n &lt;span class=\"menu-text\"&gt;Syllabus&lt;/span&gt;&lt;/a&gt;\\n          &lt;a class=\"sidebar-item-toggle text-start\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar-section-7\" aria-expanded=\"true\" aria-label=\"Toggle section\"&gt;\\n            &lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\\n          &lt;/a&gt; \\n      &lt;/div&gt;\\n      &lt;ul id=\"quarto-sidebar-section-7\" class=\"collapse list-unstyled sidebar-section depth1 show\"&gt;  \\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Syllabus/Key_Info.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Key Info&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n          &lt;li class=\"sidebar-item\"&gt;\\n  &lt;div class=\"sidebar-item-container\"&gt; \\n  &lt;a href=\"../Syllabus/Syllabus.html\" class=\"sidebar-item-text sidebar-link\"&gt;\\n &lt;span class=\"menu-text\"&gt;Official Syllabus&lt;/span&gt;&lt;/a&gt;\\n  &lt;/div&gt;\\n&lt;/li&gt;\\n      &lt;/ul&gt;\\n  &lt;/li&gt;\\n    &lt;/ul&gt;\\n    &lt;/div&gt;\\n&lt;/nav&gt;\\n&lt;div id=\"quarto-sidebar-glass\" data-bs-toggle=\"collapse\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\"&gt;&lt;/div&gt;\\n&lt;!-- margin-sidebar --&gt;\\n    &lt;div id=\"quarto-margin-sidebar\" class=\"sidebar margin-sidebar\"&gt;\\n        &lt;nav id=\"TOC\" role=\"doc-toc\" class=\"toc-active\"&gt;\\n    &lt;h2 id=\"toc-title\"&gt;On this page&lt;/h2&gt;\\n   \\n  &lt;ul&gt;\\n  &lt;li&gt;&lt;a href=\"#sonams-dilemma---a-gift-for-few-or-a-chance-for-all\" id=\"toc-sonams-dilemma---a-gift-for-few-or-a-chance-for-all\" class=\"nav-link active\" data-scroll-target=\"#sonams-dilemma---a-gift-for-few-or-a-chance-for-all\"&gt;Sonam’s Dilemma - A Gift for Few or a Chance for All&lt;/a&gt;\\n  &lt;ul class=\"collapse\"&gt;\\n  &lt;li&gt;&lt;a href=\"#instructions\" id=\"toc-instructions\" class=\"nav-link\" data-scroll-target=\"#instructions\"&gt;Instructions&lt;/a&gt;&lt;/li&gt;\\n  &lt;li&gt;&lt;a href=\"#question\" id=\"toc-question\" class=\"nav-link\" data-scroll-target=\"#question\"&gt;Question&lt;/a&gt;&lt;/li&gt;\\n  &lt;/ul&gt;&lt;/li&gt;\\n  &lt;/ul&gt;\\n&lt;/nav&gt;\\n    &lt;/div&gt;\\n&lt;!-- main --&gt;\\n&lt;main class=\"content\" id=\"quarto-document-content\"&gt;\\n\\n\\n\\n&lt;section id=\"sonams-dilemma---a-gift-for-few-or-a-chance-for-all\" class=\"level1\"&gt;\\n&lt;h1&gt;Sonam’s Dilemma - A Gift for Few or a Chance for All&lt;/h1&gt;\\n&lt;section id=\"bonus-question-1-point\" class=\"level6\"&gt;\\n&lt;h6 class=\"anchored\" data-anchor-id=\"bonus-question-1-point\"&gt;Bonus Question : 1 point&lt;/h6&gt;\\n&lt;p&gt;&lt;br&gt;&lt;/p&gt;\\n&lt;/section&gt;\\n&lt;section id=\"instructions\" class=\"level3\"&gt;\\n&lt;h3 class=\"anchored\" data-anchor-id=\"instructions\"&gt;Instructions&lt;/h3&gt;\\n&lt;ul&gt;\\n&lt;li&gt;Due Date : July 7, Friday, 5 pm&lt;/li&gt;\\n&lt;li&gt;Submit your answer at D2 &gt; Quiz &gt; “Sonam’s Dilemma : a gift for few or a chance for all”&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/section&gt;\\n&lt;section id=\"question\" class=\"level3\"&gt;\\n&lt;h3 class=\"anchored\" data-anchor-id=\"question\"&gt;Question&lt;/h3&gt;\\n&lt;p&gt;&lt;br&gt; &lt;em&gt;Source : Story I made up, text improved by ChatGPT, and then edited by myself.&lt;/em&gt;&lt;/p&gt;\\n&lt;p&gt;Once upon a time, in the grand kingdom of Razia, there existed a mesmerizing lead dancer named Sonam. She possesed a combination of charm, extraordinary talent, and intelligence that made her stand out amongst her peers. Sonam’s passion for dance was matched only by her love for her dance group,with 2000 other talented dancers, who all shared an unbreakable bond and admired their noble Queen above all.&lt;/p&gt;\\n&lt;p&gt;One fine day, as Sonam’s heart overflowed with gratitude for their benevolent ruler, she decided to express her deep admiration through the power of music. She crafted a beautiful song that portrayed the love and adoration her dance group held for Queen Razia. With eager anticipation, Sonam’s troupe dedicated themselves to perfecting their dance moves to match the rhythm and emotion of her heartfelt composition.&lt;/p&gt;\\n&lt;p&gt;The auspicious occasion of Queen Razia’s birthday arrived, filling the grand palace with a sense of joy and celebration. The Queen, known for her discerning taste and appreciation for the arts, eagerly awaited the performance of Sonam’s dance group. As the music began to play and the dancers gracefully glided across the grand ballroom, a spell was cast upon all those in attendance.&lt;/p&gt;\\n&lt;p&gt;The Queen was captivated by the talent displayed before her, and her heart swelled with pride and delight. She could not help but marvel at the synchronized movements and the sheer passion that emanated from each and every member of the dance group. As the performance reached its crescendo, Queen Razia’s decision was made in an instant—this remarkable group deserved a gift to commemorate their extraordinary dedication.&lt;/p&gt;\\n&lt;div class=\"Secret\"&gt;\\n&lt;dr_cynthia_breazeal&gt;&lt;/dr_cynthia_breazeal&gt;\\n&lt;/div&gt;\\n&lt;p&gt;However, there were only a limited number of gift available to give out right now. She presented Sonam with two choices, each with its own intriguing possibilities. The first option was straightforward : 660 randomly chosen dancers would receive a gift. This meant that a select few would be rewarded, leaving the remaining members without a token of appreciation.&lt;/p&gt;\\n&lt;p&gt;The second choice, however, offered a twist. Queen Shah proposed distributing the available gifts among all dancers, but with a twist of uncertainty. Each dancer would have a 33% chance of receiving a gift. 67% of not receiving a gift.&lt;/p&gt;\\n&lt;p&gt;Now faced with this dilemma, Sonam’s sharp mind began to evaluate the situation. Which option should Sonam pick? Why? Why not the other?&lt;/p&gt;\\n\\n\\n&lt;/section&gt;\\n&lt;/section&gt;\\n\\n&lt;/main&gt; &lt;!-- /main --&gt;\\n&lt;script id=\"quarto-html-after-body\" type=\"application/javascript\"&gt;\\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\\n    const bodyEl = window.document.querySelector(\"body\");\\n    if (mode === \"dark\") {\\n      bodyEl.classList.add(\"quarto-dark\");\\n      bodyEl.classList.remove(\"quarto-light\");\\n    } else {\\n      bodyEl.classList.add(\"quarto-light\");\\n      bodyEl.classList.remove(\"quarto-dark\");\\n    }\\n  }\\n  const toggleBodyColorPrimary = () =&gt; {\\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\\n    if (bsSheetEl) {\\n      toggleBodyColorMode(bsSheetEl);\\n    }\\n  }\\n  toggleBodyColorPrimary();  \\n  const icon = \"\\ue9cb\";\\n  const anchorJS = new window.AnchorJS();\\n  anchorJS.options = {\\n    placement: \\'right\\',\\n    icon: icon\\n  };\\n  anchorJS.add(\\'.anchored\\');\\n  const isCodeAnnotation = (el) =&gt; {\\n    for (const clz of el.classList) {\\n      if (clz.startsWith(\\'code-annotation-\\')) {                     \\n        return true;\\n      }\\n    }\\n    return false;\\n  }\\n  const clipboard = new window.ClipboardJS(\\'.code-copy-button\\', {\\n    text: function(trigger) {\\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\\n      for (const childEl of codeEl.children) {\\n        if (isCodeAnnotation(childEl)) {\\n          childEl.remove();\\n        }\\n      }\\n      return codeEl.innerText;\\n    }\\n  });\\n  clipboard.on(\\'success\\', function(e) {\\n    // button target\\n    const button = e.trigger;\\n    // don\\'t keep focus\\n    button.blur();\\n    // flash \"checked\"\\n    button.classList.add(\\'code-copy-button-checked\\');\\n    var currentTitle = button.getAttribute(\"title\");\\n    button.setAttribute(\"title\", \"Copied!\");\\n    let tooltip;\\n    if (window.bootstrap) {\\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\\n      button.setAttribute(\"data-bs-placement\", \"left\");\\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\\n      tooltip = new bootstrap.Tooltip(button, \\n        { trigger: \"manual\", \\n          customClass: \"code-copy-button-tooltip\",\\n          offset: [0, -8]});\\n      tooltip.show();    \\n    }\\n    setTimeout(function() {\\n      if (tooltip) {\\n        tooltip.hide();\\n        button.removeAttribute(\"data-bs-title\");\\n        button.removeAttribute(\"data-bs-toggle\");\\n        button.removeAttribute(\"data-bs-placement\");\\n      }\\n      button.setAttribute(\"title\", currentTitle);\\n      button.classList.remove(\\'code-copy-button-checked\\');\\n    }, 1000);\\n    // clear code selection\\n    e.clearSelection();\\n  });\\n  function tippyHover(el, contentFn) {\\n    const config = {\\n      allowHTML: true,\\n      content: contentFn,\\n      maxWidth: 500,\\n      delay: 100,\\n      arrow: false,\\n      appendTo: function(el) {\\n          return el.parentElement;\\n      },\\n      interactive: true,\\n      interactiveBorder: 10,\\n      theme: \\'quarto\\',\\n      placement: \\'bottom-start\\'\\n    };\\n    window.tippy(el, config); \\n  }\\n  const noterefs = window.document.querySelectorAll(\\'a[role=\"doc-noteref\"]\\');\\n  for (var i=0; i&lt;noterefs.length; i++) {\\n    const ref = noterefs[i];\\n    tippyHover(ref, function() {\\n      // use id or data attribute instead here\\n      let href = ref.getAttribute(\\'data-footnote-href\\') || ref.getAttribute(\\'href\\');\\n      try { href = new URL(href).hash; } catch {}\\n      const id = href.replace(/^#\\\\/?/, \"\");\\n      const note = window.document.getElementById(id);\\n      return note.innerHTML;\\n    });\\n  }\\n      let selectedAnnoteEl;\\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\\n        let cellAttr = \\'data-code-cell=\"\\' + cell + \\'\"\\';\\n        let lineAttr = \\'data-code-annotation=\"\\' +  annotation + \\'\"\\';\\n        const selector = \\'span[\\' + cellAttr + \\'][\\' + lineAttr + \\']\\';\\n        return selector;\\n      }\\n      const selectCodeLines = (annoteEl) =&gt; {\\n        const doc = window.document;\\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\\n        const lineIds = lines.map((line) =&gt; {\\n          return targetCell + \"-\" + line;\\n        })\\n        let top = null;\\n        let height = null;\\n        let parent = null;\\n        if (lineIds.length &gt; 0) {\\n            //compute the position of the single el (top and bottom and make a div)\\n            const el = window.document.getElementById(lineIds[0]);\\n            top = el.offsetTop;\\n            height = el.offsetHeight;\\n            parent = el.parentElement.parentElement;\\n          if (lineIds.length &gt; 1) {\\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\\n            height = bottom - top;\\n          }\\n          if (top !== null && height !== null && parent !== null) {\\n            // cook up a div (if necessary) and position it \\n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\\n            if (div === null) {\\n              div = window.document.createElement(\"div\");\\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\\n              div.style.position = \\'absolute\\';\\n              parent.appendChild(div);\\n            }\\n            div.style.top = top - 2 + \"px\";\\n            div.style.height = height + 4 + \"px\";\\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\\n            if (gutterDiv === null) {\\n              gutterDiv = window.document.createElement(\"div\");\\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\\n              gutterDiv.style.position = \\'absolute\\';\\n              const codeCell = window.document.getElementById(targetCell);\\n              const gutter = codeCell.querySelector(\\'.code-annotation-gutter\\');\\n              gutter.appendChild(gutterDiv);\\n            }\\n            gutterDiv.style.top = top - 2 + \"px\";\\n            gutterDiv.style.height = height + 4 + \"px\";\\n          }\\n          selectedAnnoteEl = annoteEl;\\n        }\\n      };\\n      const unselectCodeLines = () =&gt; {\\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\\n        elementsIds.forEach((elId) =&gt; {\\n          const div = window.document.getElementById(elId);\\n          if (div) {\\n            div.remove();\\n          }\\n        });\\n        selectedAnnoteEl = undefined;\\n      };\\n      // Attach click handler to the DT\\n      const annoteDls = window.document.querySelectorAll(\\'dt[data-target-cell]\\');\\n      for (const annoteDlNode of annoteDls) {\\n        annoteDlNode.addEventListener(\\'click\\', (event) =&gt; {\\n          const clickedEl = event.target;\\n          if (clickedEl !== selectedAnnoteEl) {\\n            unselectCodeLines();\\n            const activeEl = window.document.querySelector(\\'dt[data-target-cell].code-annotation-active\\');\\n            if (activeEl) {\\n              activeEl.classList.remove(\\'code-annotation-active\\');\\n            }\\n            selectCodeLines(clickedEl);\\n            clickedEl.classList.add(\\'code-annotation-active\\');\\n          } else {\\n            // Unselect the line\\n            unselectCodeLines();\\n            clickedEl.classList.remove(\\'code-annotation-active\\');\\n          }\\n        });\\n      }\\n  const findCites = (el) =&gt; {\\n    const parentEl = el.parentElement;\\n    if (parentEl) {\\n      const cites = parentEl.dataset.cites;\\n      if (cites) {\\n        return {\\n          el,\\n          cites: cites.split(\\' \\')\\n        };\\n      } else {\\n        return findCites(el.parentElement)\\n      }\\n    } else {\\n      return undefined;\\n    }\\n  };\\n  var bibliorefs = window.document.querySelectorAll(\\'a[role=\"doc-biblioref\"]\\');\\n  for (var i=0; i&lt;bibliorefs.length; i++) {\\n    const ref = bibliorefs[i];\\n    const citeInfo = findCites(ref);\\n    if (citeInfo) {\\n      tippyHover(citeInfo.el, function() {\\n        var popup = window.document.createElement(\\'div\\');\\n        citeInfo.cites.forEach(function(cite) {\\n          var citeDiv = window.document.createElement(\\'div\\');\\n          citeDiv.classList.add(\\'hanging-indent\\');\\n          citeDiv.classList.add(\\'csl-entry\\');\\n          var biblioDiv = window.document.getElementById(\\'ref-\\' + cite);\\n          if (biblioDiv) {\\n            citeDiv.innerHTML = biblioDiv.innerHTML;\\n          }\\n          popup.appendChild(citeDiv);\\n        });\\n        return popup.innerHTML;\\n      });\\n    }\\n  }\\n});\\n&lt;/script&gt;\\n&lt;/div&gt; &lt;!-- /content --&gt;\\n\\n\\n\\n&lt;/body&gt;&lt;/html&gt;'\n\n\n\n\nQ 2. Find the section with the secret code by using the Beautiful Soup’s find function [ 2 points ]. See 5:53 in the Solutions Video\n\nbonus_bs4_obj = BeautifulSoup(bonus_page_html,\"html.parser\")\nbonus_bs4_obj.prettify\n\n&lt;bound method Tag.prettify of &lt;!DOCTYPE html&gt;\n\n&lt;html lang=\"en\" xml:lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\"&gt;&lt;head&gt;\n&lt;meta charset=\"utf-8\"/&gt;\n&lt;meta content=\"quarto-1.3.361\" name=\"generator\"/&gt;\n&lt;meta content=\"width=device-width, initial-scale=1.0, user-scalable=yes\" name=\"viewport\"/&gt;\n&lt;title&gt;CSC 380 – quarto-inputa5805b45&lt;/title&gt;\n&lt;style&gt;\ncode{white-space: pre-wrap;}\nspan.smallcaps{font-variant: small-caps;}\ndiv.columns{display: flex; gap: min(4vw, 1.5em);}\ndiv.column{flex: auto; overflow-x: auto;}\ndiv.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}\nul.task-list{list-style: none;}\nul.task-list li input[type=\"checkbox\"] {\n  width: 0.8em;\n  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ \n  vertical-align: middle;\n}\n&lt;/style&gt;\n&lt;script src=\"../site_libs/quarto-nav/quarto-nav.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-nav/headroom.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/clipboard/clipboard.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-search/autocomplete.umd.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-search/fuse.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-search/quarto-search.js\"&gt;&lt;/script&gt;\n&lt;meta content=\"../\" name=\"quarto:offset\"/&gt;\n&lt;script src=\"../site_libs/quarto-html/quarto.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-html/popper.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-html/tippy.umd.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../site_libs/quarto-html/anchor.min.js\"&gt;&lt;/script&gt;\n&lt;link href=\"../site_libs/quarto-html/tippy.css\" rel=\"stylesheet\"/&gt;\n&lt;link href=\"../site_libs/quarto-html/quarto-syntax-highlighting.css\" id=\"quarto-text-highlighting-styles\" rel=\"stylesheet\"/&gt;\n&lt;script src=\"../site_libs/bootstrap/bootstrap.min.js\"&gt;&lt;/script&gt;\n&lt;link href=\"../site_libs/bootstrap/bootstrap-icons.css\" rel=\"stylesheet\"/&gt;\n&lt;link data-mode=\"light\" href=\"../site_libs/bootstrap/bootstrap.min.css\" id=\"quarto-bootstrap\" rel=\"stylesheet\"/&gt;\n&lt;script id=\"quarto-search-options\" type=\"application/json\"&gt;{\n  \"location\": \"sidebar\",\n  \"copy-button\": false,\n  \"collapse-after\": 3,\n  \"panel-placement\": \"start\",\n  \"type\": \"textbox\",\n  \"limit\": 20,\n  \"language\": {\n    \"search-no-results-text\": \"No results\",\n    \"search-matching-documents-text\": \"matching documents\",\n    \"search-copy-link-title\": \"Copy link to search\",\n    \"search-hide-matches-text\": \"Hide additional matches\",\n    \"search-more-match-text\": \"more match in this document\",\n    \"search-more-matches-text\": \"more matches in this document\",\n    \"search-clear-button-title\": \"Clear\",\n    \"search-detached-cancel-button-title\": \"Cancel\",\n    \"search-submit-button-title\": \"Submit\",\n    \"search-label\": \"Search\"\n  }\n}&lt;/script&gt;\n&lt;link href=\"../styles.css\" rel=\"stylesheet\"/&gt;\n&lt;/head&gt;\n&lt;body class=\"nav-sidebar docked\"&gt;\n&lt;div id=\"quarto-search-results\"&gt;&lt;/div&gt;\n&lt;header class=\"headroom fixed-top\" id=\"quarto-header\"&gt;\n&lt;nav class=\"quarto-secondary-nav\"&gt;\n&lt;div class=\"container-fluid d-flex\"&gt;\n&lt;button aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" class=\"quarto-btn-toggle btn\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" data-bs-toggle=\"collapse\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\" type=\"button\"&gt;\n&lt;i class=\"bi bi-layout-text-sidebar-reverse\"&gt;&lt;/i&gt;\n&lt;/button&gt;\n&lt;nav aria-label=\"breadcrumb\" class=\"quarto-page-breadcrumbs\"&gt;&lt;ol class=\"breadcrumb\"&gt;&lt;li class=\"breadcrumb-item\"&gt;\n      CSC 380\n      &lt;/li&gt;&lt;/ol&gt;&lt;/nav&gt;\n&lt;a aria-controls=\"quarto-sidebar\" aria-expanded=\"false\" aria-label=\"Toggle sidebar navigation\" class=\"flex-grow-1\" data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" data-bs-toggle=\"collapse\" onclick=\"if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }\" role=\"button\"&gt;\n&lt;/a&gt;\n&lt;button aria-label=\"\" class=\"btn quarto-search-button\" onclick=\"window.quartoOpenSearch();\" type=\"button\"&gt;\n&lt;i class=\"bi bi-search\"&gt;&lt;/i&gt;\n&lt;/button&gt;\n&lt;/div&gt;\n&lt;/nav&gt;\n&lt;/header&gt;\n&lt;!-- content --&gt;\n&lt;div class=\"quarto-container page-columns page-rows-contents page-layout-article\" id=\"quarto-content\"&gt;\n&lt;!-- sidebar --&gt;\n&lt;nav class=\"sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto\" id=\"quarto-sidebar\"&gt;\n&lt;div class=\"pt-lg-2 mt-2 text-left sidebar-header\"&gt;\n&lt;div class=\"sidebar-title mb-0 py-0\"&gt;\n&lt;a href=\"../\"&gt;CSC 380&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"mt-2 flex-shrink-0 align-items-center\"&gt;\n&lt;div class=\"sidebar-search\"&gt;\n&lt;div class=\"\" id=\"quarto-search\" title=\"Search\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"sidebar-menu-container\"&gt;\n&lt;ul class=\"list-unstyled mt-1\"&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-1\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Course Content&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-1\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-1\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Course_Content/Week_1/home.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Week 1&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Course_Content/Week_2/home.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Week 2&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Course_Content/Week_3/home.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Week 3&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Course_Content/Week_4&amp;5/home.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Week 4 &amp; 5&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-2\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Homework&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-2\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-2\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Homework/1/HW.html\"&gt;\n&lt;span class=\"menu-text\"&gt;HW1: Probability&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Homework/2/HW.html\"&gt;\n&lt;span class=\"menu-text\"&gt;HW2: Data Collection&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-3\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Ethics Discussions&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-3\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-3\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_2.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W2: Political Content&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_3.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W3: Creative Work&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_4.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W4: Mental Health Support&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_6.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W6: Recommendations&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_7.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W7: Autonomous Driving&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_8.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W8: Housekeeping&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Ethics/Week_9.html\"&gt;\n&lt;span class=\"menu-text\"&gt;W9: Facial Recognition&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-4\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Bonus Questions&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-4\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-4\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Bonus/home.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Home&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-5\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Participation&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-5\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-5\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Participation_Activities/home.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Home&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-6\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Honors&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-6\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-6\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Honors/home.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Assignments&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item sidebar-item-section\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a aria-expanded=\"true\" class=\"sidebar-item-text sidebar-link text-start\" data-bs-target=\"#quarto-sidebar-section-7\" data-bs-toggle=\"collapse\"&gt;\n&lt;span class=\"menu-text\"&gt;Syllabus&lt;/span&gt;&lt;/a&gt;\n&lt;a aria-expanded=\"true\" aria-label=\"Toggle section\" class=\"sidebar-item-toggle text-start\" data-bs-target=\"#quarto-sidebar-section-7\" data-bs-toggle=\"collapse\"&gt;\n&lt;i class=\"bi bi-chevron-right ms-2\"&gt;&lt;/i&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;ul class=\"collapse list-unstyled sidebar-section depth1 show\" id=\"quarto-sidebar-section-7\"&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Syllabus/Key_Info.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Key Info&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;li class=\"sidebar-item\"&gt;\n&lt;div class=\"sidebar-item-container\"&gt;\n&lt;a class=\"sidebar-item-text sidebar-link\" href=\"../Syllabus/Syllabus.html\"&gt;\n&lt;span class=\"menu-text\"&gt;Official Syllabus&lt;/span&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/nav&gt;\n&lt;div data-bs-target=\"#quarto-sidebar,#quarto-sidebar-glass\" data-bs-toggle=\"collapse\" id=\"quarto-sidebar-glass\"&gt;&lt;/div&gt;\n&lt;!-- margin-sidebar --&gt;\n&lt;div class=\"sidebar margin-sidebar\" id=\"quarto-margin-sidebar\"&gt;\n&lt;nav class=\"toc-active\" id=\"TOC\" role=\"doc-toc\"&gt;\n&lt;h2 id=\"toc-title\"&gt;On this page&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;a class=\"nav-link active\" data-scroll-target=\"#sonams-dilemma---a-gift-for-few-or-a-chance-for-all\" href=\"#sonams-dilemma---a-gift-for-few-or-a-chance-for-all\" id=\"toc-sonams-dilemma---a-gift-for-few-or-a-chance-for-all\"&gt;Sonam’s Dilemma - A Gift for Few or a Chance for All&lt;/a&gt;\n&lt;ul class=\"collapse\"&gt;\n&lt;li&gt;&lt;a class=\"nav-link\" data-scroll-target=\"#instructions\" href=\"#instructions\" id=\"toc-instructions\"&gt;Instructions&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a class=\"nav-link\" data-scroll-target=\"#question\" href=\"#question\" id=\"toc-question\"&gt;Question&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/nav&gt;\n&lt;/div&gt;\n&lt;!-- main --&gt;\n&lt;main class=\"content\" id=\"quarto-document-content\"&gt;\n&lt;section class=\"level1\" id=\"sonams-dilemma---a-gift-for-few-or-a-chance-for-all\"&gt;\n&lt;h1&gt;Sonam’s Dilemma - A Gift for Few or a Chance for All&lt;/h1&gt;\n&lt;section class=\"level6\" id=\"bonus-question-1-point\"&gt;\n&lt;h6 class=\"anchored\" data-anchor-id=\"bonus-question-1-point\"&gt;Bonus Question : 1 point&lt;/h6&gt;\n&lt;p&gt;&lt;br/&gt;&lt;/p&gt;\n&lt;/section&gt;\n&lt;section class=\"level3\" id=\"instructions\"&gt;\n&lt;h3 class=\"anchored\" data-anchor-id=\"instructions\"&gt;Instructions&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;Due Date : July 7, Friday, 5 pm&lt;/li&gt;\n&lt;li&gt;Submit your answer at D2 &gt; Quiz &gt; “Sonam’s Dilemma : a gift for few or a chance for all”&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section class=\"level3\" id=\"question\"&gt;\n&lt;h3 class=\"anchored\" data-anchor-id=\"question\"&gt;Question&lt;/h3&gt;\n&lt;p&gt;&lt;br/&gt; &lt;em&gt;Source : Story I made up, text improved by ChatGPT, and then edited by myself.&lt;/em&gt;&lt;/p&gt;\n&lt;p&gt;Once upon a time, in the grand kingdom of Razia, there existed a mesmerizing lead dancer named Sonam. She possesed a combination of charm, extraordinary talent, and intelligence that made her stand out amongst her peers. Sonam’s passion for dance was matched only by her love for her dance group,with 2000 other talented dancers, who all shared an unbreakable bond and admired their noble Queen above all.&lt;/p&gt;\n&lt;p&gt;One fine day, as Sonam’s heart overflowed with gratitude for their benevolent ruler, she decided to express her deep admiration through the power of music. She crafted a beautiful song that portrayed the love and adoration her dance group held for Queen Razia. With eager anticipation, Sonam’s troupe dedicated themselves to perfecting their dance moves to match the rhythm and emotion of her heartfelt composition.&lt;/p&gt;\n&lt;p&gt;The auspicious occasion of Queen Razia’s birthday arrived, filling the grand palace with a sense of joy and celebration. The Queen, known for her discerning taste and appreciation for the arts, eagerly awaited the performance of Sonam’s dance group. As the music began to play and the dancers gracefully glided across the grand ballroom, a spell was cast upon all those in attendance.&lt;/p&gt;\n&lt;p&gt;The Queen was captivated by the talent displayed before her, and her heart swelled with pride and delight. She could not help but marvel at the synchronized movements and the sheer passion that emanated from each and every member of the dance group. As the performance reached its crescendo, Queen Razia’s decision was made in an instant—this remarkable group deserved a gift to commemorate their extraordinary dedication.&lt;/p&gt;\n&lt;div class=\"Secret\"&gt;\n&lt;dr_cynthia_breazeal&gt;&lt;/dr_cynthia_breazeal&gt;\n&lt;/div&gt;\n&lt;p&gt;However, there were only a limited number of gift available to give out right now. She presented Sonam with two choices, each with its own intriguing possibilities. The first option was straightforward : 660 randomly chosen dancers would receive a gift. This meant that a select few would be rewarded, leaving the remaining members without a token of appreciation.&lt;/p&gt;\n&lt;p&gt;The second choice, however, offered a twist. Queen Shah proposed distributing the available gifts among all dancers, but with a twist of uncertainty. Each dancer would have a 33% chance of receiving a gift. 67% of not receiving a gift.&lt;/p&gt;\n&lt;p&gt;Now faced with this dilemma, Sonam’s sharp mind began to evaluate the situation. Which option should Sonam pick? Why? Why not the other?&lt;/p&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id=\"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n&lt;/body&gt;&lt;/html&gt;&gt;\n\n\n\nsecret_section = bonus_bs4_obj.find(\"div\",\"Secret\")\nsecret_section\n\n&lt;div class=\"Secret\"&gt;\n&lt;dr_cynthia_breazeal&gt;&lt;/dr_cynthia_breazeal&gt;\n&lt;/div&gt;\n\n\n\n\nQ 3. Clean up the secret code and print it as “The Secret Code is: CSC380” [ 1 point ]. See 10:30 in the Solutions Video\n\nlist(secret_section.children)\n\n['\\n', &lt;dr_cynthia_breazeal&gt;&lt;/dr_cynthia_breazeal&gt;, '\\n']\n\n\n\nsecret_code_tag = list(secret_section.children)[1]\nsecret_code_tag\n\n&lt;dr_cynthia_breazeal&gt;&lt;/dr_cynthia_breazeal&gt;\n\n\n\nsecret_code = secret_code_tag.name\nsecret_code\n\n'dr_cynthia_breazeal'\n\n\n\nprint(\"The Secret Code is: \",secret_code)\n\nThe Secret Code is:  dr_cynthia_breazeal"
  },
  {
    "objectID": "Homework/2/Solutions.html#problem-2.-random-facts-api-11-points-.",
    "href": "Homework/2/Solutions.html#problem-2.-random-facts-api-11-points-.",
    "title": "Homework 2 : Data Collection",
    "section": "Problem 2. Random Facts API [ 11 points ].",
    "text": "Problem 2. Random Facts API [ 11 points ].\n\nRANDOM_FACT_WEBSITE_URL = \"https://uselessfacts.jsph.pl\"\nRANDOM_FACTS_ENDPOINT = \"/api/v2/facts/random\"\nTODAY_RANDOM_FACT_ENDPOINT = \"/api/v2/facts/today\"\n\n\nQ1. Find the URL for random facts API. [1 point]. See 13:38 in the Solutions Video\n\nrandom_facts_api = RANDOM_FACT_WEBSITE_URL+RANDOM_FACTS_ENDPOINT\nrandom_facts_api\n\n'https://uselessfacts.jsph.pl/api/v2/facts/random'\n\n\n\n\nQs. 2,3,4 . Collect 10 Random Facts [3 point]. See 16:48 in the Solutions Video\n\nrandom_facts_list_of_json = []\nnos_of_facts = 10\n\nfor fact_no in range(nos_of_facts):\n\n    random_fact_req_obj = requests.get(random_facts_api)\n\n    if random_fact_req_obj.status_code == 200:\n\n        random_facts_list_of_json.append(random_fact_req_obj.json())\n\n    sleep(2)\n\n\nlen(random_facts_list_of_json)\n\n10\n\n\n\nrandom_facts_list_of_json\n\n[{'id': 'e5ab712476d43f31e76bb1729af8d864',\n  'text': 'The Main Library at Indiana University sinks over an inch every year because when it was built, engineers failed to take into account the weight of all the books that would occupy the building.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/e5ab712476d43f31e76bb1729af8d864'},\n {'id': 'f3b6f48bf9ab74f1937f61e2c00cddfe',\n  'text': 'In 1912 a law passed in Nebraska where drivers in the country at night were required to stop every 150 yards, send up a skyrocket, wait eight minutes for the road to clear before proceeding cautiously, all the while blowing their horn and shooting off flares.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/f3b6f48bf9ab74f1937f61e2c00cddfe'},\n {'id': 'ec0e1f3ff567de996194a62062e5e295',\n  'text': 'Tehran is the most expensive city on earth.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/ec0e1f3ff567de996194a62062e5e295'},\n {'id': 'be3e622e9fee8b4e6e21e1d6d7359edf',\n  'text': 'The shape of plant collenchyma’s cells and the shape of the bubbles in beer foam are the same - they are orthotetrachidecahedrons.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/be3e622e9fee8b4e6e21e1d6d7359edf'},\n {'id': '1173c33c84146fcc7825c04683ce9496',\n  'text': 'One-fourth of the world`s population lives on less than $200 a year.\\xa0 Ninety million people survive on less than $75 a year.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/1173c33c84146fcc7825c04683ce9496'},\n {'id': '3f4bac8655449adc95b7eb89c129e17e',\n  'text': 'Tigers not only have striped fur, they have striped skin!',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/3f4bac8655449adc95b7eb89c129e17e'},\n {'id': '3639a3cf749725c9b903dc0e500be8cc',\n  'text': 'One third of all cancers are sun related.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/3639a3cf749725c9b903dc0e500be8cc'},\n {'id': '3b99d35171dc8fc7abbc1891aab4d6cb',\n  'text': 'The dot over the letter `i` is called a tittle. \\xa0',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/3b99d35171dc8fc7abbc1891aab4d6cb'},\n {'id': '1c7d8f7a7ebb804addf2cbd60b454db7',\n  'text': 'The electric chair was invented by a dentist.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/1c7d8f7a7ebb804addf2cbd60b454db7'},\n {'id': 'ca89075fd3a528c2be2feac0be27383a',\n  'text': 'A spider has transparent blood.',\n  'source': 'djtech.net',\n  'source_url': 'http://www.djtech.net/humor/useless_facts.htm',\n  'language': 'en',\n  'permalink': 'https://uselessfacts.jsph.pl/api/v2/facts/ca89075fd3a528c2be2feac0be27383a'}]\n\n\n\n\nQ.5 Creating the Dataframe [1 point]. See 22:26 in the Solutions Video\n\nrandom_facts_df = pd.DataFrame(random_facts_list_of_json)\nrandom_facts_df.sample(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\ntext\nsource\nsource_url\nlanguage\npermalink\n\n\n\n\n9\nca89075fd3a528c2be2feac0be27383a\nA spider has transparent blood.\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/ca89...\n\n\n2\nec0e1f3ff567de996194a62062e5e295\nTehran is the most expensive city on earth.\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/ec0e...\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nrandom_facts_df\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\ntext\nsource\nsource_url\nlanguage\npermalink\n\n\n\n\n0\ne5ab712476d43f31e76bb1729af8d864\nThe Main Library at Indiana University sinks o...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/e5ab...\n\n\n1\nf3b6f48bf9ab74f1937f61e2c00cddfe\nIn 1912 a law passed in Nebraska where drivers...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/f3b6...\n\n\n2\nec0e1f3ff567de996194a62062e5e295\nTehran is the most expensive city on earth.\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/ec0e...\n\n\n3\nbe3e622e9fee8b4e6e21e1d6d7359edf\nThe shape of plant collenchyma’s cells and the...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/be3e...\n\n\n4\n1173c33c84146fcc7825c04683ce9496\nOne-fourth of the world`s population lives on ...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/1173...\n\n\n5\n3f4bac8655449adc95b7eb89c129e17e\nTigers not only have striped fur, they have st...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/3f4b...\n\n\n6\n3639a3cf749725c9b903dc0e500be8cc\nOne third of all cancers are sun related.\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/3639...\n\n\n7\n3b99d35171dc8fc7abbc1891aab4d6cb\nThe dot over the letter `i` is called a tittle.\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/3b99...\n\n\n8\n1c7d8f7a7ebb804addf2cbd60b454db7\nThe electric chair was invented by a dentist.\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/1c7d...\n\n\n9\nca89075fd3a528c2be2feac0be27383a\nA spider has transparent blood.\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/ca89...\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\nQ 6. Display Full Facts [2 points]. See 24:03 in the Solutions Video\n\nrandom_facts_series = random_facts_df['text'] # Part a\nrandom_facts_series\n\n0    The Main Library at Indiana University sinks o...\n1    In 1912 a law passed in Nebraska where drivers...\n2          Tehran is the most expensive city on earth.\n3    The shape of plant collenchyma’s cells and the...\n4    One-fourth of the world`s population lives on ...\n5    Tigers not only have striped fur, they have st...\n6            One third of all cancers are sun related.\n7    The dot over the letter `i` is called a tittle.  \n8        The electric chair was invented by a dentist.\n9                      A spider has transparent blood.\nName: text, dtype: object\n\n\n\nrandom_facts_list = random_facts_series.to_list() # Part b\nrandom_facts_list\n\n['The Main Library at Indiana University sinks over an inch every year because when it was built, engineers failed to take into account the weight of all the books that would occupy the building.',\n 'In 1912 a law passed in Nebraska where drivers in the country at night were required to stop every 150 yards, send up a skyrocket, wait eight minutes for the road to clear before proceeding cautiously, all the while blowing their horn and shooting off flares.',\n 'Tehran is the most expensive city on earth.',\n 'The shape of plant collenchyma’s cells and the shape of the bubbles in beer foam are the same - they are orthotetrachidecahedrons.',\n 'One-fourth of the world`s population lives on less than $200 a year.\\xa0 Ninety million people survive on less than $75 a year.',\n 'Tigers not only have striped fur, they have striped skin!',\n 'One third of all cancers are sun related.',\n 'The dot over the letter `i` is called a tittle. \\xa0',\n 'The electric chair was invented by a dentist.',\n 'A spider has transparent blood.']\n\n\n\n\nQ 7. Show 3 random facts from the data frame [1 point]. See 25:58\n\nrandom_facts_df.sample(3)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\ntext\nsource\nsource_url\nlanguage\npermalink\n\n\n\n\n1\nf3b6f48bf9ab74f1937f61e2c00cddfe\nIn 1912 a law passed in Nebraska where drivers...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/f3b6...\n\n\n9\nca89075fd3a528c2be2feac0be27383a\nA spider has transparent blood.\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/ca89...\n\n\n3\nbe3e622e9fee8b4e6e21e1d6d7359edf\nThe shape of plant collenchyma’s cells and the...\ndjtech.net\nhttp://www.djtech.net/humor/useless_facts.htm\nen\nhttps://uselessfacts.jsph.pl/api/v2/facts/be3e...\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\nQ8. What is today’s random fact? [3 points]. See 26:29 in the Solutions Video\n\nPart a\n\ntodays_random_fact_api = RANDOM_FACT_WEBSITE_URL+ TODAY_RANDOM_FACT_ENDPOINT\ntodays_random_fact_api\n\n'https://uselessfacts.jsph.pl/api/v2/facts/today'\n\n\n\ntoday_fact_req_obj = requests.get(todays_random_fact_api)\ntoday_fact_req_obj.status_code\n\n200\n\n\n\nrandom_fact_of_the_day = today_fact_req_obj.json()['text'] #Part a\nrandom_fact_of_the_day\n\n'Ants closely resemble human manners:\\xa0 When they wake, they stretch & appear to yawn in a human manner before taking up the tasks of the day.'\n\n\n\n\nPart b\n\ntime_rn = datetime.today()\nprint('Time right now is :',time_rn.strftime(\"%Y-%m-%d %I:%M:%S %p\"))\n\nTime right now is : 2023-07-13 02:52:59 AM\n\n\n\n\nPart c\n\nprint(\"At\", time_rn.strftime(\"%Y-%m-%d %I:%M:%S %p\"), \" the random fact of the day is \",random_fact_of_the_day)\n\nAt 2023-07-13 02:52:59 AM  the random fact of the day is  Ants closely resemble human manners:  When they wake, they stretch & appear to yawn in a human manner before taking up the tasks of the day."
  },
  {
    "objectID": "Homework/2/Solutions.html#part-3.-movies-and-shows-29-points",
    "href": "Homework/2/Solutions.html#part-3.-movies-and-shows-29-points",
    "title": "Homework 2 : Data Collection",
    "section": "Part 3. Movies and Shows [29 points]",
    "text": "Part 3. Movies and Shows [29 points]\n\nQ1. Download the following dataset [2 points]. See 34:18 in the Solutions Video\n\ndataset_names = ['hulu','disney','prime','netflix']\n\n\nfor dataset_name in dataset_names:\n    with zipfile.ZipFile(dataset_name+\".zip\",\"r\") as zip_ref:\n        zip_ref.extractall(dataset_name)\n\n\n\nQ2. Create one large dataframe [3 points]. See 39:10 in the Solutions Video\n\nPart a\n\nhulu_df = pd.read_csv('hulu/hulu_titles.csv')\nhulu_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n1058\ns1059\nTV Show\nWWE Main Event\nNaN\nNaN\nUnited States\nJanuary 7, 2021\n2012\nTV-PG\n8 Seasons\nSports\nJoin us each week for WWE Main Event, as the S...\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nnetflix_df = pd.read_csv('netflix/netflix_titles.csv')\nnetflix_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n6487\ns6488\nTV Show\nCity in the Sky\nNaN\nNaN\nUnited Kingdom\nOctober 1, 2017\n2016\nTV-PG\n1 Season\nBritish TV Shows, Docuseries, Science & Nature TV\nThis series explores the magnitude and scale o...\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nprime_df = pd.read_csv('prime/amazon_prime_titles.csv')\nprime_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n7912\ns7913\nMovie\nOne More Saturday Night\nDennis Klein\nTom Davis, Al Franken, Moira Sinise, Frank How...\nNaN\nNaN\n1986\nR\n96 min\nComedy\nThe problems faced by both teenagers and adult...\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\ndisney_df = pd.read_csv('disney/disney_plus_titles.csv')\ndisney_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\n\n\n\n\n1020\ns1021\nMovie\nLilo & Stitch\nChristopher Sanders, Dean DeBlois\nDaveigh Chase, Christopher Sanders, Tia Carrer...\nUnited States\nNovember 12, 2019\n2002\nTV-PG\n86 min\nAction-Adventure, Animation, Family\nA little girl adopts a wanted alien wreaking h...\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\nPart b\n\nhulu_df['Platform'] = \"Hulu\"\nnetflix_df['Platform'] = \"Netflix\"\ndisney_df['Platform'] = \"Disney\"\nprime_df['Platform'] = \"Prime\"\n\n\nhulu_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n1193\ns1194\nTV Show\nA Sap Story\nNaN\nNaN\nNaN\nNovember 13, 2020\n2016\nNaN\n1 Season\nNews\nCancel your plans, there’s sap coming out of a...\nHulu\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nnetflix_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n6818\ns6819\nMovie\nG-Force\nHoyt Yeatman\nBill Nighy, Will Arnett, Zach Galifianakis, Ke...\nUnited States\nMarch 15, 2019\n2009\nPG\n88 min\nChildren & Family Movies, Comedies\nWhen a billionaire sets out to take over the w...\nNetflix\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\ndisney_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n1135\ns1136\nMovie\nPixel Perfect\nMark Dippé\nRicky Ullman, Leah Pipes, Spencer Redford, Chr...\nUnited States\nNovember 12, 2019\n2004\nTV-G\n90 min\nComedy, Coming of Age, Music\nA techno whiz must protect his hologram from f...\nDisney\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nprime_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n7077\ns7078\nMovie\nJapanese Story\nSue Brooks\nToni Collette, Otaro Tsunashima, Matthew Dykty...\nNaN\nNaN\n2003\nR\n99 min\nArthouse, Arts, Entertainment, and Culture, Drama\nSandy (Toni Collette), a geologist, finds hers...\nPrime\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\nPart c\n\nall_platforms_df = pd.concat([prime_df,netflix_df,disney_df,hulu_df])\nall_platforms_df.sample(5)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n6181\ns6182\nMovie\nApril and the Extraordinary World\nChristian Desmares, Franck Ekinci\nAngela Galuppo, Paul Giamatti, Susan Sarandon,...\nFrance, Canada, Belgium\nOctober 1, 2017\n2015\nPG\n107 min\nAction & Adventure, Comedies, International Mo...\nAfter the world's top scientists disappear, or...\nNetflix\n\n\n2290\ns2291\nMovie\nTanu Weds Manu\nAanand Rai\nMadhavan, Kangana Ranaut, Jimmy Shergill, Deep...\nIndia\nJuly 5, 2020\n2011\nTV-14\n114 min\nComedies, Dramas, International Movies\nWhen London-based doctor Manu reluctantly retu...\nNetflix\n\n\n4278\ns4279\nMovie\nAggretsuko: We Wish You a Metal Christmas\nRarecho\nKaolip, Shingo Kato, Komegumi Koiwasaki, Maki ...\nJapan\nDecember 20, 2018\n2018\nTV-PG\n22 min\nMovies\nWhile Retsuko desperately makes plans for Chri...\nNetflix\n\n\n1802\ns1803\nTV Show\nThe Other Guy\nNaN\nNaN\nAustralia\nFebruary 14, 2020\n2017\nTV-PG\n2 Seasons\nComedy, Drama, International\nA successful radio host finds himself unexpect...\nHulu\n\n\n2403\ns2404\nMovie\nGalapagos: Realm of Giant Sharks\nThomas Lucas\nNaN\nNaN\nNaN\n2012\nALL\n53 min\nDocumentary\nThis visually rich production follows a group ...\nPrime\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\n\nQ3. Longest show and movie [6 points]. See 44:49 for part a, and 55:20 for part b, in the Solutions Video\n\nPart a\n\nshows_df = all_platforms_df[all_platforms_df['type']=='TV Show']\nshows_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n2137\ns2138\nTV Show\nLight as a Feather\nNaN\nNaN\nUnited States\nJuly 26, 2019\n2018\nTV-14\n2 Seasons\nDrama, Horror, Teen\nAn innocent game of “Light as a Feather, Stiff...\nHulu\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nshows_count_df = shows_df.value_counts(\"duration\").to_frame()\nshows_count_df.sample(3)\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n\n\nduration\n\n\n\n\n\n4 Seasons\n280\n\n\n19 Seasons\n3\n\n\n26 Seasons\n1\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nshows_count_df.reset_index(inplace=True)\nshows_count_df.sample(3)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nduration\n0\n\n\n\n\n24\n26 Seasons\n1\n\n\n12\n13 Seasons\n6\n\n\n15\n16 Seasons\n4\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nshows_count_df = shows_count_df.rename({\n    'duration' : 'Number of seasons',\n    0: 'No of shows'\n    }, axis = 1)\n\nshows_count_df.sample(3)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nNumber of seasons\nNo of shows\n\n\n\n\n22\n32 Seasons\n1\n\n\n6\n7 Seasons\n89\n\n\n0\n1 Season\n4183\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nprint(\"Before preprocessing , Longest running season appears to be :\",shows_count_df['Number of seasons'].max())\n\nBefore preprocessing , Longest running season appears to be : 9 Seasons\n\n\nBonus\n\nshows_count_df['Number of seasons'] = shows_count_df['Number of seasons'].apply(lambda x : int(x.replace('Seasons','').replace('Season','')))\nshows_count_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nNumber of seasons\nNo of shows\n\n\n\n\n6\n7\n89\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nprint(\"After preprocessing , Longest running season appears to be :\",shows_count_df['Number of seasons'].max(), \"Seasons\")\n\nAfter preprocessing , Longest running season appears to be : 34 Seasons\n\n\n\n\nPart b\n\nmovies_df = all_platforms_df[all_platforms_df['type']=='Movie']\nmovies_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n2595\ns2596\nMovie\nDwayne Perkins: Take Note\nIan Harris\nDwayne Perkins\nNaN\nNaN\n2016\n18+\n68 min\nArts, Entertainment, and Culture, Comedy, Spec...\nDwayne delivers his first hour special with a ...\nPrime\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nmovies_count_df = movies_df.value_counts(\"duration\").to_frame()\nmovies_count_df.sample(3)\n\n\n\n  \n    \n      \n\n\n\n\n\n\n0\n\n\nduration\n\n\n\n\n\n25 min\n25\n\n\n69 min\n66\n\n\n229 min\n1\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nmovies_count_df.reset_index(inplace=True)\nmovies_count_df.sample(3)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nduration\n0\n\n\n\n\n184\n175 min\n5\n\n\n159\n160 min\n13\n\n\n100\n145 min\n43\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nmovies_count_df = movies_count_df.rename({\n    'duration' : 'Duration in minutes',\n    0: 'No of movies'\n    }, axis = 1)\n\nmovies_count_df.sample(3)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nDuration in minutes\nNo of movies\n\n\n\n\n88\n48 min\n54\n\n\n31\n111 min\n153\n\n\n55\n125 min\n87\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nPart i\n\nmovies_count_df_i = movies_count_df.sort_values(by=\"Duration in minutes\", ascending=False)\nmovies_count_df_i\n\n\n\n  \n    \n      \n\n\n\n\n\n\nDuration in minutes\nNo of movies\n\n\n\n\n12\n99 min\n288\n\n\n13\n98 min\n286\n\n\n8\n97 min\n324\n\n\n10\n96 min\n303\n\n\n7\n95 min\n340\n\n\n...\n...\n...\n\n\n16\n101 min\n253\n\n\n14\n100 min\n260\n\n\n126\n10 min\n25\n\n\n165\n1 min\n11\n\n\n166\n0 min\n10\n\n\n\n\n\n225 rows × 2 columns\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nlongest_movie_duration = movies_count_df_i.head(1)['Duration in minutes'].to_list()[0]\nprint(\"Longest Movie Duration is : \",longest_movie_duration)\n\nLongest Movie Duration is :  99 min\n\n\n\nlongest_movies_df = movies_df[movies_df[\"duration\"] == longest_movie_duration ]\nlongest_movies_df.sample(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n4293\ns4294\nMovie\nCaught Up\nDarin Scott\nBokeem Woodbine, Snoop Doggy Dogg, LL Cool J, ...\nNaN\nNaN\n1998\n16+\n99 min\nAction, Drama, Suspense\nAfter being released from prison a man finds h...\nPrime\n\n\n5277\ns5278\nMovie\n#realityhigh\nFernando Lebrija\nNesta Cooper, Kate Walsh, John Michael Higgins...\nUnited States\nSeptember 8, 2017\n2017\nTV-14\n99 min\nComedies\nWhen nerdy high schooler Dani finally attracts...\nNetflix\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nprint(\"Longest Movie Duration is : \",longest_movie_duration)\nprint(\"The Movies are : \",\"\\n\".join(longest_movies_df['title'].to_list()))\n\nLongest Movie Duration is :  99 min\nThe Movies are :  Woman Of Desire\nWhy We Fight\nValley Uprising\nThe Zookeeper\nThe Woman in Black 2: Angel of Death\nThe Ultimate Legacy\nThe Italian Job (1969)\nThe Hungry\nThe Donkey King\nSuper Size Me\nSunshine Hotel\nStreet Dance\nStorm Boy\nShottas\nOur Town\nMy Foolish Heart\nLine of Duty\nHonest Thief\nHiding Out\nHellbound: Hellraiser 2\nHearts in Bondage\nHappythankyoumoreplease\nGrace of God\nGo Fast. Go North.\nDoe\nDigging to China\nBetter Luck Tomorrow\nBelle and Sebastian\nAtlas Shrugged: Part III\nAnna\n23 Blast\nThe Adventurer: The Curse of the Midas Box\nHallowed Be Thy Name\nMadness in the Method\nHick\nAssault on Wall Street\nThe Baytown Outlaws\nRolling Stone: Life And Death Of Brian Jones\nDark Was the Night\nCaught Up\nStinger\nGoodbye, Butterfly\nLittle Kingdom\nPERSECUTION\nBlame it On Fidel\nReversion\nHappy Home\nSuper Fast\nIntersection\nTusks\nThe Shootist\nDeepStar Six\nFirefly\nRock the Casbah\nNightingale: A Melody of Life\nToys Storage 2\nHoop Soldiers\nThe Naked Truth\nAlejandro\nP!nk: All I Know So Far\nVivarium\nThaen\nCBI Vs Lovers\nNight Hunter\nDIVOS!\nThe Map of Tiny Perfect Things\nThe Booksellers\nSeven Days In Utopia\nTales of an American Hoodrat\nRoad to Damascus\nPerfect Strangers\nBoonie Bears: The Big Top Secret\nPunarjagran\nSomeone to Carry Me\nKansas City Confidential\nThe Silent Mountain\nPlayback\nTobruk\nMay Morning\nThe Grotto\nTransmigrate (The Troubled One)\nB. A. Pass\nSideshow\nAngel And The Badman\nPartners\nM.O.M. Mothers of Monsters\nBottom Feeders\nRun For The Sun\nAll About Steve\nJapanese Story\nAzhaggiye Thee\nAcross The Line\nNot That Funny\nThe Woman In The Window\nIlakku\nA Social Cure\nBlack and White\nCruel Train\nDelicatessen\nAbsence of The Good\nFair Play\nThe Boss\nPhysical Evidence\nBad Impulse\nMara\nChinese Box\nBlue Juice\nRoute 24\nRebirth\nThe Descent\nPaper Dragons\nAlienated\nThe Bloody Vampire\nPasser By\nDeath Machine\nThe Omega Code\nGolden Job\nMadison\nLittle Man Tate\nTotò Eva e il pennello proibito\nThe Lady Doctor\nThe Intruder\nOutlaws: The Legend of O.B. Taggart\nGli Onorevoli\nNo Loss, No Gain\nThe Sacrament\nRed Sun Rising\nDriven to Dance\nThe Trench\nOng Bak 3\nDead Serious\nWhat Happens in Vegas\nThe Rocky Horror Picture Show\nInuYasha the Movie 2: The Castle Beyond the Looking Glass\nInuYasha the Movie 3: Swords of an Honorable Ruler\nHere and There\nGood Luck Chuck\nMy Girl 2\nCousins\nRogue Warfare: Death of a Nation\nCopenhagen\nBlack Holes | The Edge of All We Know\nNinja Assassin\nSwordfish\n678\nCinema Bandi\nDance of the Forty One\nMonster\nDead Again in Tombstone\nThe Whole Nine Yards\nSniper: Ghost Shooter\nThe Block Island Sound\nThe Misadventures of Hedi and Cokeman\nIs Love Enough? Sir\nTony Parker: The Final Shot\nU-Turn\nDolly Parton’s Christmas on the Square\nZozo\nA Babysitter's Guide to Monster Hunting\n#Alive\nWhat Keeps You Alive\nRide Like a Girl\nFreej Al Taibeen\nCurtiz\nMindGamers\nA Trip to Jamaica\nIn My Country\nUp North\nThe Bling Lagosians\nRim of the World\nNappily Ever After\nStudio 54\nThe Last Laugh\nAll's Well, End's Well (2009)\nErrementari: The Blacksmith and the Devil\nHarishchandrachi Factory\nThi Mai\nI Am not an Easy Man\nLayla M.\nBad Day for the Cut\nThe Worthy\n#realityhigh\nIn the Shadow of Iris\nSlam\nLife 2.0\nOperações Especiais\nTini: The New Life of Violetta\nJourney to Greenland\nMercenary\nWelcome Mr. President\nAudrie & Daisy\nWinter on Fire: Ukraine's Fight for Freedom\nÇok Filim Hareketler Bunlar\n50 First Dates\nA.X.L.\nAdrishya\nAmerican Hangman\nB.A. Pass\nBarely Lethal\nBecause We're Heading Out\nBeing AP\nBerlin Kaplani\nBobbi Jene\nBobby Robson: More Than a Manager\nBolt\nCandyman\nChasing Trane\nChristian Mingle\nChristmas with the Kranks\nDemocrats\nDetention Letter\nDon't Be Afraid of the Dark\nDriven to Dance\nEyyvah Eyyvah\nFamiliye\nFireflies\nFroning: The Fittest Man in History\nGood Luck\nHangman\nHaunting on Fraternity Row\nIndiscretion\nInuyasha the Movie - La spada del dominatore del mondo\nInuYasha: The Movie 2: The Castle Beyond the Looking Glass\nKaleidoscope\nKnock Knock\nKung Fu Hustle\nLike Arrows\nMad Money\nMara\nMay We Chat\nMy Dog is My Guide\nMy Scientology Movie\nMy Week with Marilyn\nMy Wife and My Wifey\nNowhere Boy\nRace to Witch Mountain\nReaction\nSalem: His Sister's Father\nSay When\nThe Aerial\nThe Croods\nThe Damned Rain\nThe Drowning\nThe Heroes of Evil\nThe Intent\nThe Invention of Lying\nThe Nutcracker and the Four Realms\nThe Sapphires\nThe Tuxedo\nTremors 5:  Bloodline\nVS.\nMaleficent\nDan in Real Life\nBaby's Day Out\nRio\nCheaper By the Dozen\nSecret Society of Second-Born Royals\nMorning Light\nThe Princess Bride\nCool Runnings\nGeek Charming\nIce Princess\nThe Princess and the Frog\nWALL-E\nMaggie's Plan\nClass\nIntersection\nLight It Up\nThe Rocky Horror Picture Show\nTransporter 3\nGirls! Girls! Girls!\nChasing Trane: The John Coltrane Documentary\nHer Name Is Chef\nThe Donut King\nPriceless\nSonic the Hedgehog\nEmperor\nBernie the Dolphin 2\nLucky Day\n'71\nThe Wolf Hour\nVault\nBrian Banks\nHellbound: Hellraiser II\nUntouchable\nAsk Dr. Ruth\nDriverX\nDust 2 Glory\n\n\nPart ii\n\nmovies_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n8078\ns8079\nMovie\nSt. Agatha\nDarren Lynn Bousman\nSabrina Kern, Carolyn Hennesy, Courtney Halver...\nUnited States\nAugust 8, 2019\n2018\nTV-MA\n103 min\nHorror Movies, Independent Movies\nUnwed and pregnant, a young woman flees her ab...\nNetflix\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nmovies_count_df_ii = movies_df[\"duration\"].value_counts().to_frame()\nmovies_count_df_ii.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nduration\n\n\n\n\n20 min\n25\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nlongest_duration_for_movies = movies_count_df_ii.sort_index().tail(1).index[0]\nlongest_duration_for_movies\n\n'99 min'\n\n\n\nlongest_movies_df = movies_df[movies_df[\"duration\"] == longest_movie_duration ]\nlongest_movies_df.sample(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n875\ns876\nMovie\nStreet Dance\nDania Pasquini, Max Giwa\nNichola Burley, Richard Winsor, Charlotte Ramp...\nNaN\nNaN\n2013\n13+\n99 min\nArts, Entertainment, and Culture, Drama\nA high-energy dance drama about a street dance...\nPrime\n\n\n2352\ns2353\nMovie\nDust 2 Glory\nNaN\nNaN\nUnited States\nNovember 15, 2018\n2017\nNaN\n99 min\nDocumentaries\nFilmmaker Dana Brown explores the SCORE Baja 1...\nHulu\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nprint(\"Longest Movie Duration is : \",longest_movie_duration)\nprint(\"The Movies are : \",\"\\n\".join(longest_movies_df['title'].to_list()))\n\nLongest Movie Duration is :  99 min\nThe Movies are :  Woman Of Desire\nWhy We Fight\nValley Uprising\nThe Zookeeper\nThe Woman in Black 2: Angel of Death\nThe Ultimate Legacy\nThe Italian Job (1969)\nThe Hungry\nThe Donkey King\nSuper Size Me\nSunshine Hotel\nStreet Dance\nStorm Boy\nShottas\nOur Town\nMy Foolish Heart\nLine of Duty\nHonest Thief\nHiding Out\nHellbound: Hellraiser 2\nHearts in Bondage\nHappythankyoumoreplease\nGrace of God\nGo Fast. Go North.\nDoe\nDigging to China\nBetter Luck Tomorrow\nBelle and Sebastian\nAtlas Shrugged: Part III\nAnna\n23 Blast\nThe Adventurer: The Curse of the Midas Box\nHallowed Be Thy Name\nMadness in the Method\nHick\nAssault on Wall Street\nThe Baytown Outlaws\nRolling Stone: Life And Death Of Brian Jones\nDark Was the Night\nCaught Up\nStinger\nGoodbye, Butterfly\nLittle Kingdom\nPERSECUTION\nBlame it On Fidel\nReversion\nHappy Home\nSuper Fast\nIntersection\nTusks\nThe Shootist\nDeepStar Six\nFirefly\nRock the Casbah\nNightingale: A Melody of Life\nToys Storage 2\nHoop Soldiers\nThe Naked Truth\nAlejandro\nP!nk: All I Know So Far\nVivarium\nThaen\nCBI Vs Lovers\nNight Hunter\nDIVOS!\nThe Map of Tiny Perfect Things\nThe Booksellers\nSeven Days In Utopia\nTales of an American Hoodrat\nRoad to Damascus\nPerfect Strangers\nBoonie Bears: The Big Top Secret\nPunarjagran\nSomeone to Carry Me\nKansas City Confidential\nThe Silent Mountain\nPlayback\nTobruk\nMay Morning\nThe Grotto\nTransmigrate (The Troubled One)\nB. A. Pass\nSideshow\nAngel And The Badman\nPartners\nM.O.M. Mothers of Monsters\nBottom Feeders\nRun For The Sun\nAll About Steve\nJapanese Story\nAzhaggiye Thee\nAcross The Line\nNot That Funny\nThe Woman In The Window\nIlakku\nA Social Cure\nBlack and White\nCruel Train\nDelicatessen\nAbsence of The Good\nFair Play\nThe Boss\nPhysical Evidence\nBad Impulse\nMara\nChinese Box\nBlue Juice\nRoute 24\nRebirth\nThe Descent\nPaper Dragons\nAlienated\nThe Bloody Vampire\nPasser By\nDeath Machine\nThe Omega Code\nGolden Job\nMadison\nLittle Man Tate\nTotò Eva e il pennello proibito\nThe Lady Doctor\nThe Intruder\nOutlaws: The Legend of O.B. Taggart\nGli Onorevoli\nNo Loss, No Gain\nThe Sacrament\nRed Sun Rising\nDriven to Dance\nThe Trench\nOng Bak 3\nDead Serious\nWhat Happens in Vegas\nThe Rocky Horror Picture Show\nInuYasha the Movie 2: The Castle Beyond the Looking Glass\nInuYasha the Movie 3: Swords of an Honorable Ruler\nHere and There\nGood Luck Chuck\nMy Girl 2\nCousins\nRogue Warfare: Death of a Nation\nCopenhagen\nBlack Holes | The Edge of All We Know\nNinja Assassin\nSwordfish\n678\nCinema Bandi\nDance of the Forty One\nMonster\nDead Again in Tombstone\nThe Whole Nine Yards\nSniper: Ghost Shooter\nThe Block Island Sound\nThe Misadventures of Hedi and Cokeman\nIs Love Enough? Sir\nTony Parker: The Final Shot\nU-Turn\nDolly Parton’s Christmas on the Square\nZozo\nA Babysitter's Guide to Monster Hunting\n#Alive\nWhat Keeps You Alive\nRide Like a Girl\nFreej Al Taibeen\nCurtiz\nMindGamers\nA Trip to Jamaica\nIn My Country\nUp North\nThe Bling Lagosians\nRim of the World\nNappily Ever After\nStudio 54\nThe Last Laugh\nAll's Well, End's Well (2009)\nErrementari: The Blacksmith and the Devil\nHarishchandrachi Factory\nThi Mai\nI Am not an Easy Man\nLayla M.\nBad Day for the Cut\nThe Worthy\n#realityhigh\nIn the Shadow of Iris\nSlam\nLife 2.0\nOperações Especiais\nTini: The New Life of Violetta\nJourney to Greenland\nMercenary\nWelcome Mr. President\nAudrie & Daisy\nWinter on Fire: Ukraine's Fight for Freedom\nÇok Filim Hareketler Bunlar\n50 First Dates\nA.X.L.\nAdrishya\nAmerican Hangman\nB.A. Pass\nBarely Lethal\nBecause We're Heading Out\nBeing AP\nBerlin Kaplani\nBobbi Jene\nBobby Robson: More Than a Manager\nBolt\nCandyman\nChasing Trane\nChristian Mingle\nChristmas with the Kranks\nDemocrats\nDetention Letter\nDon't Be Afraid of the Dark\nDriven to Dance\nEyyvah Eyyvah\nFamiliye\nFireflies\nFroning: The Fittest Man in History\nGood Luck\nHangman\nHaunting on Fraternity Row\nIndiscretion\nInuyasha the Movie - La spada del dominatore del mondo\nInuYasha: The Movie 2: The Castle Beyond the Looking Glass\nKaleidoscope\nKnock Knock\nKung Fu Hustle\nLike Arrows\nMad Money\nMara\nMay We Chat\nMy Dog is My Guide\nMy Scientology Movie\nMy Week with Marilyn\nMy Wife and My Wifey\nNowhere Boy\nRace to Witch Mountain\nReaction\nSalem: His Sister's Father\nSay When\nThe Aerial\nThe Croods\nThe Damned Rain\nThe Drowning\nThe Heroes of Evil\nThe Intent\nThe Invention of Lying\nThe Nutcracker and the Four Realms\nThe Sapphires\nThe Tuxedo\nTremors 5:  Bloodline\nVS.\nMaleficent\nDan in Real Life\nBaby's Day Out\nRio\nCheaper By the Dozen\nSecret Society of Second-Born Royals\nMorning Light\nThe Princess Bride\nCool Runnings\nGeek Charming\nIce Princess\nThe Princess and the Frog\nWALL-E\nMaggie's Plan\nClass\nIntersection\nLight It Up\nThe Rocky Horror Picture Show\nTransporter 3\nGirls! Girls! Girls!\nChasing Trane: The John Coltrane Documentary\nHer Name Is Chef\nThe Donut King\nPriceless\nSonic the Hedgehog\nEmperor\nBernie the Dolphin 2\nLucky Day\n'71\nThe Wolf Hour\nVault\nBrian Banks\nHellbound: Hellraiser II\nUntouchable\nAsk Dr. Ruth\nDriverX\nDust 2 Glory\n\n\nPart iii\n\nmovies_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n1999\ns2000\nMovie\nJourney Through The Stars\nMark Knight\nNaN\nNaN\nNaN\n2020\nALL\n60 min\nDocumentary, Special Interest\nAn out-of-this-world experience where we dance...\nPrime\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nmovies_df['duration'].max()\n\nTypeError: ignored\n\n\n\nmovies_df.idxmax(axis=\"columns\")\n\nTypeError: ignored\n\n\nMax function tried to compare values in the column. But the column had two datatypes in it - string and float( for null values ). Comparison was not a permitted operation between string and float. Hence the error\n\n\n\nQ 4.Shows streaming on multiple platforms [7 points]. See 1:19:12 in the Solutions Video\n\nall_platforms_df.sample()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n2235\ns2236\nTV Show\nDark Desire\nNaN\nMaite Perroni, Erik Hayser, Alejandro Speitzer...\nMexico\nJuly 15, 2020\n2020\nTV-MA\n1 Season\nCrime TV Shows, International TV Shows, Spanis...\nMarried Alma spends a fateful weekend away fro...\nNetflix\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nPart a\n\nprint(\"The number of rows is :\",all_platforms_df.shape[0])\n\nThe number of rows is : 22998\n\n\nPart b\n\nunique_titles = all_platforms_df['title'].unique()\nunique_titles\n\narray(['The Grand Seduction', 'Take Care Good Night',\n       'Secrets of Deception', ..., 'Star Trek: The Original Series',\n       'The Twilight Zone', 'Tokyo Magnitude 8.0'], dtype=object)\n\n\n\nprint('Number of unique titles :',len(unique_titles))\n\nNumber of unique titles : 22115\n\n\n\n# alternative solution from student submissions\nprint(f\"Number of unique titles: {all_platforms_df['title'].nunique()}\")\n\nNumber of unique titles: 22115\n\n\nPart c\n\ntitles_count_df = all_platforms_df['title'].value_counts().to_frame()\ntitles_count_df.sample(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\ntitle\n\n\n\n\nOnce Upon a Time in China III\n1\n\n\nCatch-22\n2\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nPart d\n\ntitles_count_df.reset_index(inplace=True)\n\n\ntitles_count_df.rename({\n    \"index\":\"name\"\n}, axis = 1, inplace = True)\n\n\ntitles_count_df.sample(5)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nname\ntitle\n\n\n\n\n6530\nAmerica's Future: The Power of the Latino Vote\n1\n\n\n1021\nFlavors of Youth: International Version\n1\n\n\n13793\nRock the Casbah\n1\n\n\n7422\nSon of Zorn\n1\n\n\n12259\nCelebrity Close Calls\n1\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nPart e\n\ntitles_count_df.rename({\n    'name' : \"Movie or Show Name\",\n    \"title\" : \"No of Platforms\"\n}, axis = 1, inplace = True)\n\n\ntitles_count_df.sample(5)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nMovie or Show Name\nNo of Platforms\n\n\n\n\n20247\nJeevan Ek Sanghursh\n1\n\n\n21334\nUnder Crystal Lake\n1\n\n\n4994\nSisters\n1\n\n\n18946\nChhota Bheem & Krishna: Mayanagari\n1\n\n\n17751\nGajendra\n1\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nPart f\n\nmax_platform_shows = shows_df['title'].value_counts().to_frame()['title'].max()\nmax_platform_shows\n\n3\n\n\n\nprint(\"The Maximum number of platforms a show is on is \",max_platform_shows)\n\nThe Maximum number of platforms a show is on is  3\n\n\n\n\nQ5. Favorite show or movie [2 points]. See 1:31:20 in the Solutions Video\nPart a\n\nduplicated_shows = shows_df[shows_df.duplicated(subset=['title'], keep = False)]\nduplicated_shows.sample(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n8117\ns8118\nTV Show\nSuits\nNaN\nJang Dong-gun, Park Hyung-sik, Jin Hee-kyung, ...\nSouth Korea\nDecember 1, 2019\n2018\nTV-MA\n1 Season\nInternational TV Shows, Korean TV Shows, TV Co...\nA renowned corporate attorney at a prestigious...\nNetflix\n\n\n326\ns327\nTV Show\nCosmos: Possible Worlds\nNaN\nNeil deGrasse Tyson\nUnited States\nDecember 25, 2020\n2020\nTV-14\n1 Season\nAction-Adventure, Docuseries, Family\nCOSMOS: POSSIBLE WORLDS continues Carl Sagan’s...\nDisney\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nPart b\n\nfavorite_movie = 'Everything Everywhere All at Once'\n\n\nall_platforms_df[all_platforms_df['title'].apply(lambda x : True if x.lower()==favorite_movie.lower() else False )]\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nrandom_row = all_platforms_df.sample(1)\nrandom_title = random_row['title'].values[0]\nprint(random_title)\nrandom_row\n\nFear Street Part 2: 1978\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n479\ns480\nMovie\nFear Street Part 2: 1978\nLeigh Janiak\nSadie Sink, Emily Rudd, Ryan Simpkins, McCabe ...\nNaN\nJuly 9, 2021\n2021\nR\n111 min\nHorror Movies\nIn the cursed town of Shadyside, a killer's mu...\nNetflix\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nall_platforms_df[all_platforms_df['title'] == random_title]\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n479\ns480\nMovie\nFear Street Part 2: 1978\nLeigh Janiak\nSadie Sink, Emily Rudd, Ryan Simpkins, McCabe ...\nNaN\nJuly 9, 2021\n2021\nR\n111 min\nHorror Movies\nIn the cursed town of Shadyside, a killer's mu...\nNetflix\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\n\nQ 6. Save Data [1 points]. See 1:39:11 in the Solutions Video\n\ntarget_file_name = \"streaming_titles.csv\"\nall_platforms_df.to_csv(target_file_name,index=False)\n\n\n\nQ 7 . Name starts with [8 points]. See 1:39:29 in the Solutions Video\nPart a\n\nstreaming_titles_df = pd.read_csv(target_file_name)\nstreaming_titles_df.sample(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n5757\ns5758\nMovie\nWildflower\nNicholas DiBella\nNathalia Ramos, Cody Longo, Shari Rigby, Alex ...\nNaN\nNaN\n2016\nPG-13\n92 min\nDrama, Suspense\nWhen a college student starts having a reoccur...\nPrime\n\n\n19196\ns722\nMovie\nAlice in Wonderland\nClyde Geronimi, Hamilton Luske, Wilfred Jackson\nEd Wynn, Richard Haydn, Sterling Holloway, Jer...\nUnited States\nNovember 12, 2019\n1951\nG\n79 min\nAction-Adventure, Animation, Family\nJoin Alice as she chases the White Rabbit into...\nDisney\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nPart b\n\nfirst_name = \"Enfa\"\nfirst_letter_match_movies = movies_df[movies_df['title'].str.startswith(first_name[0])]\nfirst_letter_match_movies.sample(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n6721\ns6722\nMovie\nExit Strategy\nMichael Whitton\nJameel Saleem, Kimelia Weathers, Quincy Harris...\nUnited States\nMarch 3, 2019\n2012\nNR\n79 min\nComedies, Romantic Movies\nWhen James's living situation turns sour, he h...\nNetflix\n\n\n4188\ns4189\nMovie\nEchcharikkai\nSarjun\nSathyaraj, Varalakshmi Sarathkumar, Kishore Ku...\nIndia\nJanuary 15, 2019\n2018\nTV-14\n127 min\nInternational Movies, Thrillers\nAfter kidnapping a millionaire’s daughter, two...\nNetflix\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nPart c\n\nfirst_letter_match_shows = shows_df[shows_df['title'].str.startswith(first_name[0])]\nfirst_letter_match_shows.sample(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n2583\ns2584\nTV Show\nEd Gamble: Blood Sugar\nNaN\nNaN\nNaN\nNaN\n2019\n16+\n1 Season\nArts, Entertainment, and Culture\nStand-up comedian Ed Gamble's first special, B...\nPrime\n\n\n6690\ns6691\nTV Show\nEmogenius\nNaN\nHunter March\nUnited States\nDecember 15, 2018\n2017\nTV-PG\n1 Season\nReality TV\nTwo pairs of contestants go head-to-head for s...\nNetflix\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nprint(\"Number of unique shows that start with my first name first letter are : \",len(first_letter_match_shows['title'].unique()))\n\nNumber of unique shows that start with my first name first letter are :  140\n\n\nPart d\n\nTA_first_letter = 'B'\n\n\nfirst_letter_match_movies = movies_df[movies_df['title'].str.startswith(TA_first_letter)]\nfirst_letter_match_movies.sample(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n240\ns241\nMovie\nBaby's Day Out\nPatrick Read Johnson\nJoe Mantegna, Lara Flynn Boyle, Joe Pantoliano...\nUnited States\nApril 23, 2021\n1994\nPG\n99 min\nAction-Adventure, Comedy, Crime\nThree bumbling kidnappers attempt to nab the b...\nDisney\n\n\n4840\ns4841\nMovie\nBad Genius\nNattawut Poonpiriya\nChutimon Chuengcharoensukying, Chanon Santinat...\nThailand\nJune 1, 2018\n2017\nTV-MA\n130 min\nDramas, International Movies, Thrillers\nA top student gets pulled into a cheating rack...\nNetflix\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\nPart e\n\nfirst_letter_match_shows = shows_df[shows_df['title'].str.startswith(TA_first_letter)]\nfirst_letter_match_shows.sample(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n9240\ns9241\nTV Show\nBucket\nNaN\nFrog Stone, Miriam Margolyes\nNaN\nNaN\n2017\nTV-14\n1 Season\nComedy, Drama\nFran is a reserved high school history teacher...\nPrime\n\n\n3151\ns3152\nTV Show\nBarbie Dreamtopia\nNaN\nErica Lindbeck, Meira Blinkoff, Lucien Dodge\nNaN\nNaN\n2019\nTV-Y\n1 Season\nAnimation, Kids\nJoin Barbie and Chelsea on magical adventures ...\nPrime\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nfirst_name_TA_uniq = len(first_letter_match_shows['title'].unique())\nprint(\"Number of unique shows that start with our TA's first name first letter are : \",first_name_TA_uniq)\n\nNumber of unique shows that start with our TA's first name first letter are :  428\n\n\nPart f\n\nfirst_letter_match_shows = shows_df[shows_df['title'].str.startswith(TA_first_letter)]\nfirst_letter_match_shows.sample(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nrating\nduration\nlisted_in\ndescription\nPlatform\n\n\n\n\n2801\ns2802\nTV Show\nBakuman\nNaN\nNaN\nJapan\nMay 1, 2016\n2010\nTV-14\n3 Seasons\nAnime, Comedy, Drama\nThe creators of Death Note return with this in...\nHulu\n\n\n1940\ns1941\nTV Show\nBlack Jesus\nNaN\nNaN\nUnited States\nDecember 24, 2019\n2014\nTV-MA\n3 Seasons\nBlack Stories, Comedy\nBlack Jesus, the live-action comedy from award...\nHulu\n\n\n\n\n\n\n      \n\n  \n    \n    \n  \n      \n\n\n\n    \n      \n\n\n    \n        \n    \n\n      \n    \n\n\n\n    \n\n      \n      \n\n      \n    \n  \n\n\n\nlast_name_TA_uniq = len(first_letter_match_shows['title'].unique())\nprint(\"Number of unique shows that start with our TA's Last name first letter are : \",last_name_TA_uniq)\n\nNumber of unique shows that start with our TA's Last name first letter are :  428\n\n\nPart g\n\nprint(\"Diff between f and g is\", last_name_TA_uniq-first_name_TA_uniq)\n\nDiff between f and g is 0"
  },
  {
    "objectID": "Homework/2/HW.html",
    "href": "Homework/2/HW.html",
    "title": "Homework 2",
    "section": "",
    "text": "(Partially) Convert a raw data source into a version appropriate for downstream analysis using Python.\n\n\n\n\n\nQuestions\nSolutions PDF\nSolutions IPYNB\nSolutions Video"
  },
  {
    "objectID": "Homework/2/HW.html#tested-learning-outcome",
    "href": "Homework/2/HW.html#tested-learning-outcome",
    "title": "Homework 2",
    "section": "",
    "text": "(Partially) Convert a raw data source into a version appropriate for downstream analysis using Python."
  },
  {
    "objectID": "Homework/2/HW.html#attachments",
    "href": "Homework/2/HW.html#attachments",
    "title": "Homework 2",
    "section": "",
    "text": "Questions\nSolutions PDF\nSolutions IPYNB\nSolutions Video"
  },
  {
    "objectID": "Homework/4/home.html",
    "href": "Homework/4/home.html",
    "title": "Homework 4",
    "section": "",
    "text": "Convert a raw data source into a version appropriate for downstream analysis using Python.\nWrite appropriate visualizations for different sources and types of data\nExplain the different uses for training, validation, and testing datasets\nSelect the appropriate evaluation measure for the dataset and task being solved\n\n\n\n\n\nQuestions\nGithub Project URL TBD\nSheets linked in D2L in compliance with FERPA."
  },
  {
    "objectID": "Homework/4/home.html#tested-learning-outcome",
    "href": "Homework/4/home.html#tested-learning-outcome",
    "title": "Homework 4",
    "section": "",
    "text": "Convert a raw data source into a version appropriate for downstream analysis using Python.\nWrite appropriate visualizations for different sources and types of data\nExplain the different uses for training, validation, and testing datasets\nSelect the appropriate evaluation measure for the dataset and task being solved"
  },
  {
    "objectID": "Homework/4/home.html#attachments",
    "href": "Homework/4/home.html#attachments",
    "title": "Homework 4",
    "section": "",
    "text": "Questions\nGithub Project URL TBD\nSheets linked in D2L in compliance with FERPA."
  },
  {
    "objectID": "Homework/4/HW.html",
    "href": "Homework/4/HW.html",
    "title": "Homework 4",
    "section": "",
    "text": "Convert a raw data source into a version appropriate for downstream analysis using Python.\nWrite appropriate visualizations for different sources and types of data\nExplain the different uses for training, validation, and testing datasets\nSelect the appropriate evaluation measure for the dataset and task being solved\n\n\n\n\n\nQuestions\nGithub Project URL\nSheets linked in D2L in compliance with FERPA."
  },
  {
    "objectID": "Homework/4/HW.html#tested-learning-outcome",
    "href": "Homework/4/HW.html#tested-learning-outcome",
    "title": "Homework 4",
    "section": "",
    "text": "Convert a raw data source into a version appropriate for downstream analysis using Python.\nWrite appropriate visualizations for different sources and types of data\nExplain the different uses for training, validation, and testing datasets\nSelect the appropriate evaluation measure for the dataset and task being solved"
  },
  {
    "objectID": "Homework/4/HW.html#attachments",
    "href": "Homework/4/HW.html#attachments",
    "title": "Homework 4",
    "section": "",
    "text": "Questions\nGithub Project URL\nSheets linked in D2L in compliance with FERPA."
  },
  {
    "objectID": "Course_Content/Week_7/1/home.html",
    "href": "Course_Content/Week_7/1/home.html",
    "title": "7.1 -Supervised Learning Continuation",
    "section": "",
    "text": "Data Preprocessing:\n\nWhy\nCommon Tasks\n\nCleaning:\n\nMissing Data :\n\nIgnore\nImputation\n\nNoisy Data :\n\nBinning\nRegression\nClustering\n\n\nIntegration\nTransformation\nReduction :\n\nFeature Selection :\n\ncorrelation analysis\nmutual information\nprincipal component analysis (PCA)\n\nFeature Extraction\nSampling\nClustering\nCompression\n\nDiscretization\nNormalization\nEncoding:\n\nOrdinal\nNominal\n\nOne hot encoding\nLabel encoding\n\n\n\n\nCurse of dimensionality\nMachine Learning Models :\n\nSupervised :\n\nRegression : Revision Linear Regression\n\nRegularisation for Feature Selection\nBest subset, and sequential subsets of features\n\nClassification : Naive Bayes\n\nUnsupervised :\n\nClustering:\n\nDefinition\nTypes\nKmeans algorithm:\n\nChoosing k:\n\nElbow\nSilhouette\n\nKmeans++\n\n\n\n\n\n\n\n\n\nSlides\nJupyter Notebook"
  },
  {
    "objectID": "Course_Content/Week_7/1/home.html#materials",
    "href": "Course_Content/Week_7/1/home.html#materials",
    "title": "7.1 -Supervised Learning Continuation",
    "section": "",
    "text": "Slides\nJupyter Notebook"
  },
  {
    "objectID": "Course_Content/Week_7/1/Notebook.html",
    "href": "Course_Content/Week_7/1/Notebook.html",
    "title": "1. Data Cleaning",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "Course_Content/Week_7/1/Notebook.html#a.-missing-data",
    "href": "Course_Content/Week_7/1/Notebook.html#a.-missing-data",
    "title": "1. Data Cleaning",
    "section": "a. Missing Data",
    "text": "a. Missing Data\n\nDataset - Detailed NFL Play-by-Play Data 2009-2018\nGuide : https://www.kaggle.com/code/rtatman/data-cleaning-challenge-handling-missing-values\n\n\nnfl_df = pd.read_csv(\"data/nfl/NFL Play by Play 2009-2018 (v5).csv\")\nnfl_df.sample()\n\n/var/folders/r0/bggl6hf15j708chpxqrx5tp00000gn/T/ipykernel_6195/401957088.py:1: DtypeWarning: Columns (42,166,167,168,169,174,175,178,179,182,183,188,189,190,191,194,195,203,204,205,218,219,220,231,232,233,238,240,241,249) have mixed types. Specify dtype option on import or set low_memory=False.\n  nfl_df = pd.read_csv(\"data/nfl/NFL Play by Play 2009-2018 (v5).csv\")\n\n\n\n\n\n\n\n\n\nplay_id\ngame_id\nhome_team\naway_team\nposteam\nposteam_type\ndefteam\nside_of_field\nyardline_100\ngame_date\n...\npenalty_player_id\npenalty_player_name\npenalty_yards\nreplay_or_challenge\nreplay_or_challenge_result\npenalty_type\ndefensive_two_point_attempt\ndefensive_two_point_conv\ndefensive_extra_point_attempt\ndefensive_extra_point_conv\n\n\n\n\n137741\n4045\n2012091612\nMIA\nOAK\nOAK\naway\nMIA\nOAK\n70.0\n2012-09-16\n...\nNaN\nNaN\nNaN\n0\nNaN\nNaN\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n1 rows × 255 columns\n\n\n\n\nnfl_df.info(verbose=True, show_counts=True)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 449371 entries, 0 to 449370\nData columns (total 255 columns):\n #    Column                                Non-Null Count   Dtype  \n---   ------                                --------------   -----  \n 0    play_id                               449371 non-null  int64  \n 1    game_id                               449371 non-null  int64  \n 2    home_team                             449371 non-null  object \n 3    away_team                             449371 non-null  object \n 4    posteam                               436129 non-null  object \n 5    posteam_type                          436492 non-null  object \n 6    defteam                               436492 non-null  object \n 7    side_of_field                         448771 non-null  object \n 8    yardline_100                          436301 non-null  float64\n 9    game_date                             449371 non-null  object \n 10   quarter_seconds_remaining             449230 non-null  float64\n 11   half_seconds_remaining                449206 non-null  float64\n 12   game_seconds_remaining                449208 non-null  float64\n 13   game_half                             449371 non-null  object \n 14   quarter_end                           449371 non-null  int64  \n 15   drive                                 449371 non-null  int64  \n 16   sp                                    449371 non-null  int64  \n 17   qtr                                   449371 non-null  int64  \n 18   down                                  381409 non-null  float64\n 19   goal_to_go                            436664 non-null  float64\n 20   time                                  449230 non-null  object \n 21   yrdln                                 448771 non-null  object \n 22   ydstogo                               449371 non-null  int64  \n 23   ydsnet                                449371 non-null  int64  \n 24   desc                                  449369 non-null  object \n 25   play_type                             436497 non-null  object \n 26   yards_gained                          449158 non-null  float64\n 27   shotgun                               449371 non-null  int64  \n 28   no_huddle                             449371 non-null  int64  \n 29   qb_dropback                           436497 non-null  float64\n 30   qb_kneel                              449371 non-null  int64  \n 31   qb_spike                              449371 non-null  int64  \n 32   qb_scramble                           449371 non-null  int64  \n 33   pass_length                           174715 non-null  object \n 34   pass_location                         174715 non-null  object \n 35   air_yards                             175719 non-null  float64\n 36   yards_after_catch                     108907 non-null  float64\n 37   run_location                          131307 non-null  object \n 38   run_gap                               94921 non-null   object \n 39   field_goal_result                     9813 non-null    object \n 40   kick_distance                         50740 non-null   float64\n 41   extra_point_result                    11150 non-null   object \n 42   two_point_conv_result                 696 non-null     object \n 43   home_timeouts_remaining               449371 non-null  int64  \n 44   away_timeouts_remaining               449371 non-null  int64  \n 45   timeout                               436497 non-null  float64\n 46   timeout_team                          18656 non-null   object \n 47   td_team                               12833 non-null   object \n 48   posteam_timeouts_remaining            436492 non-null  float64\n 49   defteam_timeouts_remaining            436492 non-null  float64\n 50   total_home_score                      449371 non-null  int64  \n 51   total_away_score                      449371 non-null  int64  \n 52   posteam_score                         433952 non-null  float64\n 53   defteam_score                         433952 non-null  float64\n 54   score_differential                    433952 non-null  float64\n 55   posteam_score_post                    436492 non-null  float64\n 56   defteam_score_post                    436492 non-null  float64\n 57   score_differential_post               436492 non-null  float64\n 58   no_score_prob                         448693 non-null  float64\n 59   opp_fg_prob                           448693 non-null  float64\n 60   opp_safety_prob                       448693 non-null  float64\n 61   opp_td_prob                           448693 non-null  float64\n 62   fg_prob                               448693 non-null  float64\n 63   safety_prob                           448693 non-null  float64\n 64   td_prob                               448693 non-null  float64\n 65   extra_point_prob                      449371 non-null  float64\n 66   two_point_conversion_prob             449371 non-null  float64\n 67   ep                                    435812 non-null  float64\n 68   epa                                   433535 non-null  float64\n 69   total_home_epa                        449371 non-null  float64\n 70   total_away_epa                        449371 non-null  float64\n 71   total_home_rush_epa                   449371 non-null  float64\n 72   total_away_rush_epa                   449371 non-null  float64\n 73   total_home_pass_epa                   449371 non-null  float64\n 74   total_away_pass_epa                   449371 non-null  float64\n 75   air_epa                               174082 non-null  float64\n 76   yac_epa                               173699 non-null  float64\n 77   comp_air_epa                          436299 non-null  float64\n 78   comp_yac_epa                          436111 non-null  float64\n 79   total_home_comp_air_epa               449371 non-null  float64\n 80   total_away_comp_air_epa               449371 non-null  float64\n 81   total_home_comp_yac_epa               449371 non-null  float64\n 82   total_away_comp_yac_epa               449371 non-null  float64\n 83   total_home_raw_air_epa                449371 non-null  float64\n 84   total_away_raw_air_epa                449371 non-null  float64\n 85   total_home_raw_yac_epa                449371 non-null  float64\n 86   total_away_raw_yac_epa                449371 non-null  float64\n 87   wp                                    433313 non-null  float64\n 88   def_wp                                433313 non-null  float64\n 89   home_wp                               435388 non-null  float64\n 90   away_wp                               435388 non-null  float64\n 91   wpa                                   444050 non-null  float64\n 92   home_wp_post                          433334 non-null  float64\n 93   away_wp_post                          433334 non-null  float64\n 94   total_home_rush_wpa                   449371 non-null  float64\n 95   total_away_rush_wpa                   449371 non-null  float64\n 96   total_home_pass_wpa                   449371 non-null  float64\n 97   total_away_pass_wpa                   449371 non-null  float64\n 98   air_wpa                               173940 non-null  float64\n 99   yac_wpa                               173724 non-null  float64\n 100  comp_air_wpa                          436156 non-null  float64\n 101  comp_yac_wpa                          436064 non-null  float64\n 102  total_home_comp_air_wpa               449371 non-null  float64\n 103  total_away_comp_air_wpa               449371 non-null  float64\n 104  total_home_comp_yac_wpa               449371 non-null  float64\n 105  total_away_comp_yac_wpa               449371 non-null  float64\n 106  total_home_raw_air_wpa                449371 non-null  float64\n 107  total_away_raw_air_wpa                449371 non-null  float64\n 108  total_home_raw_yac_wpa                449371 non-null  float64\n 109  total_away_raw_yac_wpa                449371 non-null  float64\n 110  punt_blocked                          436497 non-null  float64\n 111  first_down_rush                       436497 non-null  float64\n 112  first_down_pass                       436497 non-null  float64\n 113  first_down_penalty                    436497 non-null  float64\n 114  third_down_converted                  436497 non-null  float64\n 115  third_down_failed                     436497 non-null  float64\n 116  fourth_down_converted                 436497 non-null  float64\n 117  fourth_down_failed                    436497 non-null  float64\n 118  incomplete_pass                       436497 non-null  float64\n 119  interception                          436497 non-null  float64\n 120  punt_inside_twenty                    436497 non-null  float64\n 121  punt_in_endzone                       436497 non-null  float64\n 122  punt_out_of_bounds                    436497 non-null  float64\n 123  punt_downed                           436497 non-null  float64\n 124  punt_fair_catch                       436497 non-null  float64\n 125  kickoff_inside_twenty                 436497 non-null  float64\n 126  kickoff_in_endzone                    436497 non-null  float64\n 127  kickoff_out_of_bounds                 436497 non-null  float64\n 128  kickoff_downed                        436497 non-null  float64\n 129  kickoff_fair_catch                    436497 non-null  float64\n 130  fumble_forced                         436497 non-null  float64\n 131  fumble_not_forced                     436497 non-null  float64\n 132  fumble_out_of_bounds                  436497 non-null  float64\n 133  solo_tackle                           436497 non-null  float64\n 134  safety                                436497 non-null  float64\n 135  penalty                               436497 non-null  float64\n 136  tackled_for_loss                      436497 non-null  float64\n 137  fumble_lost                           436497 non-null  float64\n 138  own_kickoff_recovery                  436497 non-null  float64\n 139  own_kickoff_recovery_td               436497 non-null  float64\n 140  qb_hit                                436497 non-null  float64\n 141  rush_attempt                          436497 non-null  float64\n 142  pass_attempt                          436497 non-null  float64\n 143  sack                                  436497 non-null  float64\n 144  touchdown                             436497 non-null  float64\n 145  pass_touchdown                        436497 non-null  float64\n 146  rush_touchdown                        436497 non-null  float64\n 147  return_touchdown                      436497 non-null  float64\n 148  extra_point_attempt                   436497 non-null  float64\n 149  two_point_attempt                     436497 non-null  float64\n 150  field_goal_attempt                    436497 non-null  float64\n 151  kickoff_attempt                       436497 non-null  float64\n 152  punt_attempt                          436497 non-null  float64\n 153  fumble                                436497 non-null  float64\n 154  complete_pass                         436497 non-null  float64\n 155  assist_tackle                         436497 non-null  float64\n 156  lateral_reception                     436497 non-null  float64\n 157  lateral_rush                          436497 non-null  float64\n 158  lateral_return                        436497 non-null  float64\n 159  lateral_recovery                      436497 non-null  float64\n 160  passer_player_id                      188267 non-null  object \n 161  passer_player_name                    188267 non-null  object \n 162  receiver_player_id                    173716 non-null  object \n 163  receiver_player_name                  173703 non-null  object \n 164  rusher_player_id                      136448 non-null  object \n 165  rusher_player_name                    136447 non-null  object \n 166  lateral_receiver_player_id            94 non-null      object \n 167  lateral_receiver_player_name          94 non-null      object \n 168  lateral_rusher_player_id              17 non-null      object \n 169  lateral_rusher_player_name            17 non-null      object \n 170  lateral_sack_player_id                0 non-null       float64\n 171  lateral_sack_player_name              0 non-null       float64\n 172  interception_player_id                4630 non-null    object \n 173  interception_player_name              4630 non-null    object \n 174  lateral_interception_player_id        23 non-null      object \n 175  lateral_interception_player_name      23 non-null      object \n 176  punt_returner_player_id               16724 non-null   object \n 177  punt_returner_player_name             16719 non-null   object \n 178  lateral_punt_returner_player_id       23 non-null      object \n 179  lateral_punt_returner_player_name     23 non-null      object \n 180  kickoff_returner_player_name          13441 non-null   object \n 181  kickoff_returner_player_id            13442 non-null   object \n 182  lateral_kickoff_returner_player_id    38 non-null      object \n 183  lateral_kickoff_returner_player_name  38 non-null      object \n 184  punter_player_id                      23883 non-null   object \n 185  punter_player_name                    23713 non-null   object \n 186  kicker_player_name                    46702 non-null   object \n 187  kicker_player_id                      46726 non-null   object \n 188  own_kickoff_recovery_player_id        94 non-null      object \n 189  own_kickoff_recovery_player_name      94 non-null      object \n 190  blocked_player_id                     419 non-null     object \n 191  blocked_player_name                   409 non-null     object \n 192  tackle_for_loss_1_player_id           23623 non-null   object \n 193  tackle_for_loss_1_player_name         23623 non-null   object \n 194  tackle_for_loss_2_player_id           8 non-null       object \n 195  tackle_for_loss_2_player_name         8 non-null       object \n 196  qb_hit_1_player_id                    24213 non-null   object \n 197  qb_hit_1_player_name                  24208 non-null   object \n 198  qb_hit_2_player_id                    976 non-null     object \n 199  qb_hit_2_player_name                  976 non-null     object \n 200  forced_fumble_player_1_team           4403 non-null    object \n 201  forced_fumble_player_1_player_id      4403 non-null    object \n 202  forced_fumble_player_1_player_name    4403 non-null    object \n 203  forced_fumble_player_2_team           33 non-null      object \n 204  forced_fumble_player_2_player_id      33 non-null      object \n 205  forced_fumble_player_2_player_name    32 non-null      object \n 206  solo_tackle_1_team                    209699 non-null  object \n 207  solo_tackle_2_team                    2834 non-null    object \n 208  solo_tackle_1_player_id               209699 non-null  object \n 209  solo_tackle_2_player_id               2834 non-null    object \n 210  solo_tackle_1_player_name             209696 non-null  object \n 211  solo_tackle_2_player_name             2833 non-null    object \n 212  assist_tackle_1_player_id             54419 non-null   object \n 213  assist_tackle_1_player_name           54416 non-null   object \n 214  assist_tackle_1_team                  54419 non-null   object \n 215  assist_tackle_2_player_id             54416 non-null   object \n 216  assist_tackle_2_player_name           54416 non-null   object \n 217  assist_tackle_2_team                  54416 non-null   object \n 218  assist_tackle_3_player_id             3 non-null       object \n 219  assist_tackle_3_player_name           3 non-null       object \n 220  assist_tackle_3_team                  3 non-null       object \n 221  assist_tackle_4_player_id             0 non-null       float64\n 222  assist_tackle_4_player_name           0 non-null       float64\n 223  assist_tackle_4_team                  0 non-null       float64\n 224  pass_defense_1_player_id              22738 non-null   object \n 225  pass_defense_1_player_name            22738 non-null   object \n 226  pass_defense_2_player_id              729 non-null     object \n 227  pass_defense_2_player_name            729 non-null     object \n 228  fumbled_1_team                        6555 non-null    object \n 229  fumbled_1_player_id                   6555 non-null    object \n 230  fumbled_1_player_name                 6554 non-null    object \n 231  fumbled_2_player_id                   38 non-null      object \n 232  fumbled_2_player_name                 38 non-null      object \n 233  fumbled_2_team                        38 non-null      object \n 234  fumble_recovery_1_team                6032 non-null    object \n 235  fumble_recovery_1_yards               6026 non-null    float64\n 236  fumble_recovery_1_player_id           6032 non-null    object \n 237  fumble_recovery_1_player_name         6027 non-null    object \n 238  fumble_recovery_2_team                47 non-null      object \n 239  fumble_recovery_2_yards               47 non-null      float64\n 240  fumble_recovery_2_player_id           47 non-null      object \n 241  fumble_recovery_2_player_name         46 non-null      object \n 242  return_team                           30605 non-null   object \n 243  return_yards                          449350 non-null  float64\n 244  penalty_team                          32636 non-null   object \n 245  penalty_player_id                     32636 non-null   object \n 246  penalty_player_name                   31207 non-null   object \n 247  penalty_yards                         32618 non-null   float64\n 248  replay_or_challenge                   449371 non-null  int64  \n 249  replay_or_challenge_result            812 non-null     object \n 250  penalty_type                          31139 non-null   object \n 251  defensive_two_point_attempt           436497 non-null  float64\n 252  defensive_two_point_conv              436497 non-null  float64\n 253  defensive_extra_point_attempt         436497 non-null  float64\n 254  defensive_extra_point_conv            436497 non-null  float64\ndtypes: float64(135), int64(18), object(102)\nmemory usage: 874.2+ MB\n\n\n\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(nfl_df.isnull().sum().to_frame())\n\n                                           0\nplay_id                                    0\ngame_id                                    0\nhome_team                                  0\naway_team                                  0\nposteam                                13242\nposteam_type                           12879\ndefteam                                12879\nside_of_field                            600\nyardline_100                           13070\ngame_date                                  0\nquarter_seconds_remaining                141\nhalf_seconds_remaining                   165\ngame_seconds_remaining                   163\ngame_half                                  0\nquarter_end                                0\ndrive                                      0\nsp                                         0\nqtr                                        0\ndown                                   67962\ngoal_to_go                             12707\ntime                                     141\nyrdln                                    600\nydstogo                                    0\nydsnet                                     0\ndesc                                       2\nplay_type                              12874\nyards_gained                             213\nshotgun                                    0\nno_huddle                                  0\nqb_dropback                            12874\nqb_kneel                                   0\nqb_spike                                   0\nqb_scramble                                0\npass_length                           274656\npass_location                         274656\nair_yards                             273652\nyards_after_catch                     340464\nrun_location                          318064\nrun_gap                               354450\nfield_goal_result                     439558\nkick_distance                         398631\nextra_point_result                    438221\ntwo_point_conv_result                 448675\nhome_timeouts_remaining                    0\naway_timeouts_remaining                    0\ntimeout                                12874\ntimeout_team                          430715\ntd_team                               436538\nposteam_timeouts_remaining             12879\ndefteam_timeouts_remaining             12879\ntotal_home_score                           0\ntotal_away_score                           0\nposteam_score                          15419\ndefteam_score                          15419\nscore_differential                     15419\nposteam_score_post                     12879\ndefteam_score_post                     12879\nscore_differential_post                12879\nno_score_prob                            678\nopp_fg_prob                              678\nopp_safety_prob                          678\nopp_td_prob                              678\nfg_prob                                  678\nsafety_prob                              678\ntd_prob                                  678\nextra_point_prob                           0\ntwo_point_conversion_prob                  0\nep                                     13559\nepa                                    15836\ntotal_home_epa                             0\ntotal_away_epa                             0\ntotal_home_rush_epa                        0\ntotal_away_rush_epa                        0\ntotal_home_pass_epa                        0\ntotal_away_pass_epa                        0\nair_epa                               275289\nyac_epa                               275672\ncomp_air_epa                           13072\ncomp_yac_epa                           13260\ntotal_home_comp_air_epa                    0\ntotal_away_comp_air_epa                    0\ntotal_home_comp_yac_epa                    0\ntotal_away_comp_yac_epa                    0\ntotal_home_raw_air_epa                     0\ntotal_away_raw_air_epa                     0\ntotal_home_raw_yac_epa                     0\ntotal_away_raw_yac_epa                     0\nwp                                     16058\ndef_wp                                 16058\nhome_wp                                13983\naway_wp                                13983\nwpa                                     5321\nhome_wp_post                           16037\naway_wp_post                           16037\ntotal_home_rush_wpa                        0\ntotal_away_rush_wpa                        0\ntotal_home_pass_wpa                        0\ntotal_away_pass_wpa                        0\nair_wpa                               275431\nyac_wpa                               275647\ncomp_air_wpa                           13215\ncomp_yac_wpa                           13307\ntotal_home_comp_air_wpa                    0\ntotal_away_comp_air_wpa                    0\ntotal_home_comp_yac_wpa                    0\ntotal_away_comp_yac_wpa                    0\ntotal_home_raw_air_wpa                     0\ntotal_away_raw_air_wpa                     0\ntotal_home_raw_yac_wpa                     0\ntotal_away_raw_yac_wpa                     0\npunt_blocked                           12874\nfirst_down_rush                        12874\nfirst_down_pass                        12874\nfirst_down_penalty                     12874\nthird_down_converted                   12874\nthird_down_failed                      12874\nfourth_down_converted                  12874\nfourth_down_failed                     12874\nincomplete_pass                        12874\ninterception                           12874\npunt_inside_twenty                     12874\npunt_in_endzone                        12874\npunt_out_of_bounds                     12874\npunt_downed                            12874\npunt_fair_catch                        12874\nkickoff_inside_twenty                  12874\nkickoff_in_endzone                     12874\nkickoff_out_of_bounds                  12874\nkickoff_downed                         12874\nkickoff_fair_catch                     12874\nfumble_forced                          12874\nfumble_not_forced                      12874\nfumble_out_of_bounds                   12874\nsolo_tackle                            12874\nsafety                                 12874\npenalty                                12874\ntackled_for_loss                       12874\nfumble_lost                            12874\nown_kickoff_recovery                   12874\nown_kickoff_recovery_td                12874\nqb_hit                                 12874\nrush_attempt                           12874\npass_attempt                           12874\nsack                                   12874\ntouchdown                              12874\npass_touchdown                         12874\nrush_touchdown                         12874\nreturn_touchdown                       12874\nextra_point_attempt                    12874\ntwo_point_attempt                      12874\nfield_goal_attempt                     12874\nkickoff_attempt                        12874\npunt_attempt                           12874\nfumble                                 12874\ncomplete_pass                          12874\nassist_tackle                          12874\nlateral_reception                      12874\nlateral_rush                           12874\nlateral_return                         12874\nlateral_recovery                       12874\npasser_player_id                      261104\npasser_player_name                    261104\nreceiver_player_id                    275655\nreceiver_player_name                  275668\nrusher_player_id                      312923\nrusher_player_name                    312924\nlateral_receiver_player_id            449277\nlateral_receiver_player_name          449277\nlateral_rusher_player_id              449354\nlateral_rusher_player_name            449354\nlateral_sack_player_id                449371\nlateral_sack_player_name              449371\ninterception_player_id                444741\ninterception_player_name              444741\nlateral_interception_player_id        449348\nlateral_interception_player_name      449348\npunt_returner_player_id               432647\npunt_returner_player_name             432652\nlateral_punt_returner_player_id       449348\nlateral_punt_returner_player_name     449348\nkickoff_returner_player_name          435930\nkickoff_returner_player_id            435929\nlateral_kickoff_returner_player_id    449333\nlateral_kickoff_returner_player_name  449333\npunter_player_id                      425488\npunter_player_name                    425658\nkicker_player_name                    402669\nkicker_player_id                      402645\nown_kickoff_recovery_player_id        449277\nown_kickoff_recovery_player_name      449277\nblocked_player_id                     448952\nblocked_player_name                   448962\ntackle_for_loss_1_player_id           425748\ntackle_for_loss_1_player_name         425748\ntackle_for_loss_2_player_id           449363\ntackle_for_loss_2_player_name         449363\nqb_hit_1_player_id                    425158\nqb_hit_1_player_name                  425163\nqb_hit_2_player_id                    448395\nqb_hit_2_player_name                  448395\nforced_fumble_player_1_team           444968\nforced_fumble_player_1_player_id      444968\nforced_fumble_player_1_player_name    444968\nforced_fumble_player_2_team           449338\nforced_fumble_player_2_player_id      449338\nforced_fumble_player_2_player_name    449339\nsolo_tackle_1_team                    239672\nsolo_tackle_2_team                    446537\nsolo_tackle_1_player_id               239672\nsolo_tackle_2_player_id               446537\nsolo_tackle_1_player_name             239675\nsolo_tackle_2_player_name             446538\nassist_tackle_1_player_id             394952\nassist_tackle_1_player_name           394955\nassist_tackle_1_team                  394952\nassist_tackle_2_player_id             394955\nassist_tackle_2_player_name           394955\nassist_tackle_2_team                  394955\nassist_tackle_3_player_id             449368\nassist_tackle_3_player_name           449368\nassist_tackle_3_team                  449368\nassist_tackle_4_player_id             449371\nassist_tackle_4_player_name           449371\nassist_tackle_4_team                  449371\npass_defense_1_player_id              426633\npass_defense_1_player_name            426633\npass_defense_2_player_id              448642\npass_defense_2_player_name            448642\nfumbled_1_team                        442816\nfumbled_1_player_id                   442816\nfumbled_1_player_name                 442817\nfumbled_2_player_id                   449333\nfumbled_2_player_name                 449333\nfumbled_2_team                        449333\nfumble_recovery_1_team                443339\nfumble_recovery_1_yards               443345\nfumble_recovery_1_player_id           443339\nfumble_recovery_1_player_name         443344\nfumble_recovery_2_team                449324\nfumble_recovery_2_yards               449324\nfumble_recovery_2_player_id           449324\nfumble_recovery_2_player_name         449325\nreturn_team                           418766\nreturn_yards                              21\npenalty_team                          416735\npenalty_player_id                     416735\npenalty_player_name                   418164\npenalty_yards                         416753\nreplay_or_challenge                        0\nreplay_or_challenge_result            448559\npenalty_type                          418232\ndefensive_two_point_attempt            12874\ndefensive_two_point_conv               12874\ndefensive_extra_point_attempt          12874\ndefensive_extra_point_conv             12874\n\n\n\nIs this value missing becuase it wasn’t recorded or because it doesn’t exist?\nIs your data MAR (Missing At Random)?\n\n\nIgnoring the data point\n\nnfl_df.dropna()\n\n\n\n\n\n\n\n\nplay_id\ngame_id\nhome_team\naway_team\nposteam\nposteam_type\ndefteam\nside_of_field\nyardline_100\ngame_date\n...\npenalty_player_id\npenalty_player_name\npenalty_yards\nreplay_or_challenge\nreplay_or_challenge_result\npenalty_type\ndefensive_two_point_attempt\ndefensive_two_point_conv\ndefensive_extra_point_attempt\ndefensive_extra_point_conv\n\n\n\n\n\n\n0 rows × 255 columns\n\n\n\n\n\nFilling in missing values automatically\nConstant,back and forward fill\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html\n\nnfl_df.fillna(0)\n\n\n\nMean, Median, Most Frequent, Constant\n\nSimple Imputer - https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\nIterativeImputer : Imputer that estimates values to impute for each feature with missing values from all the others.\nKNNImputer : Imputer that estimates missing features using nearest samples.\n\n\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\n\n\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])\nX = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]\nprint(imp_mean.transform(X))\n\n[[ 7.   2.   3. ]\n [ 4.   3.5  6. ]\n [10.   3.5  9. ]]"
  },
  {
    "objectID": "Course_Content/Week_7/1/home.html#objectives",
    "href": "Course_Content/Week_7/1/home.html#objectives",
    "title": "7.1 -Supervised Learning Continuation",
    "section": "",
    "text": "Data Preprocessing:\n\nWhy\nCommon Tasks\n\nCleaning:\n\nMissing Data :\n\nIgnore\nImputation\n\nNoisy Data :\n\nBinning\nRegression\nClustering\n\n\nIntegration\nTransformation\nReduction :\n\nFeature Selection :\n\ncorrelation analysis\nmutual information\nprincipal component analysis (PCA)\n\nFeature Extraction\nSampling\nClustering\nCompression\n\nDiscretization\nNormalization\nEncoding:\n\nOrdinal\nNominal\n\nOne hot encoding\nLabel encoding\n\n\n\n\nCurse of dimensionality\nMachine Learning Models :\n\nSupervised :\n\nRegression : Revision Linear Regression\n\nRegularisation for Feature Selection\nBest subset, and sequential subsets of features\n\nClassification : Naive Bayes\n\nUnsupervised :\n\nClustering:\n\nDefinition\nTypes\nKmeans algorithm:\n\nChoosing k:\n\nElbow\nSilhouette\n\nKmeans++"
  },
  {
    "objectID": "Course_Content/Week_7/3/home.html",
    "href": "Course_Content/Week_7/3/home.html",
    "title": "7.3 Non-Linear Models and Neural Networks",
    "section": "",
    "text": "Terminology:\n\nLinear v/s Non-Linear\nParametric v/s Non-paramteric\nProbabilistic v/s Non-probabilistic\nGenerative v/s Discriminative\n\nEvaluating models\n\nAccuracy\nConfusion matrix\nPrecision\nRecall\nF1 score\nPrecision-Recall or PR curve\nROC (Receiver Operating Characteristics) curve\n\nDecision Tree Classifier\nKnn ( not to be confused with Kmeans from last lecture)\nLogistic regression\n\n\n\n\n\nSlides\nK Nearest Neighbours\nLogistic Regression"
  },
  {
    "objectID": "Course_Content/Week_7/3/home.html#objectives",
    "href": "Course_Content/Week_7/3/home.html#objectives",
    "title": "7.3 Non-Linear Models and Neural Networks",
    "section": "",
    "text": "Terminology:\n\nLinear v/s Non-Linear\nParametric v/s Non-paramteric\nProbabilistic v/s Non-probabilistic\nGenerative v/s Discriminative\n\nEvaluating models\n\nAccuracy\nConfusion matrix\nPrecision\nRecall\nF1 score\nPrecision-Recall or PR curve\nROC (Receiver Operating Characteristics) curve\n\nDecision Tree Classifier\nKnn ( not to be confused with Kmeans from last lecture)\nLogistic regression"
  },
  {
    "objectID": "Course_Content/Week_7/3/home.html#materials",
    "href": "Course_Content/Week_7/3/home.html#materials",
    "title": "7.3 Non-Linear Models and Neural Networks",
    "section": "",
    "text": "Slides\nK Nearest Neighbours\nLogistic Regression"
  },
  {
    "objectID": "Course_Content/Week_7/2/home.html",
    "href": "Course_Content/Week_7/2/home.html",
    "title": "7.2 Evaluation and ML Algorithms Continutaion",
    "section": "",
    "text": "Terminology:\n\nLinear v/s Non-Linear\nParametric v/s Non-paramteric\nProbabilistic v/s Non-probabilistic\nGenerative v/s Discriminative\n\nEvaluating models\n\nAccuracy\nConfusion matrix\nPrecision\nRecall\nF1 score\nPrecision-Recall or PR curve\nROC (Receiver Operating Characteristics) curve\n\nDecision Tree Classifier\nKnn ( not to be confused with Kmeans from last lecture)\nLogistic regression\n\n\n\n\n\nSlides\nK Nearest Neighbours\nLogistic Regression"
  },
  {
    "objectID": "Course_Content/Week_7/2/home.html#objectives",
    "href": "Course_Content/Week_7/2/home.html#objectives",
    "title": "7.2 Evaluation and ML Algorithms Continutaion",
    "section": "",
    "text": "Terminology:\n\nLinear v/s Non-Linear\nParametric v/s Non-paramteric\nProbabilistic v/s Non-probabilistic\nGenerative v/s Discriminative\n\nEvaluating models\n\nAccuracy\nConfusion matrix\nPrecision\nRecall\nF1 score\nPrecision-Recall or PR curve\nROC (Receiver Operating Characteristics) curve\n\nDecision Tree Classifier\nKnn ( not to be confused with Kmeans from last lecture)\nLogistic regression"
  },
  {
    "objectID": "Course_Content/Week_7/2/home.html#materials",
    "href": "Course_Content/Week_7/2/home.html#materials",
    "title": "7.2 Evaluation and ML Algorithms Continutaion",
    "section": "",
    "text": "Slides\nK Nearest Neighbours\nLogistic Regression"
  },
  {
    "objectID": "Finals/home.html",
    "href": "Finals/home.html",
    "title": "Final Project",
    "section": "",
    "text": "Questions\n\n\n\nTests course objectives and expected learning outcomes.\n\n\n\nUnderstand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nApply data analysis and visualization techniques to derive insights from diverse datasets on Week 6\nGain familiarity with machine learning algorithms and their practical applications.\nDevelop proficiency in using data science tools and programming languages.\nEngage in critical thinking and problem-solving through project-based assignments.\nExplore the ethical considerations associated with data-driven decision-making.\nStay informed about current trends and developments in data science and artificial intelligence.\n\n\n\n\nA student who successfully completes this course will be able to:\n\nExplain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.)\nConvert a raw data source into a version appropriate for downstream analysis using Python. | Week 4 & 5\nWrite appropriate visualizations for different sources and types of data | Week 6\nExplain why we seek to build machine learning models that generalize rather than memorize their input. | Week 6\nExplain the different uses for training, validation, and testing datasets | Week 6\nSelect the appropriate evaluation measure for the dataset and task being solved\nArticulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem | Week 6\nDemonstrate awareness of bias and ethics in data science."
  },
  {
    "objectID": "Finals/home.html#objective",
    "href": "Finals/home.html#objective",
    "title": "Final Project",
    "section": "",
    "text": "Tests course objectives and expected learning outcomes.\n\n\n\nUnderstand the fundamental concepts and principles of data science, including data collection, preprocessing, analysis, and interpretation.\nApply data analysis and visualization techniques to derive insights from diverse datasets on Week 6\nGain familiarity with machine learning algorithms and their practical applications.\nDevelop proficiency in using data science tools and programming languages.\nEngage in critical thinking and problem-solving through project-based assignments.\nExplore the ethical considerations associated with data-driven decision-making.\nStay informed about current trends and developments in data science and artificial intelligence.\n\n\n\n\nA student who successfully completes this course will be able to:\n\nExplain the difference between different measures of centrality and variability (means vs. medians, variance vs. interquartile range, etc.)\nConvert a raw data source into a version appropriate for downstream analysis using Python. | Week 4 & 5\nWrite appropriate visualizations for different sources and types of data | Week 6\nExplain why we seek to build machine learning models that generalize rather than memorize their input. | Week 6\nExplain the different uses for training, validation, and testing datasets | Week 6\nSelect the appropriate evaluation measure for the dataset and task being solved\nArticulate the difference between supervised and unsupervised machine learning, as well as select the appropriate methodology for a given problem | Week 6\nDemonstrate awareness of bias and ethics in data science."
  },
  {
    "objectID": "Homework/3/Homework 3 | Solutions.html",
    "href": "Homework/3/Homework 3 | Solutions.html",
    "title": "1. Data Processing",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('../dataset.csv')\ndf.head()\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\n\n\n\n\n0\n01-01-2016 21:11\n01-01-2016 21:17\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n\n\n1\n01-02-2016 01:25\n01-02-2016 01:37\nBusiness\nFort Pierce\nFort Pierce\n5.0\nNaN\n\n\n2\n01-02-2016 20:25\n01-02-2016 20:38\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n\n\n3\n01-05-2016 17:31\n01-05-2016 17:45\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n\n\n4\n01-06-2016 14:42\n01-06-2016 15:49\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1156 entries, 0 to 1155\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   START_DATE  1156 non-null   object \n 1   END_DATE    1155 non-null   object \n 2   CATEGORY    1155 non-null   object \n 3   START       1155 non-null   object \n 4   STOP        1155 non-null   object \n 5   MILES       1156 non-null   float64\n 6   PURPOSE     653 non-null    object \ndtypes: float64(1), object(6)\nmemory usage: 63.3+ KB\ndf.describe()\n\n\n\n\n\n\n\n\nMILES\n\n\n\n\ncount\n1156.000000\n\n\nmean\n21.115398\n\n\nstd\n359.299007\n\n\nmin\n0.500000\n\n\n25%\n2.900000\n\n\n50%\n6.000000\n\n\n75%\n10.400000\n\n\nmax\n12204.700000\nSTART_DATE [ 2 points ]\ndf['START_DATE'].isna().any()\n\nFalse\ndf['start time'] = df['START_DATE'].apply(lambda x : x.split(\" \")[-1])\ndf['START_DATE'] = pd.to_datetime(df['START_DATE'], errors= \"ignore\" )\ndf.sample()\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\n\n\n\n\n1043\n12-12-2016 13:22\n12-12-2016 13:32\nBusiness\nCary\nCary\n3.1\nErrand/Supplies\n13:22\nEND_DATE [ 2 points ]\ndf['END_DATE'].isna().any()\n\nTrue\ndf['END_DATE'].isna().sum()\n\n1\ndf[df['END_DATE'].isna()]\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\n\n\n\n\n1155\nTotals\nNaN\nNaN\nNaN\nNaN\n12204.7\nNaN\nTotals\ndf.drop(index=1155,inplace = True)\ndf['END_DATE'].isna().sum()\n\n0\ndf['end time'] = df['END_DATE'].apply(lambda x : x.split(\" \")[-1])\ndf.sample()\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n121\n2/18/2016 8:19\n2/18/2016 8:27\nBusiness\nUnknown Location\nUnknown Location\n23.5\nTemporary Site\n8:19\n8:27\ndf['END_DATE'] = pd.to_datetime(df['END_DATE'])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1155 entries, 0 to 1154\nData columns (total 9 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   START_DATE  1155 non-null   object        \n 1   END_DATE    1155 non-null   datetime64[ns]\n 2   CATEGORY    1155 non-null   object        \n 3   START       1155 non-null   object        \n 4   STOP        1155 non-null   object        \n 5   MILES       1155 non-null   float64       \n 6   PURPOSE     653 non-null    object        \n 7   start time  1155 non-null   object        \n 8   end time    1155 non-null   object        \ndtypes: datetime64[ns](1), float64(1), object(7)\nmemory usage: 81.3+ KB\nCategory\ndf['CATEGORY'].isna().sum()\n\n0\ndf['CATEGORY'].unique()\n\narray(['Business', 'Personal'], dtype=object)\ndf['CATEGORY'].value_counts()\n\nBusiness    1078\nPersonal      77\nName: CATEGORY, dtype: int64\nSTART\ndf['START'].isna().sum()\n\n0\ndf['START'].unique()\n\narray(['Fort Pierce', 'West Palm Beach', 'Cary', 'Jamaica', 'New York',\n       'Elmhurst', 'Midtown', 'East Harlem', 'Flatiron District',\n       'Midtown East', 'Hudson Square', 'Lower Manhattan',\n       \"Hell's Kitchen\", 'Downtown', 'Gulfton', 'Houston', 'Eagan Park',\n       'Morrisville', 'Durham', 'Farmington Woods', 'Whitebridge',\n       'Lake Wellingborough', 'Fayetteville Street', 'Raleigh',\n       'Hazelwood', 'Fairmont', 'Meredith Townes', 'Apex', 'Chapel Hill',\n       'Northwoods', 'Edgehill Farms', 'Tanglewood', 'Preston',\n       'Eastgate', 'East Elmhurst', 'Jackson Heights', 'Long Island City',\n       'Katunayaka', 'Unknown Location', 'Colombo', 'Nugegoda',\n       'Islamabad', 'R?walpindi', 'Noorpur Shahan', 'Heritage Pines',\n       'Westpark Place', 'Waverly Place', 'Wayne Ridge', 'Weston',\n       'East Austin', 'West University', 'South Congress', 'The Drag',\n       'Congress Ave District', 'Red River District', 'Georgian Acres',\n       'North Austin', 'Coxville', 'Convention Center District', 'Austin',\n       'Katy', 'Sharpstown', 'Sugar Land', 'Galveston', 'Port Bolivar',\n       'Washington Avenue', 'Briar Meadow', 'Latta', 'Jacksonville',\n       'Couples Glen', 'Kissimmee', 'Lake Reams', 'Orlando',\n       'Sand Lake Commons', 'Sky Lake', 'Daytona Beach', 'Ridgeland',\n       'Florence', 'Meredith', 'Holly Springs', 'Chessington', 'Burtrose',\n       'Parkway', 'Mcvan', 'Capitol One', 'University District',\n       'Seattle', 'Redmond', 'Bellevue', 'San Francisco', 'Palo Alto',\n       'Sunnyvale', 'Newark', 'Menlo Park', 'Old City', 'Savon Height',\n       'Kilarney Woods', 'Townes at Everett Crossing', 'Huntington Woods',\n       'Seaport', 'Medical Centre', 'Rose Hill', 'Soho', 'Tribeca',\n       'Financial District', 'Oakland', 'Emeryville', 'Berkeley',\n       'Kenner', 'CBD', 'Lower Garden District', 'Lakeview', 'Storyville',\n       'New Orleans', 'Metairie', 'Chalmette', 'Arabi',\n       'Pontchartrain Shores', 'Marigny', 'Covington', 'Mandeville',\n       'Jamestown Court', 'Summerwinds', 'Parkwood',\n       'Pontchartrain Beach', 'St Thomas', 'Banner Elk', 'Elk Park',\n       'Newland', 'Boone', 'Stonewater', 'Lexington Park at Amberly',\n       'Arlington Park at Amberly', 'Arlington', 'Kalorama Triangle',\n       'K Street', 'West End', 'Connecticut Avenue', 'Columbia Heights',\n       'Washington', 'Wake Forest', 'Lahore', 'Karachi', 'SOMISSPO',\n       'West Berkeley', 'North Berkeley Hills', 'San Jose', 'Eagle Rock',\n       'Winston Salem', 'Asheville', 'Topton', 'Hayesville',\n       'Bryson City', 'Almond', 'Mebane', 'Agnew', 'Cory', 'Renaissance',\n       'Santa Clara', 'NOMA', 'Sunnyside', 'Ingleside', 'Central',\n       'Tenderloin', 'College Avenue', 'South', 'Southside',\n       'South Berkeley', 'Mountain View', 'El Cerrito', 'Krendle Woods',\n       'Wake Co.', 'Fuquay-Varina', 'Rawalpindi', 'Kar?chi', 'Katunayake',\n       'Gampaha'], dtype=object)\ndf['START'].value_counts()\n\nCary                201\nUnknown Location    148\nMorrisville          85\nWhitebridge          68\nIslamabad            57\n                   ... \nFlorence              1\nRidgeland             1\nDaytona Beach         1\nSky Lake              1\nGampaha               1\nName: START, Length: 177, dtype: int64\ndf[df['START']=='Unknown Location']\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n109\n2/16/2016 8:29\n2016-02-16 09:34:00\nBusiness\nUnknown Location\nColombo\n14.1\nNaN\n8:29\n9:34\n\n\n117\n2/17/2016 13:18\n2016-02-17 14:04:00\nBusiness\nUnknown Location\nColombo\n14.7\nTemporary Site\n13:18\n14:04\n\n\n121\n2/18/2016 8:19\n2016-02-18 08:27:00\nBusiness\nUnknown Location\nUnknown Location\n23.5\nTemporary Site\n8:19\n8:27\n\n\n122\n2/18/2016 14:03\n2016-02-18 14:45:00\nBusiness\nUnknown Location\nIslamabad\n12.7\nTemporary Site\n14:03\n14:45\n\n\n124\n2/18/2016 18:44\n2016-02-18 18:58:00\nBusiness\nUnknown Location\nIslamabad\n5.2\nCustomer Visit\n18:44\n18:58\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1129\n12/28/2016 17:02\n2016-12-28 17:16:00\nBusiness\nUnknown Location\nKar?chi\n4.4\nErrand/Supplies\n17:02\n17:16\n\n\n1134\n12/29/2016 11:28\n2016-12-29 12:00:00\nBusiness\nUnknown Location\nKar?chi\n11.9\nMeal/Entertain\n11:28\n12:00\n\n\n1141\n12/29/2016 19:50\n2016-12-29 20:10:00\nBusiness\nUnknown Location\nKar?chi\n4.1\nCustomer Visit\n19:50\n20:10\n\n\n1144\n12/29/2016 23:14\n2016-12-29 23:47:00\nBusiness\nUnknown Location\nKar?chi\n12.9\nMeeting\n23:14\n23:47\n\n\n1152\n12/31/2016 15:03\n2016-12-31 15:38:00\nBusiness\nUnknown Location\nUnknown Location\n16.2\nMeeting\n15:03\n15:38\n\n\n\n\n148 rows × 9 columns\ndf = df[df['START']!='Unknown Location']\ndf.sample()\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n982\n11/20/2016 17:45\n2016-11-20 18:37:00\nBusiness\nCary\nCary\n18.5\nErrand/Supplies\n17:45\n18:37\nSTOP\ndf[df['STOP']=='Unknown Location']\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n108\n2/16/2016 3:21\n2016-02-16 04:13:00\nBusiness\nKatunayaka\nUnknown Location\n43.7\nCustomer Visit\n3:21\n4:13\n\n\n116\n2/16/2016 17:40\n2016-02-16 17:44:00\nBusiness\nNugegoda\nUnknown Location\n3.6\nErrand/Supplies\n17:40\n17:44\n\n\n123\n2/18/2016 15:16\n2016-02-18 15:31:00\nBusiness\nIslamabad\nUnknown Location\n6.0\nTemporary Site\n15:16\n15:31\n\n\n125\n2/18/2016 19:27\n2016-02-18 20:08:00\nBusiness\nIslamabad\nUnknown Location\n10.0\nMeeting\n19:27\n20:08\n\n\n131\n2/19/2016 12:09\n2016-02-19 12:27:00\nBusiness\nIslamabad\nUnknown Location\n7.3\nTemporary Site\n12:09\n12:27\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1128\n12/28/2016 15:04\n2016-12-28 15:39:00\nBusiness\nKar?chi\nUnknown Location\n8.5\nMeal/Entertain\n15:04\n15:39\n\n\n1133\n12/29/2016 9:44\n2016-12-29 10:07:00\nBusiness\nKar?chi\nUnknown Location\n11.6\nMeal/Entertain\n9:44\n10:07\n\n\n1140\n12/29/2016 18:59\n2016-12-29 19:14:00\nBusiness\nKar?chi\nUnknown Location\n3.0\nMeal/Entertain\n18:59\n19:14\n\n\n1143\n12/29/2016 20:53\n2016-12-29 21:42:00\nBusiness\nKar?chi\nUnknown Location\n6.4\nNaN\n20:53\n21:42\n\n\n1151\n12/31/2016 13:24\n2016-12-31 13:42:00\nBusiness\nKar?chi\nUnknown Location\n3.9\nTemporary Site\n13:24\n13:42\n\n\n\n\n63 rows × 9 columns\ndf = df[df['STOP']!='Unknown Location']\ndf\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n0\n01-01-2016 21:11\n2016-01-01 21:17:00\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n21:11\n21:17\n\n\n1\n01-02-2016 01:25\n2016-01-02 01:37:00\nBusiness\nFort Pierce\nFort Pierce\n5.0\nNaN\n01:25\n01:37\n\n\n2\n01-02-2016 20:25\n2016-01-02 20:38:00\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n20:25\n20:38\n\n\n3\n01-05-2016 17:31\n2016-01-05 17:45:00\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n17:31\n17:45\n\n\n4\n01-06-2016 14:42\n2016-01-06 15:49:00\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n14:42\n15:49\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1148\n12/30/2016 16:45\n2016-12-30 17:08:00\nBusiness\nKar?chi\nKar?chi\n4.6\nMeeting\n16:45\n17:08\n\n\n1149\n12/30/2016 23:06\n2016-12-30 23:10:00\nBusiness\nKar?chi\nKar?chi\n0.8\nCustomer Visit\n23:06\n23:10\n\n\n1150\n12/31/2016 1:07\n2016-12-31 01:14:00\nBusiness\nKar?chi\nKar?chi\n0.7\nMeeting\n1:07\n1:14\n\n\n1153\n12/31/2016 21:32\n2016-12-31 21:50:00\nBusiness\nKatunayake\nGampaha\n6.4\nTemporary Site\n21:32\n21:50\n\n\n1154\n12/31/2016 22:08\n2016-12-31 23:51:00\nBusiness\nGampaha\nIlukwatta\n48.2\nTemporary Site\n22:08\n23:51\n\n\n\n\n944 rows × 9 columns\ndf.shape\n\n(944, 9)\ndf\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n0\n01-01-2016 21:11\n2016-01-01 21:17:00\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n21:11\n21:17\n\n\n1\n01-02-2016 01:25\n2016-01-02 01:37:00\nBusiness\nFort Pierce\nFort Pierce\n5.0\nNaN\n01:25\n01:37\n\n\n2\n01-02-2016 20:25\n2016-01-02 20:38:00\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n20:25\n20:38\n\n\n3\n01-05-2016 17:31\n2016-01-05 17:45:00\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n17:31\n17:45\n\n\n4\n01-06-2016 14:42\n2016-01-06 15:49:00\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n14:42\n15:49\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1148\n12/30/2016 16:45\n2016-12-30 17:08:00\nBusiness\nKar?chi\nKar?chi\n4.6\nMeeting\n16:45\n17:08\n\n\n1149\n12/30/2016 23:06\n2016-12-30 23:10:00\nBusiness\nKar?chi\nKar?chi\n0.8\nCustomer Visit\n23:06\n23:10\n\n\n1150\n12/31/2016 1:07\n2016-12-31 01:14:00\nBusiness\nKar?chi\nKar?chi\n0.7\nMeeting\n1:07\n1:14\n\n\n1153\n12/31/2016 21:32\n2016-12-31 21:50:00\nBusiness\nKatunayake\nGampaha\n6.4\nTemporary Site\n21:32\n21:50\n\n\n1154\n12/31/2016 22:08\n2016-12-31 23:51:00\nBusiness\nGampaha\nIlukwatta\n48.2\nTemporary Site\n22:08\n23:51\n\n\n\n\n944 rows × 9 columns\nMILES\ndf['MILES'].isna().sum()\n\n0\nPURPOSE\ndf['PURPOSE'].isna().sum()\n\n372\ndf['PURPOSE'].unique()\n\narray(['Meal/Entertain', nan, 'Errand/Supplies', 'Meeting',\n       'Customer Visit', 'Temporary Site', 'Between Offices',\n       'Charity ($)', 'Commute', 'Moving', 'Airport/Travel'], dtype=object)\ndf['PURPOSE'] = df['PURPOSE'].fillna('Unknown')\ndf['PURPOSE'].value_counts()\n\nUnknown            372\nMeeting            164\nMeal/Entertain     148\nErrand/Supplies    111\nCustomer Visit      92\nTemporary Site      32\nBetween Offices     18\nMoving               4\nCharity ($)          1\nCommute              1\nAirport/Travel       1\nName: PURPOSE, dtype: int64\ndf.sample()\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n972\n11/18/2016 21:56\n2016-11-18 22:21:00\nBusiness\nKrendle Woods\nWhitebridge\n6.1\nMeeting\n21:56\n22:21"
  },
  {
    "objectID": "Homework/3/Homework 3 | Solutions.html#data-exploration",
    "href": "Homework/3/Homework 3 | Solutions.html#data-exploration",
    "title": "1. Data Processing",
    "section": "Data Exploration",
    "text": "Data Exploration\n\ndf['CATEGORY'].value_counts().plot.pie()\n\n&lt;AxesSubplot: ylabel='CATEGORY'&gt;\n\n\n\n\n\n\ndf['START'].value_counts().to_frame()\n\n\n\n\n\n\n\n\nSTART\n\n\n\n\nCary\n200\n\n\nMorrisville\n85\n\n\nWhitebridge\n68\n\n\nDurham\n37\n\n\nIslamabad\n29\n\n\n...\n...\n\n\nCoxville\n1\n\n\nLakeview\n1\n\n\nLower Garden District\n1\n\n\nConvention Center District\n1\n\n\nGampaha\n1\n\n\n\n\n175 rows × 1 columns\n\n\n\n\ndf['STOP'].value_counts().to_frame()\n\n\n\n\n\n\n\n\nSTOP\n\n\n\n\nCary\n203\n\n\nMorrisville\n83\n\n\nWhitebridge\n65\n\n\nDurham\n36\n\n\nIslamabad\n30\n\n\n...\n...\n\n\nDaytona Beach\n1\n\n\nSand Lake Commons\n1\n\n\nSky Lake\n1\n\n\nVista East\n1\n\n\nIlukwatta\n1\n\n\n\n\n187 rows × 1 columns\n\n\n\n\ndf\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\nstart time\nend time\n\n\n\n\n0\n01-01-2016 21:11\n2016-01-01 21:17:00\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n21:11\n21:17\n\n\n1\n01-02-2016 01:25\n2016-01-02 01:37:00\nBusiness\nFort Pierce\nFort Pierce\n5.0\nUnknown\n01:25\n01:37\n\n\n2\n01-02-2016 20:25\n2016-01-02 20:38:00\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n20:25\n20:38\n\n\n3\n01-05-2016 17:31\n2016-01-05 17:45:00\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n17:31\n17:45\n\n\n4\n01-06-2016 14:42\n2016-01-06 15:49:00\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n14:42\n15:49\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1148\n12/30/2016 16:45\n2016-12-30 17:08:00\nBusiness\nKar?chi\nKar?chi\n4.6\nMeeting\n16:45\n17:08\n\n\n1149\n12/30/2016 23:06\n2016-12-30 23:10:00\nBusiness\nKar?chi\nKar?chi\n0.8\nCustomer Visit\n23:06\n23:10\n\n\n1150\n12/31/2016 1:07\n2016-12-31 01:14:00\nBusiness\nKar?chi\nKar?chi\n0.7\nMeeting\n1:07\n1:14\n\n\n1153\n12/31/2016 21:32\n2016-12-31 21:50:00\nBusiness\nKatunayake\nGampaha\n6.4\nTemporary Site\n21:32\n21:50\n\n\n1154\n12/31/2016 22:08\n2016-12-31 23:51:00\nBusiness\nGampaha\nIlukwatta\n48.2\nTemporary Site\n22:08\n23:51\n\n\n\n\n944 rows × 9 columns\n\n\n\n\nplt.figure(figsize=(8, 6))\nplt.hist(df['MILES'], bins=10, edgecolor='black')\nplt.xlabel('Miles')\nplt.ylabel('Frequency')\nplt.title('Distribution of Miles Driven')\nplt.show()\n\n\n\n\n\npurpose_counts = df[df['PURPOSE']!='Unknown']['PURPOSE'].value_counts()\nplt.figure(figsize=(8, 6))\nplt.pie(purpose_counts.values, labels=purpose_counts.index, autopct='%1.1f%%')\nplt.title('Purpose of Trips')\nplt.show()\n\n\n\n\n\ndf = df[df['START_DATE'] != \"Totals\"]\n\ndf['START_DATE'] = pd.to_datetime(df['START_DATE'])\n\ndf.set_index('START_DATE', inplace=True)\n\ndaily_miles = df.resample('D')['MILES'].sum()\n\nplt.figure(figsize=(12, 6))\nplt.plot(daily_miles.index, daily_miles.values)\nplt.xlabel('Date')\nplt.ylabel('Miles Driven')\nplt.title('Miles Driven Over Time')\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nX_train\n\n\n\n\n\n\n\n\nSTART_Agnew\nSTART_Almond\nSTART_Apex\nSTART_Arabi\nSTART_Arlington\nSTART_Arlington Park at Amberly\nSTART_Asheville\nSTART_Austin\nSTART_Banner Elk\nSTART_Bellevue\n...\nPURPOSE_Between Offices\nPURPOSE_Charity ($)\nPURPOSE_Commute\nPURPOSE_Customer Visit\nPURPOSE_Errand/Supplies\nPURPOSE_Meal/Entertain\nPURPOSE_Meeting\nPURPOSE_Moving\nPURPOSE_Temporary Site\nPURPOSE_Unknown\n\n\nSTART_DATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-03-26 16:26:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2016-02-07 20:22:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-05-01 17:33:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2016-05-22 15:39:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2016-06-29 11:49:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2016-02-14 16:35:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-04-03 02:00:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-12-02 20:41:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2016-06-24 20:44:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2016-02-13 23:45:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n755 rows × 188 columns\n\n\n\n\nfeatures = ['START', 'CATEGORY', 'PURPOSE']\n\n\nX = df[features]\nX = pd.get_dummies(X) \ny = df['MILES']\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nX_val,X_test,y_val,y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\n\nprint(len(X_train),len(X_val),len(X_test))\n\n755 94 95\n\n\n\nX_train\n\n\n\n\n\n\n\n\nSTART_Agnew\nSTART_Almond\nSTART_Apex\nSTART_Arabi\nSTART_Arlington\nSTART_Arlington Park at Amberly\nSTART_Asheville\nSTART_Austin\nSTART_Banner Elk\nSTART_Bellevue\n...\nPURPOSE_Between Offices\nPURPOSE_Charity ($)\nPURPOSE_Commute\nPURPOSE_Customer Visit\nPURPOSE_Errand/Supplies\nPURPOSE_Meal/Entertain\nPURPOSE_Meeting\nPURPOSE_Moving\nPURPOSE_Temporary Site\nPURPOSE_Unknown\n\n\nSTART_DATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-03-26 16:26:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2016-02-07 20:22:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-05-01 17:33:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2016-05-22 15:39:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2016-06-29 11:49:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2016-02-14 16:35:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-04-03 02:00:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2016-12-02 20:41:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2016-06-24 20:44:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2016-02-13 23:45:00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n755 rows × 188 columns\n\n\n\n\nmodel = LinearRegression(fit_intercept=True)\n\n\nmodel.fit(X_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ny_pred = model.predict(X_val)\nlen(y_pred)\n\n94\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(np.arange(len(y_val)), y_val, label='Actual Trend')\nplt.plot(np.arange(len(y_pred)), y_pred, label='Predicted Trend')\nplt.xlabel('Data Index')\nplt.ylabel('Trend')\nplt.title('Actual Trend vs. Predicted Trend')\nplt.legend()\nplt.show()\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R-squared Score: {r2}\")\n\nMean Squared Error: 2.6619837257798516e+27\nR-squared Score: -4.087381199893086e+24\n\n\n\nmodel.intercept_\n\n0.0\n\n\n\nfrom sklearn.linear_model import Ridge\n\n\nridge_model = Ridge()\n\n\nridge_model.fit(X_train,y_train)\n\nRidge()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge()\n\n\n\ny_pred = ridge_model.predict(X_val)\n\n\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R-squared Score: {r2}\")\n\nMean Squared Error: 657.8014876232421\nR-squared Score: -0.010030755535618496\n\n\n\nridge_model.alpha\n\n1.0\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(np.arange(len(y_test)), y_test, label='Actual Trend')\nplt.plot(np.arange(len(y_pred)), y_pred, label='Predicted Trend')\nplt.xlabel('Data Index')\nplt.ylabel('Trend')\nplt.title('Actual Trend vs. Predicted Trend')\nplt.legend()\nplt.show()\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\n\n\nlasso_model = Lasso()\n\n\nlasso_model.fit(X_train,y_train)\n\nLasso()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso()\n\n\n\ny_pred = lasso_model.predict(X_val)\n\n\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R-squared Score: {r2}\")\n\nMean Squared Error: 647.395188616323\nR-squared Score: 0.0059477459516085185"
  },
  {
    "objectID": "Ethics/Week_7.html#questions-for-discussion",
    "href": "Ethics/Week_7.html#questions-for-discussion",
    "title": "Week 6 : AI in Autonomous Driving",
    "section": "",
    "text": "Should there be universal ethical standards and regulations governing the behavior of autonomous vehicles in life-threatening situations, or should the decision-making process be left to individual manufacturers? Why or why not?\nWhat role should society play in defining the ethical boundaries of self-driving cars, and how can we engage the public in discussions about the ethical dilemmas involved in this technology to ensure a more transparent and accountable development process?"
  },
  {
    "objectID": "Ethics/Week_8.html#questions-for-discussion",
    "href": "Ethics/Week_8.html#questions-for-discussion",
    "title": "Week 7 : AI in Housekeeping",
    "section": "",
    "text": "Is it ethically acceptable for companies to present users with excessively long privacy policies, knowing that most users are unlikely to read and understand them fully?\nWhat ethical responsibilities do companies have in ensuring that their privacy policies are concise, clear, and easily understandable for users, even if it means additional effort and resources?\nShould there be legal regulations or industry standards mandating a maximum length for privacy policies to prevent companies from burdening users with overly complex and lengthy documents?\n\nElaborate."
  },
  {
    "objectID": "Ethics/Week_9.html#question-for-discussion",
    "href": "Ethics/Week_9.html#question-for-discussion",
    "title": "Week 8 : AI for Facial Recognition",
    "section": "",
    "text": "Is it ethical for governments and private entities to use facial recognition technology for surveillance without explicit consent from individuals?\nShould facial recognition technology be used in law enforcement to identify and apprehend suspects, considering the potential for misidentification and its impact on innocent individuals?"
  },
  {
    "objectID": "Ethics/Week_9.html#questions-for-discussion",
    "href": "Ethics/Week_9.html#questions-for-discussion",
    "title": "Week 8 : AI for Facial Recognition",
    "section": "",
    "text": "Is it ethical for governments and private entities to use facial recognition technology for surveillance without explicit consent from individuals?\nShould facial recognition technology be used in law enforcement to identify and apprehend suspects, considering the potential for misidentification and its impact on innocent individuals?"
  },
  {
    "objectID": "Ethics/Week_7.html#question-for-discussion",
    "href": "Ethics/Week_7.html#question-for-discussion",
    "title": "Week 7 : AI for Recommendations",
    "section": "",
    "text": "What ethical concerns arise when companies use consumer data, such as purchasing habits, to make sensitive inferences about individuals, like Target did with predicting a teen girl’s pregnancy?\nShould companies be allowed to gather and analyze personal data without explicit consent to infer potentially private and sensitive information about their customers?\nHow can companies balance the benefits of personalized marketing and targeted advertising with the ethical obligations to respect customer privacy and autonomy?\n\nElaborate your answers"
  },
  {
    "objectID": "Ethics/Week_6.html#activity",
    "href": "Ethics/Week_6.html#activity",
    "title": "Week 6 : AI in Autonomous Driving",
    "section": "",
    "text": "Moral Machine"
  },
  {
    "objectID": "Ethics/Week_6.html#questions-for-discussion",
    "href": "Ethics/Week_6.html#questions-for-discussion",
    "title": "Week 6 : AI in Autonomous Driving",
    "section": "",
    "text": "Did you try the activity listed? If not, give it a try. After that, answer the following question.\nWhat are your thoughts on moral/ethical decisions with respect to autonomous vehicles, especially in the case of who to save when getting into an accident. Do you think your opinion on this matter would be universal? It’s unlikely. In that case, who gets to decide what decision an autonomous vehicle should make?"
  },
  {
    "objectID": "Course_Content/Week_8/home.html",
    "href": "Course_Content/Week_8/home.html",
    "title": "Week 8 : Continuation of Probability and Statistics",
    "section": "",
    "text": "8.1 - Useful Discrete Distributions\n8.2 Useful Continuous Distributions and MLE\n8.3 Wrapping up Statistics and Probability\n\n\n\n\n\n\nFinal\n\n\n\nDiscussion Prompt"
  },
  {
    "objectID": "Course_Content/Week_8/home.html#lectures",
    "href": "Course_Content/Week_8/home.html#lectures",
    "title": "Week 8 : Continuation of Probability and Statistics",
    "section": "",
    "text": "8.1 - Useful Discrete Distributions\n8.2 Useful Continuous Distributions and MLE\n8.3 Wrapping up Statistics and Probability"
  },
  {
    "objectID": "Course_Content/Week_8/home.html#activities",
    "href": "Course_Content/Week_8/home.html#activities",
    "title": "Week 8 : Continuation of Probability and Statistics",
    "section": "",
    "text": "Final\n\n\n\nDiscussion Prompt"
  },
  {
    "objectID": "Course_Content/Week_8/1/home.html",
    "href": "Course_Content/Week_8/1/home.html",
    "title": "8.1 Useful Discrete Distributions",
    "section": "",
    "text": "Useful Discrete Distributions\n\nuniform distribution\n[Revise] Bernouilli\nGeometric\nCategorical\nMultinomial\nPoisson\n\n\n\n\n\n\n\n\n\n\n\n8.2 Useful Continuous Distributions and MLE"
  },
  {
    "objectID": "Course_Content/Week_8/1/home.html#objectives",
    "href": "Course_Content/Week_8/1/home.html#objectives",
    "title": "8.1 Useful Discrete Distributions",
    "section": "",
    "text": "Useful Discrete Distributions\n\nuniform distribution\n[Revise] Bernouilli\nGeometric\nCategorical\nMultinomial\nPoisson"
  },
  {
    "objectID": "Course_Content/Week_8/1/home.html#next-class",
    "href": "Course_Content/Week_8/1/home.html#next-class",
    "title": "8.1 Useful Discrete Distributions",
    "section": "",
    "text": "8.2 Useful Continuous Distributions and MLE"
  },
  {
    "objectID": "Course_Content/Week_8/3/home.html",
    "href": "Course_Content/Week_8/3/home.html",
    "title": "8.3 Wrapping up Statistics and Probability",
    "section": "",
    "text": "Confidence Intervals\n\nOverview\nBootstrap confidence intervals\n\nEstimator Properties\n\nEstimator Bias\nEstimator Variance\nBias-Variance Tradeoff\n\nLaw of Large Numbers\nCentral Limit Theorem"
  },
  {
    "objectID": "Course_Content/Week_8/3/home.html#objectives",
    "href": "Course_Content/Week_8/3/home.html#objectives",
    "title": "8.3 Wrapping up Statistics and Probability",
    "section": "",
    "text": "Confidence Intervals\n\nOverview\nBootstrap confidence intervals\n\nEstimator Properties\n\nEstimator Bias\nEstimator Variance\nBias-Variance Tradeoff\n\nLaw of Large Numbers\nCentral Limit Theorem"
  },
  {
    "objectID": "Course_Content/Week_8/2/home.html",
    "href": "Course_Content/Week_8/2/home.html",
    "title": "8.2 Useful Continuous Distributions and MLE",
    "section": "",
    "text": "Useful Continuous Distributions\n\nUniform\nExponential\nGaussian\nMultivariate Gaussian\n\nLikelihood Function & Maximum Likelihood Estimator (MLE) \n\n\n\n\n\n\n\n\n\n8.3 Wrapping up Statistics and Probability"
  },
  {
    "objectID": "Course_Content/Week_8/2/home.html#objectives",
    "href": "Course_Content/Week_8/2/home.html#objectives",
    "title": "8.2 Useful Continuous Distributions and MLE",
    "section": "",
    "text": "Useful Continuous Distributions\n\nUniform\nExponential\nGaussian\nMultivariate Gaussian\n\nLikelihood Function & Maximum Likelihood Estimator (MLE)"
  },
  {
    "objectID": "Course_Content/Week_8/2/home.html#next-class",
    "href": "Course_Content/Week_8/2/home.html#next-class",
    "title": "8.2 Useful Continuous Distributions and MLE",
    "section": "",
    "text": "8.3 Wrapping up Statistics and Probability"
  },
  {
    "objectID": "Course_Content/Week_9/1/home.html",
    "href": "Course_Content/Week_9/1/home.html",
    "title": "9.1 Neural Networks",
    "section": "",
    "text": "Introduction\nWhat is Deep Learning\nIntroduction to Neural Networks\nHow do Neural Networks LEARN?\nCore terminologies used in Deep Learning\nActivation Functions\nLoss Functions\nOptimizers\nParameters vs Hyperparameters\nEpochs, Batches & Iterations\nConclusion to Terminologies\nIntroduction to Learning\nUnsupervised Learning\nReinforcement Learning\nRegularization\nIntroduction to Neural Network Architectures\nFully-Connected Feedforward Neural Nets\nRecurrent Neural Nets\nConvolutional Neural Nets\n\n\n\n\n\n\n\n\n\n\nCourse Wrap-up"
  },
  {
    "objectID": "Course_Content/Week_9/1/home.html#objectives",
    "href": "Course_Content/Week_9/1/home.html#objectives",
    "title": "9.1 Neural Networks",
    "section": "",
    "text": "Introduction\nWhat is Deep Learning\nIntroduction to Neural Networks\nHow do Neural Networks LEARN?\nCore terminologies used in Deep Learning\nActivation Functions\nLoss Functions\nOptimizers\nParameters vs Hyperparameters\nEpochs, Batches & Iterations\nConclusion to Terminologies\nIntroduction to Learning\nUnsupervised Learning\nReinforcement Learning\nRegularization\nIntroduction to Neural Network Architectures\nFully-Connected Feedforward Neural Nets\nRecurrent Neural Nets\nConvolutional Neural Nets"
  },
  {
    "objectID": "Course_Content/Week_9/1/home.html#next-class",
    "href": "Course_Content/Week_9/1/home.html#next-class",
    "title": "9.1 Neural Networks",
    "section": "",
    "text": "Course Wrap-up"
  },
  {
    "objectID": "Course_Content/Week_9/home.html",
    "href": "Course_Content/Week_9/home.html",
    "title": "Week 9 Neural Networks and Wrap-up",
    "section": "",
    "text": "9.1 - Neural Networks\n\n\n\n\nParticipation and Bonus Activities to be released.\n\n\nFinal"
  },
  {
    "objectID": "Course_Content/Week_9/home.html#lectures",
    "href": "Course_Content/Week_9/home.html#lectures",
    "title": "Week 9 Neural Networks and Wrap-up",
    "section": "",
    "text": "9.1 - Neural Networks"
  },
  {
    "objectID": "Course_Content/Week_9/home.html#activities",
    "href": "Course_Content/Week_9/home.html#activities",
    "title": "Week 9 Neural Networks and Wrap-up",
    "section": "",
    "text": "Participation and Bonus Activities to be released.\n\n\nFinal"
  }
]