# Week 4 : AI for Mental Health Support 

## Reading
<br>
Trigger Warning: Mention of Mental Health Issues and Suicide.

- [Eating Disorder Helpline Fires Staff, Transitions to Chatbot After Unionization](https://www.vice.com/en/article/n7ezkm/eating-disorder-helpline-fires-staff-transitions-to-chatbot-after-unionization)
- [Eating Disorder Helpline Disables Chatbot for 'Harmful' Responses After Firing Human Staff](https://www.vice.com/en/article/qjvk97/eating-disorder-helpline-disables-chatbot-for-harmful-responses-after-firing-human-staff)
- ['He Would Still Be Here': Man Dies by Suicide After Talking with AI Chatbot, Widow Says](https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says)

<br>

## Question for Discussion 

As always, there is no right or wrong answers here. This is an invitation to share your opinion.

(a) Should there be legal and regulatory frameworks in place to monitor and assess the ethical implications of AI chatbot interventions in mental health support, similar to the regulations imposed on human healthcare providers? Any suggestions?

(b) Should we be limiting to what extent AI chatbots should be allowed to simulate human emotions and empathy, considering the potential ethical implications of deceiving users into believing they are interacting with a human?

(c) If an AI "friend" could effectively simulate human emotions, generate a face, voice, and create videos,all things it can already do to an extend, would you be willing to accept and form a meaningful connection with it in a similar way to how people engage in long-distance relationships? Considering this, do you believe your perception and emotional attachment would differ between an AI companion and a friend in a long-distance relationship?

[Optional] (d) Read the synopsis of the Movie Her, if you haven't watched the movie already. Did your stance on Question c change? If yes, Why?

_Questions prepared with aid of ChatGPT_